{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Techniche - Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/glmack_agoralytix_com/.local/lib/python3.7/site-packages/py4j/java_collections.py:13: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import (\n",
      "/home/glmack_agoralytix_com/.local/lib/python3.7/site-packages/py4j/java_collections.py:13: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import (\n",
      "/home/glmack_agoralytix_com/.local/lib/python3.7/site-packages/py4j/java_collections.py:13: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import (\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.corpora import mmcorpus\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models import AuthorTopicModel\n",
    "from gensim.test.utils import common_dictionary, datapath, temporary_file\n",
    "from smart_open import smart_open\n",
    "\n",
    "import spacy\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, punkt, RegexpTokenizer, wordpunct_tokenize\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
    "\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "\n",
    "from topic_model import tokenize_docs\n",
    "\n",
    "from smart_open import smart_open\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import pyspark\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to download stop words from nltk and language package from spacy\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# !python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import data from PatentsView API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_colwidth', -1)\n",
    "pd.options.display.max_columns = 50\n",
    "pd.set_option('display.max_rows', 50)\n",
    "\n",
    "# patents endpoint\n",
    "endpoint_url = 'http://www.patentsview.org/api/patents/query'\n",
    "\n",
    "# build list of possible fields that endpoint request will return\n",
    "df = pd.read_excel(\"data/patents_view_patents_fields.xlsx\")\n",
    "df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\n",
    "pat_fields = df.api_field_name.values.tolist()\n",
    "\n",
    "# build query\n",
    "query={\"_or\":[{\"_text_phrase\":{\"patent_title\":\"natural language\"}},{\"_text_phrase\":{\"patent_abstract\":\"natural language\"}}]}\n",
    "fields=pat_fields\n",
    "options={\"per_page\":2500}\n",
    "sort=[{\"patent_date\":\"desc\"}]\n",
    "\n",
    "params={'q': json.dumps(query),\n",
    "        'f': json.dumps(fields),\n",
    "        'o': json.dumps(options),\n",
    "        's': json.dumps(sort)}\n",
    "\n",
    "# request and results\n",
    "resp = requests.get(endpoint_url, params=params)\n",
    "results = resp.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Structure data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract metadata from response\n",
    "print(\"status code:\", resp.status_code,';', \"reason:\", resp.reason)\n",
    "total_patent_count = results[\"total_patent_count\"]\n",
    "patents_per_page = results['count']\n",
    "print(\"total_patent_count:\",total_patent_count,';', \"patents_per_page:\", patents_per_page)\n",
    "\n",
    "# extract data from response\n",
    "data_resp = results['patents']\n",
    "# data_resp[0]\n",
    "raw_df = pd.DataFrame(data_resp)\n",
    "raw_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import data from bulk download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13439083003"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download TSV files containing detailed patent descriptions from PatentsView \n",
    "# http://data.patentsview.org/detail-description-text/detail-desc-text-2016.tsv.zip # 2016\n",
    "# http://data.patentsview.org/detail-description-text/detail-desc-text-2017.tsv.zip # 2017\n",
    "# http://data.patentsview.org/detail-description-text/detail-desc-text-2018.tsv.zip # 2018\n",
    "# http://data.patentsview.org/detail-description-text/detail-desc-text-2019.tsv.zip # 2019\n",
    "\n",
    "# uncomment to view size of tsv files to load\n",
    "os.path.getsize(\"data/detail-desc-text-2016.tsv\")\n",
    "# os.path.getsize(\"data/detail-desc-text-2017.tsv\")\n",
    "# os.path.getsize(\"data/detail-desc-text-2018.tsv\")\n",
    "# os.path.getsize(\"data/detail_desc_text_2019.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12.5GB'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_bytes(13439083003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12.52 GB'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_size(13439083003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://my-fastai-instance.us-west2-b.c.weighty-elf-228123.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create SparkSession/SparkContext as entry point to Spark's Dataset/DataFrame API\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2018 = (spark.read.format(\"csv\")\n",
    "               .option(\"delimiter\", \"\\t\")\n",
    "               .load(\"data/detail-desc-text-2018.tsv\"))\n",
    "               #.write\n",
    "               #.save(\"df_2018.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160249"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 160249 rows in 2018 dataset\n",
    "df_2018.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_c0', '_c1', '_c2']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2018.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) FileScan csv [_c0#10,_c1#11,_c2#12] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/glmack_agoralytix_com/techniche/data/detail-desc-text-2018.tsv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<_c0:string,_c1:string,_c2:string>\n"
     ]
    }
   ],
   "source": [
    "df_2018.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+------------------+\n",
      "|summary|                 _c0|                 _c1|               _c2|\n",
      "+-------+--------------------+--------------------+------------------+\n",
      "|  count|              160249|              160249|             37278|\n",
      "|   mean|1.0055075034692628E7|                null|47672.901443210474|\n",
      "| stddev|   80038.35207688982|                null|53306.603326141594|\n",
      "|    min|            10000000|#System-Minimal# ...|             10000|\n",
      "|    max|             RE47183|•Microphone• Here...|             99984|\n",
      "+-------+--------------------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2018.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_c0', 'string'), ('_c1', 'string'), ('_c2', 'string')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2018.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_171819 = df_2017.union(df_2018).union(df_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_171819.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_171819.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2018.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2019 = (spark.read.format(\"csv\")\n",
    "               .option(\"delimiter\", \"\\t\")\n",
    "               .load(\"data/detail-desc-text-2018.tsv\"))\n",
    "               #.write\n",
    "               #.save(\"df_2018.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2019.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2016 = (spark.read.format(\"csv\")\n",
    "               .option(\"delimiter\", \"\\t\")\n",
    "               .option(\"header\", \"true\")\n",
    "               .load(\"data/detail-desc-text-2016.tsv\"))\n",
    "               #.write\n",
    "               #.save(\"df_2018.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2016.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark.read.format(\"csv\")\n",
    "           .option(\"delimiter\", \",\")\n",
    "           .infer\n",
    "           .load(\"data/df.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2017 = (spark.read.format(\"csv\")\n",
    "               .option(\"delimiter\", \"\\t\")\n",
    "               .load(\"data/detail-desc-text-2017.tsv\")\n",
    "               .persist())\n",
    "               #.write\n",
    "               #.save(\"df_2018.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2018 = spark.read.parquet(\"data/df_2018.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2018.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2018.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2018.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2018.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subset dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset dataframe - comment/uncomment to include fields\n",
    "df = raw_df[['patent_number', \n",
    "         'patent_date', \n",
    "         'patent_title',\n",
    "         'patent_abstract', \n",
    "         'patent_firstnamed_assignee_id',\n",
    "         'patent_firstnamed_assignee_location_id',\n",
    "         'patent_firstnamed_assignee_latitude',\n",
    "         'patent_firstnamed_assignee_longitude',\n",
    "         'patent_firstnamed_assignee_city',\n",
    "         'patent_firstnamed_assignee_state',\n",
    "         'patent_firstnamed_assignee_country', \n",
    "         'patent_firstnamed_inventor_id',\n",
    "         'patent_firstnamed_inventor_location_id',\n",
    "         'patent_firstnamed_inventor_latitude',\n",
    "         'patent_firstnamed_inventor_longitude',\n",
    "         'patent_firstnamed_inventor_city',\n",
    "         'patent_firstnamed_inventor_state',\n",
    "         'patent_firstnamed_inventor_country',\n",
    "         'patent_year', \n",
    "         'patent_type', \n",
    "         'patent_kind',\n",
    "         'inventors'\n",
    "#          'patent_processing_time', \n",
    "#          'patent_num_us_application_citations', \n",
    "#          'patent_num_us_patent_citations', \n",
    "#          'patent_num_foreign_citations', \n",
    "#          'patent_num_combined_citations', \n",
    "#          'patent_num_claims', \n",
    "#          'patent_num_cited_by_us_patents',\n",
    "#          'detail_desc_length'\n",
    "            ]]\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 561 different assignees\n",
    "len(df.patent_firstnamed_assignee_id.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new column that combines the patent title and the patent abstract columns into a single string\n",
    "df['patent_title_abstract'] = df.patent_title + ' ' + df.patent_abstract\n",
    "df.patent_title_abstract.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TODO (Lee) Partition data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.patent_number.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=['patent_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = df.patent_title_abstract.tolist()\n",
    "text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partition data\n",
    "len(text_data)\n",
    "text_train = text_data[:round(len(text_data)*.8)]\n",
    "text_test = text_data[round(len(text_data)*.8):]\n",
    "print(len(text_data), len(text_train), len(text_test), len(text_train)+len(text_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to download stop words from nltk and language package from spacy\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# !python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct pipeline using Spacy Language object and associated pipeline/components\n",
    "nlp = spacy.load(\"en\")\n",
    "pprint(nlp.pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = []   \n",
    "\n",
    "# process patent documents in pipeline\n",
    "for doc in nlp.pipe(text_train, n_threads=4, batch_size=100):\n",
    "   \n",
    "    ents = doc.ents  # Named entities.\n",
    "\n",
    "    # Keep only words (no numbers, no punctuation).\n",
    "    # Lemmatize tokens, remove punctuation and remove stopwords.\n",
    "    doc = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
    "\n",
    "    # Remove common words from a stopword list.\n",
    "    doc = [token for token in doc if token not in stop_words]\n",
    "\n",
    "    # Add named entities, but only if they are a compound of more than word.\n",
    "    doc.extend([str(entity) for entity in ents if len(entity) > 1])\n",
    "    \n",
    "    processed_docs.append(doc)\n",
    "\n",
    "processed_docs[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[token.text for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = set([w.label_ for w in doc.ents]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in labels: \n",
    "    entities = [cleanup(e.string, lower=False) for e in document.ents if label==e.label_] \n",
    "    entities = list(set(entities)) \n",
    "    print(label,entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processed_docs = []\n",
    "for doc in nlp.pipe(docs, n_threads=4, batch_size=100):\n",
    "    # Process document using Spacy NLP pipeline.\n",
    "    \n",
    "    ents = doc.ents  # Named entities.\n",
    "\n",
    "    # Keep only words (no numbers, no punctuation).\n",
    "    # Lemmatize tokens, remove punctuation and remove stopwords.\n",
    "    doc = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
    "\n",
    "    # Remove common words from a stopword list.\n",
    "    #doc = [token for token in doc if token not in STOPWORDS]\n",
    "\n",
    "    # Add named entities, but only if they are a compound of more than word.\n",
    "    doc.extend([str(entity) for entity in ents if len(entity) > 1])\n",
    "    \n",
    "    pre_processed_docs.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize documents\n",
    "\n",
    "def tokenize_docs(docs):\n",
    "    tokenized_docs = []\n",
    "    for doc in docs:\n",
    "        tokenized_docs.append(word_tokenize(doc))\n",
    "    return tokenized_docs\n",
    "\n",
    "tokenized_docs = tokenize_docs(text_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean punctuation\n",
    "def clean_docs(tokenized_docs):\n",
    "    clean_docs = []\n",
    "    for doc in tokenized_docs:\n",
    "       clean_docs.append([word for word in doc if word.isalpha()])  \n",
    "    return clean_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = clean_docs(tokenized_docs)\n",
    "cleaned_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to lowercase\n",
    "def lower_words(docs):\n",
    "    lowered_words = []\n",
    "    for doc in docs:\n",
    "        lowered_words.append([word.lower() for word in doc])\n",
    "    return lowered_words\n",
    "\n",
    "lowered_data = lower_words(cleaned_data)\n",
    "lowered_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_stopwords(docs):\n",
    "    filtered_docs = []\n",
    "    for doc in docs:\n",
    "       filtered_docs.append([word for word in doc if word not in stop_words])\n",
    "    return filtered_docs\n",
    "\n",
    "# remove stopwords\n",
    "filtered_data = filter_stopwords(lowered_data)\n",
    "filtered_data\n",
    "# TODO (Lee) - resolve un-lowered stopwords \"A\" and \"An\", 'By', 'The'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct bigrams and trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train bigram phrases model\n",
    "bigram_model = Phrases(filtered_data, min_count=1, threshold=1)\n",
    "\n",
    "# train trigram phrases model\n",
    "trigram_model = Phrases(bigram_model[filtered_data], threshold=100)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigrams\n",
    "def bigrams(docs):\n",
    "    \"\"\"create bigrams\"\"\"\n",
    "    return [bigram_model[doc] for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize bigram and trigram models\n",
    "bigram_model = gensim.models.phrases.Phraser(bigram_model)\n",
    "trigram_model = gensim.models.phrases.Phraser(trigram_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams(filtered_data)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigrams(docs):\n",
    "    \"\"\"create trigrams\"\"\"\n",
    "    return [trigram_model[bigram_model[doc]] for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams(filtered_data)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stem and Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_docs(docs, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"lemmatize documents\"\"\"\n",
    "    lemmatized_docs = []\n",
    "    for doc in docs: \n",
    "        lemmatized_docs.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return lemmatized_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Lee)\n",
    "\n",
    "lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)\n",
    "# for doc in cleaned_data:\n",
    "#     for token in doc:\n",
    "#         token.lemma_\n",
    "\n",
    "# uncomment to use\n",
    "# download english model with \"python -m spacy download en\"\n",
    "\n",
    "# for token in doc:\n",
    "#     print(token, token.lemma, token.lemma_)\n",
    "\n",
    "# TODO (Lee) - lemmatize_docs(cleaned_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create corpus and dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using spacy pipeline components\n",
    "# build dictionary\n",
    "id_to_word = corpora.Dictionary(processed_docs)\n",
    "\n",
    "# build corpus\n",
    "texts = processed_docs\n",
    "\n",
    "# apply term document frequency\n",
    "# converts documents in corpus to bag-of-words format, a list of (token_id, token_count) tuples\n",
    "corpus = [id_to_word.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # build dictionary\n",
    "id_to_word = corpora.Dictionary(filtered_data)\n",
    "\n",
    "# build corpus\n",
    "texts = filtered_data\n",
    "\n",
    "# apply term document frequency\n",
    "# converts documents in corpus to bag-of-words format, a list of (token_id, token_count) tuples\n",
    "corpus = [id_to_word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view formatted corpus (term-doc-frequency)\n",
    "[[(id_to_word[id], freq) for id, freq in text] for text in corpus][:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model - model #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Lee) - deprecation warnings\n",
    "# construct LDA model\n",
    "model_lda = LdaModel(corpus=corpus,\n",
    "                     id2word=id_to_word,\n",
    "                     num_topics=25, \n",
    "                     random_state=100,\n",
    "                     update_every=1,\n",
    "                     chunksize=100,\n",
    "                     passes=10,\n",
    "                     alpha='auto',\n",
    "                     per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print keywords in n topics\n",
    "pprint(model_lda.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print top 10 keywords that comprise topic with index of 0\n",
    "pprint(model_lda.print_topic(24))\n",
    "# the most import keywords, and the respective weight, that form topic 0 are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print top 10 keywords that comprise topic with index of 1\n",
    "pprint(model_lda.print_topic(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Lee) - infer topic from keywords?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate - model #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate perplexity metrics\n",
    "perplexity = model_lda.log_perplexity(corpus)\n",
    "perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Lee) - confirm that filtered_data is indeed the correct dataset to pass to texts param\n",
    "# calculate coherence metric\n",
    "coherence = CoherenceModel(model=model_lda, texts=processed_docs, dictionary=id_to_word, coherence='c_v')\n",
    "coherence_1 = coherence.get_coherence()\n",
    "coherence_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Lee) - confirm that filtered_data is indeed the correct dataset to pass to texts param\n",
    "# calculate coherence metric\n",
    "coherence = CoherenceModel(model=model_lda, texts=filtered_docs, dictionary=id_to_word, coherence='c_v')\n",
    "coherence_1 = coherence.get_coherence()\n",
    "coherence_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate coherence metric or each of the n topicss\n",
    "coherence_1 = coherence.get_coherence_per_topic()\n",
    "coherence_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore topics\n",
    "pyLDAvis.enable_notebook()\n",
    "viz_topics_1 = pyLDAvis.gensim.prepare(model_lda, corpus, id_to_word)\n",
    "viz_topics_1\n",
    "# TODO (Lee) - salient vs relevant terms in pyLDA ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2-  Mallet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to download Mallet topic model\n",
    "# !wget http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip\n",
    "# update this path\n",
    "path_mallet = 'data/mallet-2.0.8/bin/mallet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = gensim.models.wrappers.LdaMallet(path_mallet, corpus=corpus, num_topics=25, id2word=id_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topics\n",
    "pprint(model_2.show_topics(formatted=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate coherence metric\n",
    "coherence_model_2 = CoherenceModel(model=model_2, texts=data, dictionary=id_to_word, coherence='c_v')\n",
    "coherence_model_2 = coherence_model_2.get_coherence()\n",
    "coherence_model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Lee)\n",
    "# def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "#     \"\"\"\n",
    "#     Compute c_v coherence for various number of topics\n",
    "\n",
    "#     Parameters:\n",
    "#     ----------\n",
    "#     dictionary : Gensim dictionary\n",
    "#     corpus : Gensim corpus\n",
    "#     texts : List of input texts\n",
    "#     limit : Max num of topics\n",
    "\n",
    "#     Returns:\n",
    "#     -------\n",
    "#     model_list : List of LDA topic models\n",
    "#     coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "#     \"\"\"\n",
    "#     coherence_values = []\n",
    "#     model_list = []\n",
    "#     for num_topics in range(start, limit, step):\n",
    "#         model = gensim.models.wrappers.LdaMallet(path_mallet, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "#         model_list.append(model)\n",
    "#         coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "#         coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "#     return model_list, coherence_values\n",
    "\n",
    "# model_list, coherence_values = compute_coherence_values(dictionary=id_to_word, corpus=corpus, texts=data, start=2, limit=40, step=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3 - Author topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct inventor-to-doc mapping as df from nested inventors column in json api response\n",
    "df_inventors = json_normalize(results['patents'], record_path=['inventors'], meta=['patent_number', 'patent_date'])\n",
    "df_inventors = df_inventors[['inventor_id', 'patent_number', 'patent_date']]\n",
    "df_inventors.sort_values(by=['patent_date'])\n",
    "df_inventors.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Lee) - resolve workaround\n",
    "df_idx = df\n",
    "df_idx['idx'] = df.index\n",
    "df_idx\n",
    "df_idx_1 = df_idx[['patent_number', 'idx', 'inventors']]\n",
    "df_idx_2 = df_idx_1.set_index('patent_number')\n",
    "df_idx_2.pop('inventors')\n",
    "df_idx_2\n",
    "df_pat_idx = df_idx_2.T.to_dict('records')\n",
    "for i in df_pat_idx:\n",
    "    df_pat_idx = dict(i)\n",
    "df_pat_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pat_idx = df_idx_2.T.to_dict('records')\n",
    "for i in df_pat_idx:\n",
    "    df_pat_idx = dict(i)\n",
    "df_pat_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inv_test = json_normalize(results['patents'], record_path=['inventors'], meta=['patent_number', 'patent_date'])\n",
    "df_inv_test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idx_pat_inv_map = df[['patent_number', 'inventors']]\n",
    "df_idx_pat_inv_map.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Lee) - find out how to get list of patents_view_field names from API - I did it accidentally but need to replicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.patent_title_abstract[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inventors.set_index('inventor_id').T.to_dict('list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k, v in pat2inv.items():\n",
    "#     name_dict[new_key] = name_dict.pop(k)\n",
    "#     time.sleep(4)\n",
    "\n",
    "# pprint.pprint(name_dict)\n",
    "\n",
    "# d = {'x':1, 'y':2, 'z':3}\n",
    "# d1 = {'x':'a', 'y':'b', 'z':'c'}\n",
    "\n",
    "# dict((d1[key], value) for (key, value) in d.items())\n",
    "# {'a': 1, 'b': 2, 'c': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patdf2inv = dict((df_pat_idx[key], value) for (key, value) in pat2inv.items())\n",
    "patdf2inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat2inv = {k: list(v) for k,v in df_inventors.groupby(\"patent_number\")[\"inventor_id\"]}\n",
    "pat2inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "idx_pat_map = df.patent_number.to_dict()\n",
    "idx_pat_map = {str(key): value for key, value in idx_pat_map.items()}\n",
    "idx_pat_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct author-topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct author-topic model\n",
    "model_at = AuthorTopicModel(corpus=corpus,\n",
    "                         doc2author=patdf2inv,\n",
    "                         id2word=id_to_word, \n",
    "                         num_topics=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct vectors for authors\n",
    "author_vecs = [model_at.get_author_topics(author) for author in model_at.id2author.values()]\n",
    "author_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the topic distribution for an author using use model[name] syntax\n",
    "# each topic has a probability of being expressed given the particular author, but only the ones above a certain threshold are shown.\n",
    "\n",
    "model_at['7788103-1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def show_author(name):\n",
    "#     print('\\n%s' % name)\n",
    "#     print('Docs:', model.author2doc[name])\n",
    "#     print('Topics:')\n",
    "#     pprint([(topic_labels[topic[0]], topic[1]) for topic in model[name]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate per-word bound, which is a measure of the model's predictive performance (reconstruction error?)\n",
    "\n",
    "build doc2author dictionary\n",
    "\n",
    "doc2author = atmodel.construct_doc2author(model.corpus, model.author2doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import atmodel\n",
    "doc2author = atmodel.construct_doc2author(model.corpus, model.author2doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim.models.atmodel.construct_author2doc(doc2author)\n",
    "# construct mapping from author IDs to document IDs.\n",
    "\n",
    "Parameters:\tdoc2author (dict of (int, list of str)) – Mapping of document id to authors.\n",
    "Returns:\tMapping of authors to document ids.\n",
    "Return type:\tdict of (str, list of int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim.models.atmodel.construct_doc2author(corpus, author2doc)\n",
    "construct mapping from document IDs to author IDs\n",
    "\n",
    "Parameters:\t\n",
    "corpus (iterable of list of (int, float)) – Corpus in BoW format.\n",
    "author2doc (dict of (str, list of int)) – Mapping of authors to documents.\n",
    "Returns:\t\n",
    "Document to Author mapping.\n",
    "\n",
    "Return type:\t\n",
    "dict of (int, list of str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
