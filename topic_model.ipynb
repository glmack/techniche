{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Techniche - Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/glmack_agoralytix_com/.local/lib/python3.7/site-packages/py4j/java_collections.py:13: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import (\n",
      "/home/glmack_agoralytix_com/.local/lib/python3.7/site-packages/py4j/java_collections.py:13: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import (\n",
      "/home/glmack_agoralytix_com/.local/lib/python3.7/site-packages/py4j/java_collections.py:13: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import (\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.corpora import mmcorpus\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models import AuthorTopicModel\n",
    "from gensim.test.utils import common_dictionary, datapath, temporary_file\n",
    "from smart_open import smart_open\n",
    "\n",
    "import spacy\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, punkt, RegexpTokenizer, wordpunct_tokenize\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
    "\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "import calendar\n",
    "\n",
    "from topic_model import tokenize_docs#, (TODO) Lee convert_bytes\n",
    "\n",
    "from smart_open import smart_open\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import pyspark\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "# pd.set_option('display.max_colwidth', -1)\n",
    "pd.options.display.max_columns = 50\n",
    "pd.set_option('display.max_rows', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to download stop words from nltk and language package from spacy\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# !python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import data from PatentsView API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patents endpoint\n",
    "endpoint_url = 'http://www.patentsview.org/api/patents/query'\n",
    "\n",
    "# build list of possible fields that endpoint request will return\n",
    "df = pd.read_excel(\"data/patents_view_patents_fields.xlsx\")\n",
    "df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\n",
    "pat_fields = df.api_field_name.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build query - initial smalldata dataset\n",
    "query={\"_or\":[{\"_text_phrase\":{\"patent_title\":\"natural language\"}},{\"_text_phrase\":{\"patent_abstract\":\"natural language\"}}]}\n",
    "fields=pat_fields\n",
    "options={\"per_page\":2500}\n",
    "sort=[{\"patent_date\":\"desc\"}]\n",
    "\n",
    "params={'q': json.dumps(query),\n",
    "        'f': json.dumps(fields),\n",
    "        'o': json.dumps(options),\n",
    "        's': json.dumps(sort)}\n",
    "\n",
    "# request and results\n",
    "resp = requests.get(endpoint_url, params=params)\n",
    "results = resp.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # build query - testing keyword search in title and abstract\n",
    "# query={\"_and\":\n",
    "#         [{\"_or\":\n",
    "#             [{\"_text_phrase\":{\"patent_title\":\"natural language\"}}\n",
    "#             ,{\"_text_phrase\":{\"patent_abstract\":\"natural language\"}}]}\n",
    "#         ,{\"_and\":\n",
    "#       [{\"patent_year\":2016}]}]} \n",
    "\n",
    "# fields=pat_fields\n",
    "# options={\"per_page\":2500}\n",
    "# sort=[{\"patent_date\":\"desc\"}]\n",
    "\n",
    "# params={'q': json.dumps(query),\n",
    "#         'f': json.dumps(fields),\n",
    "#         'o': json.dumps(options),\n",
    "#         's': json.dumps(sort)}\n",
    "\n",
    "# # request and results\n",
    "# resp = requests.get(endpoint_url, params=params)\n",
    "# results = resp.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # build query based on date range\n",
    "# # query={\"patent_year\":2016}\n",
    "# query = {\"_and\":[{\"_gte\":{\"patent_date\":\"2017-01-01\"}},{\"_lte\":{\"patent_date\":\"2017-01-31\"}}]}\n",
    "# fields=pat_fields\n",
    "# options={\"per_page\":2500}\n",
    "# sort=[{\"patent_date\":\"desc\"}]\n",
    "\n",
    "# params={'q': json.dumps(query),\n",
    "#         'f': json.dumps(fields),\n",
    "#         'o': json.dumps(options),\n",
    "#         's': json.dumps(sort)}\n",
    "\n",
    "# # request and results\n",
    "# resp = requests.get(endpoint_url, params=params)\n",
    "# results = resp.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build function to iteratively query based on date range\n",
    "# month range TODO (Lee)\n",
    "page_counter=1\n",
    "agg_data = []\n",
    "for i in range(40):\n",
    "    try:\n",
    "        query = {\"_and\":[{\"_gte\":{\"patent_date\":\"2017-01-01\"}},{\"_lte\":{\"patent_date\":\"2017-01-31\"}}]}\n",
    "        fields=pat_fields\n",
    "        options={\"page\": page_counter, \"per_page\":2500}\n",
    "        sort=[{\"patent_date\":\"desc\"}]\n",
    "\n",
    "        params={'q': json.dumps(query),\n",
    "                'f': json.dumps(fields),\n",
    "                'o': json.dumps(options),\n",
    "                's': json.dumps(sort)}\n",
    "\n",
    "        # request and results\n",
    "        resp = requests.get(endpoint_url, params=params)\n",
    "        results = resp.json()\n",
    "        agg_data.append(results)\n",
    "\n",
    "        page_counter+=1\n",
    "    except:\n",
    "        if resp.status_code!=\"200\":\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patents endpoint\n",
    "endpoint_url = 'http://www.patentsview.org/api/patents/query'\n",
    "\n",
    "# build list of possible fields that endpoint request will return\n",
    "df = pd.read_excel(\"data/patents_view_patents_fields.xlsx\")\n",
    "df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\n",
    "pat_fields = df.api_field_name.values.tolist()\n",
    "\n",
    "# build query\n",
    "query={\"cpc_subgroup_id\":\"G06T3/4046\"}\n",
    "# query = {\"_and\":[{\"_gte\":{\"patent_date\":\"2017-01-01\"}},{\"_lte\":{\"patent_date\":\"2017-01-31\"}}]}\n",
    "fields=pat_fields\n",
    "options={\"per_page\":2500}\n",
    "sort=[{\"patent_date\":\"desc\"}]\n",
    "\n",
    "params={'q': json.dumps(query),\n",
    "        'f': json.dumps(fields),\n",
    "        'o': json.dumps(options),\n",
    "        's': json.dumps(sort)}\n",
    "\n",
    "# request and results\n",
    "resp = requests.get(endpoint_url, params=params)\n",
    "results = resp.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CPC fields for block 1 of query:\n",
    "Y10S-706 OR \n",
    "G06N-003 OR \n",
    "G06N-005/003:G06N-005/027 OR \n",
    "G06N- 007/005:G06N-007/06 OR \n",
    "G06N-099/005 OR\n",
    "G06T2207/20081 OR\n",
    "G06T2207/20084 OR\n",
    "G06T-003/4046 OR\n",
    "G06T-009/002 OR\n",
    "G06F-017/16 OR\n",
    "G05B-013/027 OR\n",
    "G05B- 013/0275 OR\n",
    "G05B-013/028 OR\n",
    "G05B-013/0285 OR\n",
    "G05B-013/029 OR\n",
    "G05B-013/0295 OR\n",
    "G05B-2219/33002 OR\n",
    "G05D-001/0088 OR\n",
    "G06K-009 OR\n",
    "G10L-015 OR\n",
    "G10L-017 OR\n",
    "G06F-017/27:G06F-017/2795 OR\n",
    "G06F-017/28:G06F-017/289 OR\n",
    "G06F-017/30029:G06F- 017/30035 OR\n",
    "G06F-017/30247:G06F-017/30262 OR \n",
    "G06F-017/30401 OR\n",
    "G06F-017/3043 OR \n",
    "G06F-017/30522:G06F-017/3053 OR \n",
    "G06F-017/30654 OR \n",
    "G06F-017/30663 OR\n",
    "G06F-017/30666 OR \n",
    "G06F-017/30669 OR\n",
    "G06F-017/30672 OR \n",
    "G06F-017/30684 OR\n",
    "G06F-017/30687 OR \n",
    "G06F-017/3069 OR \n",
    "G06F-017/30702 OR\n",
    "G06F-017/30705:G06F- 017/30713 OR\n",
    "G06F-017/30731:G06F-017/30737 OR\n",
    "G06F-017/30743:G06F-017/30746 OR \n",
    "G06F-017/30784:G06F-017/30814 OR\n",
    "G06F-019/24 OR G06F-019/707 OR\n",
    "G01R- 031/2846:G01R-031/2848 OR\n",
    "G01N-2201/1296 OR\n",
    "G01N-029/4481 OR\n",
    "G01N-033/0034 ORG01R-031/3651ORG01S-007/417ORG06N-003/004:G06N-003/008 ORG06F- 011/1476 OR \n",
    "G06F-011/2257 OR \n",
    "G06F-011/2263 OR \n",
    "G06F-015/18 OR\n",
    "G06F-2207/4824 OR\n",
    "G06K-007/1482 OR\n",
    "G06N-007/046 OR\n",
    "G11B-020/10518 OR\n",
    "G10H-2250/151 OR\n",
    "G10H-2250/311 OR\n",
    "G10K-2210/3024 OR\n",
    "H01J-2237/30427 OR\n",
    "H01M-008/04992 OR\n",
    "H02H-001/0092 OR\n",
    "H02P-021/0014 OR\n",
    "H02P-023/0018 OR\n",
    "H03H-2017/0208 OR\n",
    "H03H- 2222/04 OR\n",
    "H04L-2012/5686 OR\n",
    "H04L-2025/03464 OR\n",
    "H04L-2025/03554 OR\n",
    "H04L- 025/0254 OR\n",
    "H04L-025/03165 OR\n",
    "H04L-041/16 OR\n",
    "H04L-045/08 OR\n",
    "H04N- 021/4662:H04N-021/4666 OR\n",
    "H04Q-2213/054 \n",
    "OR H04Q-2213/13343 OR\n",
    "H04Q-2213/343 OR\n",
    "H04R-025/507 OR\n",
    "G08B-029/186 OR\n",
    "B60G-2600/1876 OR\n",
    "B60G-2600/1878 OR\n",
    "B60G-2600/1879 OR\n",
    "B64G-2001/247 OR\n",
    "E21B-2041/0028 OR\n",
    "B23K-031/006 OR\n",
    "B29C- 2945/76979 OR\n",
    "B29C-066/965 OR\n",
    "B25J-009/161 OR\n",
    "A61B-005/7264:A61B-005/7267 OR\n",
    "Y10S-128/924 OR\n",
    "Y10S-128/925 OR\n",
    "F02D-041/1405 OR\n",
    "F03D-007/046 OR\n",
    "F05B- 2270/707 OR\n",
    "F05B-2270/709 OR\n",
    "F16H-2061/0081 OR\n",
    "F16H-2061/0084 OR\n",
    "B60W-030/06 OR\n",
    "B60W-030/10:B60W-030/12 OR\n",
    "B60W-030/14:B60W-030/17 OR\n",
    "B62D-015/0285 OR\n",
    "G06T-2207/30248:G06T-2207/30268 OR\n",
    "G06T-2207/30236 OR G05D-001 OR\n",
    "A61B- 005/7267 OR\n",
    "F05D-2270/709 OR\n",
    "G06T-2207/20084 OR\n",
    "G10K-2210/3038 OR\n",
    "G10L-025/30 OR\n",
    "H04N-021/4666 OR\n",
    "A63F-013/67 OR\n",
    "G06F-017/2282"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Structure data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract metadata from response\n",
    "print(\"status code:\", resp.status_code,';', \"reason:\", resp.reason)\n",
    "total_patent_count = results[\"total_patent_count\"]\n",
    "patents_per_page = results['count']\n",
    "print(\"total_patent_count:\",total_patent_count,';', \"patents_per_page:\", patents_per_page)\n",
    "\n",
    "# extract data from response\n",
    "data_resp = results['patents']\n",
    "# data_resp[0]\n",
    "\n",
    "raw_df = pd.DataFrame(data_resp)\n",
    "raw_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import data from bulk download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to download TSV files containing detailed patent descriptions from PatentsView \n",
    "# !wget http://data.patentsview.org/detail-description-text/detail-desc-text-2016.tsv.zip # 2016 - 3.0 GB zipped\n",
    "# !wget http://data.patentsview.org/detail-description-text/detail-desc-text-2017.tsv.zip # 2017 - 2.8 GB zipped\n",
    "# !wget http://data.patentsview.org/detail-description-text/detail-desc-text-2018.tsv.zip # 2018 - 1.6 GB zipped\n",
    "# !wget http://data.patentsview.org/detail-description-text/detail-desc-text-2019.tsv.zip # 2019 - 0.7 GB zipped\n",
    "\n",
    "# !unzip files\n",
    "# unzip detail-desc-text-2016.tsv.zip\n",
    "# unzip detail-desc-text-2017.tsv.zip\n",
    "# unzip detail-desc-text-2018.tsv.zip\n",
    "# unzip detail-desc-text-2019.tsv.zip\n",
    "\n",
    "def convert_bytes(num, suffix='B'):\n",
    "    \"\"\" convert bytes int to int in aggregate units\"\"\"\n",
    "    for unit in ['','K','M','G','T','P','E','Z']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f%s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
    "\n",
    "path = \"data/\"\n",
    "with os.scandir(path) as it:\n",
    "    for entry in it:\n",
    "        if not entry.name.startswith('.') and entry.is_file():\n",
    "            print(entry.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect unzipped file sizes\n",
    "convert_bytes(os.path.getsize(\"data/detail-desc-text-2016.tsv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_bytes(os.path.getsize(\"data/detail-desc-text-2017.tsv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_bytes(os.path.getsize(\"data/detail-desc-text-2018.tsv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_bytes(os.path.getsize(\"data/detail_desc_text_2019.tsv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SparkSession/SparkContext as entry point to Dataset/DataFrame API\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from functools import reduce\n",
    "\n",
    "# df = reduce(lambda x,y: x.unionAll(y), \n",
    "#             [spark.read.format('csv')\n",
    "#                        .load(f, header=\"true\", inferSchema=\"true\") \n",
    "#              for f in files])\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"data/detail-desc-text-2016.tsv\", \"data/detail-desc-text-2017.tsv\", \n",
    "         \"data/detail-desc-text-2018.tsv\", \"data/detail-desc-text-2019.tsv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2018 = (spark.read\n",
    "               .format(\"csv\")\n",
    "               .option(\"delimiter\", \"\\t\")\n",
    "               .option('inferSchema', \"true\")\n",
    "               .load(\"data/detail-desc-text-2018.tsv\")\n",
    "               .write\n",
    "               .format(\"parquet\")\n",
    "               .save(\"df_2018.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2018 = (spark.read\n",
    "               .format(\"csv\")\n",
    "               .option(\"delimiter\", \"\\t\")\n",
    "               .option('inferSchema', \"true\")\n",
    "               .load(\"data/detail-desc-text-2018.tsv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp_2018 = pd.read_csv(\"data/detail-desc-text-2018.tsv\", sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp_2018.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp_2018.columns = ['patent_number', 'desc_detail', 'len_detail']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dfp_2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp_2018.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp_2018_nl = dfp_2018.filter(like='natural language')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp_2018_nl.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp_2018_nl = dfp_2018[dfp_2018['desc_detail'].str.contains('NLP')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dfp_2018_nl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2855/160000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2018.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "            StructField(\"_c0\", IntegerType(), True),\n",
    "            StructField(\"_c1\", StringType(), True),\n",
    "            StructField(\"_c2\", IntegerType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2018 = spark.read.load('data/df_2018.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 160,249 rows in 2018 dataset\n",
    "df_2018.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2018.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partition / batching ?\n",
    "df_2018.filter(df_2018._c1.contains(\"natural language\")).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query file directly with SQL\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT * FROM parquet.`data/df_2018.parquet` WHERE _c1 LIKE 'natural language' LIMIT 100\n",
    "\"\"\"\n",
    "\n",
    "df_2018_nl = spark.sql(query)\n",
    "\n",
    "df_2018_nl.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2018.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2018.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2018.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2018.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_171819 = df_2017.union(df_2018).union(df_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_171819.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_171819.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2018.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = df_2018.agg(F.countDistinct('_c0'))\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df.createOrReplaceTempView('reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(output, n=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = spark.sql(\n",
    "  \"SELECT * FROM people\")\n",
    "names = results.map(lambda p: p.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rdd.isEmpty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark.read\n",
    "            .load(\"data/*.parquet\")\n",
    "            .write\n",
    "            .format(\"parquet\")\n",
    "            .save(\"df.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2019 = (spark.read\n",
    "               .format(\"csv\")\n",
    "               .option(\"delimiter\", \"\\t\")\n",
    "               .option('inferSchema', \"true\")\n",
    "               .load(\"data/detail-desc-text-2018.tsv\")\n",
    "               .write\n",
    "               .format(\"parquet\")\n",
    "               .save(\"df_2019.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2019.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2016.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2016 = (spark.read.format(\"csv\")\n",
    "               .option(\"delimiter\", \"\\t\")\n",
    "               .option(\"header\", \"true\")\n",
    "               .option('inferSchema', \"true\")\n",
    "               .load(\"data/detail-desc-text-2016.tsv\")\n",
    "               .write\n",
    "               .format(\"parquet\")\n",
    "               .save(\"data/df_2016.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2016.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark.read.format(\"csv\")\n",
    "           .option(\"delimiter\", \",\")\n",
    "           .infer\n",
    "           .load(\"data/df.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2017 = (spark.read.format(\"csv\")\n",
    "               .option(\"delimiter\", \"\\t\")\n",
    "               .option(\"header\", \"true\")\n",
    "               .option('inferSchema', \"true\")\n",
    "               .load(\"data/detail-desc-text-2017.tsv\")\n",
    "               .write\n",
    "               .format(\"parquet\")\n",
    "               .save(\"df_2017.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2018 = spark.read.parquet(\"data/df_2018.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2018.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2018.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2018.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2018.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subset dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset dataframe - comment/uncomment to include fields\n",
    "df = raw_df[['patent_number', \n",
    "         'patent_date', \n",
    "         'patent_title',\n",
    "         'patent_abstract', \n",
    "         'patent_firstnamed_assignee_id',\n",
    "         'patent_firstnamed_assignee_location_id',\n",
    "         'patent_firstnamed_assignee_latitude',\n",
    "         'patent_firstnamed_assignee_longitude',\n",
    "         'patent_firstnamed_assignee_city',\n",
    "         'patent_firstnamed_assignee_state',\n",
    "         'patent_firstnamed_assignee_country', \n",
    "         'patent_firstnamed_inventor_id',\n",
    "         'patent_firstnamed_inventor_location_id',\n",
    "         'patent_firstnamed_inventor_latitude',\n",
    "         'patent_firstnamed_inventor_longitude',\n",
    "         'patent_firstnamed_inventor_city',\n",
    "         'patent_firstnamed_inventor_state',\n",
    "         'patent_firstnamed_inventor_country',\n",
    "         'patent_year', \n",
    "         'patent_type', \n",
    "         'patent_kind',\n",
    "         'inventors'\n",
    "#          'patent_processing_time', \n",
    "#          'patent_num_us_application_citations', \n",
    "#          'patent_num_us_patent_citations', \n",
    "#          'patent_num_foreign_citations', \n",
    "#          'patent_num_combined_citations', \n",
    "#          'patent_num_claims', \n",
    "#          'patent_num_cited_by_us_patents',\n",
    "#          'detail_desc_length'\n",
    "            ]]\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 561 different assignees\n",
    "len(df.patent_firstnamed_assignee_id.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new column that combines the patent title and the patent abstract columns into a single string\n",
    "df['patent_title_abstract'] = df.patent_title + ' ' + df.patent_abstract\n",
    "df.patent_title_abstract.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TODO (Lee) Partition data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.patent_number.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=['patent_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = df.patent_title_abstract.tolist()\n",
    "text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partition data\n",
    "len(text_data)\n",
    "text_train = text_data[:round(len(text_data)*.8)]\n",
    "text_test = text_data[round(len(text_data)*.8):]\n",
    "print(len(text_data), len(text_train), len(text_test), len(text_train)+len(text_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to download stop words from nltk and language package from spacy\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# !python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct pipeline using Spacy Language object and associated pipeline/components\n",
    "nlp = spacy.load(\"en\")\n",
    "pprint(nlp.pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = []   \n",
    "\n",
    "# process patent documents in pipeline\n",
    "for doc in nlp.pipe(text_train, n_threads=4, batch_size=100):\n",
    "   \n",
    "    ents = doc.ents  # Named entities.\n",
    "\n",
    "    # Keep only words (no numbers, no punctuation).\n",
    "    # Lemmatize tokens, remove punctuation and remove stopwords.\n",
    "    doc = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
    "\n",
    "    # Remove common words from a stopword list.\n",
    "    doc = [token for token in doc if token not in stop_words]\n",
    "\n",
    "    # Add named entities, but only if they are a compound of more than word.\n",
    "    doc.extend([str(entity) for entity in ents if len(entity) > 1])\n",
    "    \n",
    "    processed_docs.append(doc)\n",
    "\n",
    "processed_docs[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[token.text for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = set([w.label_ for w in doc.ents]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in labels: \n",
    "    entities = [cleanup(e.string, lower=False) for e in document.ents if label==e.label_] \n",
    "    entities = list(set(entities)) \n",
    "    print(label,entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processed_docs = []\n",
    "for doc in nlp.pipe(docs, n_threads=4, batch_size=100):\n",
    "    # Process document using Spacy NLP pipeline.\n",
    "    \n",
    "    ents = doc.ents  # Named entities.\n",
    "\n",
    "    # Keep only words (no numbers, no punctuation).\n",
    "    # Lemmatize tokens, remove punctuation and remove stopwords.\n",
    "    doc = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
    "\n",
    "    # Remove common words from a stopword list.\n",
    "    #doc = [token for token in doc if token not in STOPWORDS]\n",
    "\n",
    "    # Add named entities, but only if they are a compound of more than word.\n",
    "    doc.extend([str(entity) for entity in ents if len(entity) > 1])\n",
    "    \n",
    "    pre_processed_docs.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize documents\n",
    "\n",
    "def tokenize_docs(docs):\n",
    "    tokenized_docs = []\n",
    "    for doc in docs:\n",
    "        tokenized_docs.append(word_tokenize(doc))\n",
    "    return tokenized_docs\n",
    "\n",
    "tokenized_docs = tokenize_docs(text_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean punctuation\n",
    "def clean_docs(tokenized_docs):\n",
    "    clean_docs = []\n",
    "    for doc in tokenized_docs:\n",
    "       clean_docs.append([word for word in doc if word.isalpha()])  \n",
    "    return clean_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = clean_docs(tokenized_docs)\n",
    "cleaned_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to lowercase\n",
    "def lower_words(docs):\n",
    "    lowered_words = []\n",
    "    for doc in docs:\n",
    "        lowered_words.append([word.lower() for word in doc])\n",
    "    return lowered_words\n",
    "\n",
    "lowered_data = lower_words(cleaned_data)\n",
    "lowered_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_stopwords(docs):\n",
    "    filtered_docs = []\n",
    "    for doc in docs:\n",
    "       filtered_docs.append([word for word in doc if word not in stop_words])\n",
    "    return filtered_docs\n",
    "\n",
    "# remove stopwords\n",
    "filtered_data = filter_stopwords(lowered_data)\n",
    "filtered_data\n",
    "# TODO (Lee) - resolve un-lowered stopwords \"A\" and \"An\", 'By', 'The'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct bigrams and trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train bigram phrases model\n",
    "bigram_model = Phrases(filtered_data, min_count=1, threshold=1)\n",
    "\n",
    "# train trigram phrases model\n",
    "trigram_model = Phrases(bigram_model[filtered_data], threshold=100)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigrams\n",
    "def bigrams(docs):\n",
    "    \"\"\"create bigrams\"\"\"\n",
    "    return [bigram_model[doc] for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize bigram and trigram models\n",
    "bigram_model = gensim.models.phrases.Phraser(bigram_model)\n",
    "trigram_model = gensim.models.phrases.Phraser(trigram_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams(filtered_data)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigrams(docs):\n",
    "    \"\"\"create trigrams\"\"\"\n",
    "    return [trigram_model[bigram_model[doc]] for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams(filtered_data)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stem and Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_docs(docs, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"lemmatize documents\"\"\"\n",
    "    lemmatized_docs = []\n",
    "    for doc in docs: \n",
    "        lemmatized_docs.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return lemmatized_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Lee)\n",
    "\n",
    "lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)\n",
    "# for doc in cleaned_data:\n",
    "#     for token in doc:\n",
    "#         token.lemma_\n",
    "\n",
    "# uncomment to use\n",
    "# download english model with \"python -m spacy download en\"\n",
    "\n",
    "# for token in doc:\n",
    "#     print(token, token.lemma, token.lemma_)\n",
    "\n",
    "# TODO (Lee) - lemmatize_docs(cleaned_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create corpus and dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using spacy pipeline components\n",
    "# build dictionary\n",
    "id_to_word = corpora.Dictionary(processed_docs)\n",
    "\n",
    "# build corpus\n",
    "texts = processed_docs\n",
    "\n",
    "# apply term document frequency\n",
    "# converts documents in corpus to bag-of-words format, a list of (token_id, token_count) tuples\n",
    "corpus = [id_to_word.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # build dictionary\n",
    "id_to_word = corpora.Dictionary(filtered_data)\n",
    "\n",
    "# build corpus\n",
    "texts = filtered_data\n",
    "\n",
    "# apply term document frequency\n",
    "# converts documents in corpus to bag-of-words format, a list of (token_id, token_count) tuples\n",
    "corpus = [id_to_word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view formatted corpus (term-doc-frequency)\n",
    "[[(id_to_word[id], freq) for id, freq in text] for text in corpus][:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model - model #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Lee) - deprecation warnings\n",
    "# construct LDA model\n",
    "model_lda = LdaModel(corpus=corpus,\n",
    "                     id2word=id_to_word,\n",
    "                     num_topics=25, \n",
    "                     random_state=100,\n",
    "                     update_every=1,\n",
    "                     chunksize=100,\n",
    "                     passes=10,\n",
    "                     alpha='auto',\n",
    "                     per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print keywords in n topics\n",
    "pprint(model_lda.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print top 10 keywords that comprise topic with index of 0\n",
    "pprint(model_lda.print_topic(24))\n",
    "# the most import keywords, and the respective weight, that form topic 0 are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print top 10 keywords that comprise topic with index of 1\n",
    "pprint(model_lda.print_topic(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Lee) - infer topic from keywords?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate - model #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate perplexity metrics\n",
    "perplexity = model_lda.log_perplexity(corpus)\n",
    "perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Lee) - confirm that filtered_data is indeed the correct dataset to pass to texts param\n",
    "# calculate coherence metric\n",
    "coherence = CoherenceModel(model=model_lda, texts=processed_docs, dictionary=id_to_word, coherence='c_v')\n",
    "coherence_1 = coherence.get_coherence()\n",
    "coherence_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Lee) - confirm that filtered_data is indeed the correct dataset to pass to texts param\n",
    "# calculate coherence metric\n",
    "coherence = CoherenceModel(model=model_lda, texts=filtered_docs, dictionary=id_to_word, coherence='c_v')\n",
    "coherence_1 = coherence.get_coherence()\n",
    "coherence_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate coherence metric or each of the n topicss\n",
    "coherence_1 = coherence.get_coherence_per_topic()\n",
    "coherence_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore topics\n",
    "pyLDAvis.enable_notebook()\n",
    "viz_topics_1 = pyLDAvis.gensim.prepare(model_lda, corpus, id_to_word)\n",
    "viz_topics_1\n",
    "# TODO (Lee) - salient vs relevant terms in pyLDA ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2-  Mallet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to download Mallet topic model\n",
    "# !wget http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip\n",
    "# update this path\n",
    "path_mallet = 'data/mallet-2.0.8/bin/mallet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = gensim.models.wrappers.LdaMallet(path_mallet, corpus=corpus, num_topics=25, id2word=id_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topics\n",
    "pprint(model_2.show_topics(formatted=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate coherence metric\n",
    "coherence_model_2 = CoherenceModel(model=model_2, texts=data, dictionary=id_to_word, coherence='c_v')\n",
    "coherence_model_2 = coherence_model_2.get_coherence()\n",
    "coherence_model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Lee)\n",
    "# def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "#     \"\"\"\n",
    "#     Compute c_v coherence for various number of topics\n",
    "\n",
    "#     Parameters:\n",
    "#     ----------\n",
    "#     dictionary : Gensim dictionary\n",
    "#     corpus : Gensim corpus\n",
    "#     texts : List of input texts\n",
    "#     limit : Max num of topics\n",
    "\n",
    "#     Returns:\n",
    "#     -------\n",
    "#     model_list : List of LDA topic models\n",
    "#     coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "#     \"\"\"\n",
    "#     coherence_values = []\n",
    "#     model_list = []\n",
    "#     for num_topics in range(start, limit, step):\n",
    "#         model = gensim.models.wrappers.LdaMallet(path_mallet, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "#         model_list.append(model)\n",
    "#         coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "#         coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "#     return model_list, coherence_values\n",
    "\n",
    "# model_list, coherence_values = compute_coherence_values(dictionary=id_to_word, corpus=corpus, texts=data, start=2, limit=40, step=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3 - Author topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct inventor-to-doc mapping as df from nested inventors column in json api response\n",
    "df_inventors = json_normalize(results['patents'], record_path=['inventors'], meta=['patent_number', 'patent_date'])\n",
    "df_inventors = df_inventors[['inventor_id', 'patent_number', 'patent_date']]\n",
    "df_inventors.sort_values(by=['patent_date'])\n",
    "df_inventors.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Lee) - resolve workaround\n",
    "df_idx = df\n",
    "df_idx['idx'] = df.index\n",
    "df_idx\n",
    "df_idx_1 = df_idx[['patent_number', 'idx', 'inventors']]\n",
    "df_idx_2 = df_idx_1.set_index('patent_number')\n",
    "df_idx_2.pop('inventors')\n",
    "df_idx_2\n",
    "df_pat_idx = df_idx_2.T.to_dict('records')\n",
    "for i in df_pat_idx:\n",
    "    df_pat_idx = dict(i)\n",
    "df_pat_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pat_idx = df_idx_2.T.to_dict('records')\n",
    "for i in df_pat_idx:\n",
    "    df_pat_idx = dict(i)\n",
    "df_pat_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inv_test = json_normalize(results['patents'], record_path=['inventors'], meta=['patent_number', 'patent_date'])\n",
    "df_inv_test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idx_pat_inv_map = df[['patent_number', 'inventors']]\n",
    "df_idx_pat_inv_map.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Lee) - find out how to get list of patents_view_field names from API - I did it accidentally but need to replicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.patent_title_abstract[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inventors.set_index('inventor_id').T.to_dict('list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k, v in pat2inv.items():\n",
    "#     name_dict[new_key] = name_dict.pop(k)\n",
    "#     time.sleep(4)\n",
    "\n",
    "# pprint.pprint(name_dict)\n",
    "\n",
    "# d = {'x':1, 'y':2, 'z':3}\n",
    "# d1 = {'x':'a', 'y':'b', 'z':'c'}\n",
    "\n",
    "# dict((d1[key], value) for (key, value) in d.items())\n",
    "# {'a': 1, 'b': 2, 'c': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patdf2inv = dict((df_pat_idx[key], value) for (key, value) in pat2inv.items())\n",
    "patdf2inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat2inv = {k: list(v) for k,v in df_inventors.groupby(\"patent_number\")[\"inventor_id\"]}\n",
    "pat2inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "idx_pat_map = df.patent_number.to_dict()\n",
    "idx_pat_map = {str(key): value for key, value in idx_pat_map.items()}\n",
    "idx_pat_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct author-topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct author-topic model\n",
    "model_at = AuthorTopicModel(corpus=corpus,\n",
    "                         doc2author=patdf2inv,\n",
    "                         id2word=id_to_word, \n",
    "                         num_topics=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct vectors for authors\n",
    "author_vecs = [model_at.get_author_topics(author) for author in model_at.id2author.values()]\n",
    "author_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the topic distribution for an author using use model[name] syntax\n",
    "# each topic has a probability of being expressed given the particular author, but only the ones above a certain threshold are shown.\n",
    "\n",
    "model_at['7788103-1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def show_author(name):\n",
    "#     print('\\n%s' % name)\n",
    "#     print('Docs:', model.author2doc[name])\n",
    "#     print('Topics:')\n",
    "#     pprint([(topic_labels[topic[0]], topic[1]) for topic in model[name]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate per-word bound, which is a measure of the model's predictive performance (reconstruction error?)\n",
    "\n",
    "build doc2author dictionary\n",
    "\n",
    "doc2author = atmodel.construct_doc2author(model.corpus, model.author2doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import atmodel\n",
    "doc2author = atmodel.construct_doc2author(model.corpus, model.author2doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim.models.atmodel.construct_author2doc(doc2author)\n",
    "# construct mapping from author IDs to document IDs.\n",
    "\n",
    "Parameters:\tdoc2author (dict of (int, list of str))  Mapping of document id to authors.\n",
    "Returns:\tMapping of authors to document ids.\n",
    "Return type:\tdict of (str, list of int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim.models.atmodel.construct_doc2author(corpus, author2doc)\n",
    "construct mapping from document IDs to author IDs\n",
    "\n",
    "Parameters:\t\n",
    "corpus (iterable of list of (int, float))  Corpus in BoW format.\n",
    "author2doc (dict of (str, list of int))  Mapping of authors to documents.\n",
    "Returns:\t\n",
    "Document to Author mapping.\n",
    "\n",
    "Return type:\t\n",
    "dict of (int, list of str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
