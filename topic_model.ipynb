{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Techniche - Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/glmack_agoralytix_com/.local/lib/python3.7/site-packages/py4j/java_collections.py:13: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import (\n",
      "/home/glmack_agoralytix_com/.local/lib/python3.7/site-packages/py4j/java_collections.py:13: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import (\n",
      "/home/glmack_agoralytix_com/.local/lib/python3.7/site-packages/py4j/java_collections.py:13: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import (\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.corpora import mmcorpus\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models import AuthorTopicModel\n",
    "from gensim.test.utils import common_dictionary, datapath, temporary_file\n",
    "from smart_open import smart_open\n",
    "\n",
    "import spacy\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, punkt, RegexpTokenizer, wordpunct_tokenize\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
    "\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "\n",
    "from topic_model import tokenize_docs#, (TODO) Lee convert_bytes\n",
    "\n",
    "from smart_open import smart_open\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import pyspark\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_colwidth', -1)\n",
    "pd.options.display.max_columns = 50\n",
    "pd.set_option('display.max_rows', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to download stop words from nltk and language package from spacy\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# !python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import data from PatentsView API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patents endpoint\n",
    "endpoint_url = 'http://www.patentsview.org/api/patents/query'\n",
    "\n",
    "# build list of possible fields that endpoint request will return\n",
    "df = pd.read_excel(\"data/patents_view_patents_fields.xlsx\")\n",
    "df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\n",
    "pat_fields = df.api_field_name.values.tolist()\n",
    "\n",
    "# build query\n",
    "query={\"_or\":[{\"_text_phrase\":{\"patent_title\":\"natural language\"}},{\"_text_phrase\":{\"patent_abstract\":\"natural language\"}}]}\n",
    "fields=pat_fields\n",
    "options={\"per_page\":2500}\n",
    "sort=[{\"patent_date\":\"desc\"}]\n",
    "\n",
    "params={'q': json.dumps(query),\n",
    "        'f': json.dumps(fields),\n",
    "        'o': json.dumps(options),\n",
    "        's': json.dumps(sort)}\n",
    "\n",
    "# request and results\n",
    "resp = requests.get(endpoint_url, params=params)\n",
    "results = resp.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patents endpoint\n",
    "endpoint_url = 'http://www.patentsview.org/api/patents/query'\n",
    "\n",
    "# build list of possible fields that endpoint request will return\n",
    "df = pd.read_excel(\"data/patents_view_patents_fields.xlsx\")\n",
    "df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\n",
    "pat_fields = df.api_field_name.values.tolist()\n",
    "\n",
    "# build query\n",
    "query={\"_and\":\n",
    "        [{\"_or\":\n",
    "            [{\"_text_phrase\":{\"patent_title\":\"natural language\"}}\n",
    "            ,{\"_text_phrase\":{\"patent_abstract\":\"natural language\"}}]}\n",
    "        ,{\"_and\":\n",
    "      [{\"patent_year\":2016}]}]} \n",
    "\n",
    "fields=pat_fields\n",
    "options={\"per_page\":2500}\n",
    "sort=[{\"patent_date\":\"desc\"}]\n",
    "\n",
    "params={'q': json.dumps(query),\n",
    "        'f': json.dumps(fields),\n",
    "        'o': json.dumps(options),\n",
    "        's': json.dumps(sort)}\n",
    "\n",
    "# request and results\n",
    "resp = requests.get(endpoint_url, params=params)\n",
    "results = resp.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patents endpoint\n",
    "endpoint_url = 'http://www.patentsview.org/api/patents/query'\n",
    "\n",
    "# build list of possible fields that endpoint request will return\n",
    "df = pd.read_excel(\"data/patents_view_patents_fields.xlsx\")\n",
    "df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\n",
    "pat_fields = df.api_field_name.values.tolist()\n",
    "\n",
    "# build query\n",
    "query={\"patent_year\":2016}\n",
    "fields=pat_fields\n",
    "options={\"per_page\":2500}\n",
    "sort=[{\"patent_date\":\"desc\"}]\n",
    "\n",
    "params={'q': json.dumps(query),\n",
    "        'f': json.dumps(fields),\n",
    "        'o': json.dumps(options),\n",
    "        's': json.dumps(sort)}\n",
    "\n",
    "# request and results\n",
    "resp = requests.get(endpoint_url, params=params)\n",
    "results = resp.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Structure data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "status code: 200 ; reason: OK\n",
      "total_patent_count: 100000 ; patents_per_page: 2500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IPCs</th>\n",
       "      <th>application_citations</th>\n",
       "      <th>applications</th>\n",
       "      <th>assignees</th>\n",
       "      <th>cited_patents</th>\n",
       "      <th>citedby_patents</th>\n",
       "      <th>cpcs</th>\n",
       "      <th>detail_desc_length</th>\n",
       "      <th>examiners</th>\n",
       "      <th>foreign_priority</th>\n",
       "      <th>...</th>\n",
       "      <th>patent_num_us_patent_citations</th>\n",
       "      <th>patent_number</th>\n",
       "      <th>patent_processing_time</th>\n",
       "      <th>patent_title</th>\n",
       "      <th>patent_type</th>\n",
       "      <th>patent_year</th>\n",
       "      <th>pct_data</th>\n",
       "      <th>rawinventors</th>\n",
       "      <th>uspcs</th>\n",
       "      <th>wipos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'ipc_action_date': '2016-12-27', 'ipc_class'...</td>\n",
       "      <td>[{'appcit_app_number': None, 'appcit_category'...</td>\n",
       "      <td>[{'app_country': 'US', 'app_date': '2015-05-22...</td>\n",
       "      <td>[{'assignee_city': None, 'assignee_country': N...</td>\n",
       "      <td>[{'cited_patent_category': 'cited by examiner'...</td>\n",
       "      <td>[{'citedby_patent_category': None, 'citedby_pa...</td>\n",
       "      <td>[{'cpc_category': 'inventional', 'cpc_first_se...</td>\n",
       "      <td>8938</td>\n",
       "      <td>[{'examiner_first_name': 'Paul T', 'examiner_i...</td>\n",
       "      <td>[{'forprior_country': None, 'forprior_date': N...</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>9526196</td>\n",
       "      <td>585</td>\n",
       "      <td>Multi-purpose spade having detachable tip</td>\n",
       "      <td>utility</td>\n",
       "      <td>2016</td>\n",
       "      <td>[{'pct_102_date': None, 'pct_371_date': None, ...</td>\n",
       "      <td>[{'rawinventor_first_name': 'Hyun C.', 'rawinv...</td>\n",
       "      <td>[{'uspc_first_seen_date': '2015-06-02', 'uspc_...</td>\n",
       "      <td>[{'wipo_field_id': '29', 'wipo_field_title': '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'ipc_action_date': '2016-12-27', 'ipc_class'...</td>\n",
       "      <td>[{'appcit_app_number': '2014/20140166320', 'ap...</td>\n",
       "      <td>[{'app_country': 'US', 'app_date': '2012-10-24...</td>\n",
       "      <td>[{'assignee_city': 'Uberlingen', 'assignee_cou...</td>\n",
       "      <td>[{'cited_patent_category': 'cited by examiner'...</td>\n",
       "      <td>[{'citedby_patent_category': None, 'citedby_pa...</td>\n",
       "      <td>[{'cpc_category': 'inventional', 'cpc_first_se...</td>\n",
       "      <td>12582</td>\n",
       "      <td>[{'examiner_first_name': 'Minh', 'examiner_id'...</td>\n",
       "      <td>[{'forprior_country': 'DE', 'forprior_date': '...</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>9526197</td>\n",
       "      <td>1525</td>\n",
       "      <td>Force-transmitting unit</td>\n",
       "      <td>utility</td>\n",
       "      <td>2016</td>\n",
       "      <td>[{'pct_102_date': None, 'pct_371_date': None, ...</td>\n",
       "      <td>[{'rawinventor_first_name': 'Artur', 'rawinven...</td>\n",
       "      <td>[{'uspc_first_seen_date': '2015-06-02', 'uspc_...</td>\n",
       "      <td>[{'wipo_field_id': '29', 'wipo_field_title': '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{'ipc_action_date': '2016-12-27', 'ipc_class'...</td>\n",
       "      <td>[{'appcit_app_number': '2002/20020081047', 'ap...</td>\n",
       "      <td>[{'app_country': 'US', 'app_date': '2013-08-21...</td>\n",
       "      <td>[{'assignee_city': 'Vaderstad', 'assignee_coun...</td>\n",
       "      <td>[{'cited_patent_category': 'cited by applicant...</td>\n",
       "      <td>[{'citedby_patent_category': 'cited by examine...</td>\n",
       "      <td>[{'cpc_category': 'inventional', 'cpc_first_se...</td>\n",
       "      <td>21117</td>\n",
       "      <td>[{'examiner_first_name': 'John G', 'examiner_i...</td>\n",
       "      <td>[{'forprior_country': 'SE', 'forprior_date': '...</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>9526198</td>\n",
       "      <td>1224</td>\n",
       "      <td>Suspension unit for agricultural implement, ag...</td>\n",
       "      <td>utility</td>\n",
       "      <td>2016</td>\n",
       "      <td>[{'pct_102_date': None, 'pct_371_date': None, ...</td>\n",
       "      <td>[{'rawinventor_first_name': 'Henrik', 'rawinve...</td>\n",
       "      <td>[{'uspc_first_seen_date': '2015-06-02', 'uspc_...</td>\n",
       "      <td>[{'wipo_field_id': '31', 'wipo_field_title': '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                IPCs  \\\n",
       "0  [{'ipc_action_date': '2016-12-27', 'ipc_class'...   \n",
       "1  [{'ipc_action_date': '2016-12-27', 'ipc_class'...   \n",
       "2  [{'ipc_action_date': '2016-12-27', 'ipc_class'...   \n",
       "\n",
       "                               application_citations  \\\n",
       "0  [{'appcit_app_number': None, 'appcit_category'...   \n",
       "1  [{'appcit_app_number': '2014/20140166320', 'ap...   \n",
       "2  [{'appcit_app_number': '2002/20020081047', 'ap...   \n",
       "\n",
       "                                        applications  \\\n",
       "0  [{'app_country': 'US', 'app_date': '2015-05-22...   \n",
       "1  [{'app_country': 'US', 'app_date': '2012-10-24...   \n",
       "2  [{'app_country': 'US', 'app_date': '2013-08-21...   \n",
       "\n",
       "                                           assignees  \\\n",
       "0  [{'assignee_city': None, 'assignee_country': N...   \n",
       "1  [{'assignee_city': 'Uberlingen', 'assignee_cou...   \n",
       "2  [{'assignee_city': 'Vaderstad', 'assignee_coun...   \n",
       "\n",
       "                                       cited_patents  \\\n",
       "0  [{'cited_patent_category': 'cited by examiner'...   \n",
       "1  [{'cited_patent_category': 'cited by examiner'...   \n",
       "2  [{'cited_patent_category': 'cited by applicant...   \n",
       "\n",
       "                                     citedby_patents  \\\n",
       "0  [{'citedby_patent_category': None, 'citedby_pa...   \n",
       "1  [{'citedby_patent_category': None, 'citedby_pa...   \n",
       "2  [{'citedby_patent_category': 'cited by examine...   \n",
       "\n",
       "                                                cpcs detail_desc_length  \\\n",
       "0  [{'cpc_category': 'inventional', 'cpc_first_se...               8938   \n",
       "1  [{'cpc_category': 'inventional', 'cpc_first_se...              12582   \n",
       "2  [{'cpc_category': 'inventional', 'cpc_first_se...              21117   \n",
       "\n",
       "                                           examiners  \\\n",
       "0  [{'examiner_first_name': 'Paul T', 'examiner_i...   \n",
       "1  [{'examiner_first_name': 'Minh', 'examiner_id'...   \n",
       "2  [{'examiner_first_name': 'John G', 'examiner_i...   \n",
       "\n",
       "                                    foreign_priority  \\\n",
       "0  [{'forprior_country': None, 'forprior_date': N...   \n",
       "1  [{'forprior_country': 'DE', 'forprior_date': '...   \n",
       "2  [{'forprior_country': 'SE', 'forprior_date': '...   \n",
       "\n",
       "                         ...                          \\\n",
       "0                        ...                           \n",
       "1                        ...                           \n",
       "2                        ...                           \n",
       "\n",
       "  patent_num_us_patent_citations patent_number patent_processing_time  \\\n",
       "0                              8       9526196                    585   \n",
       "1                             16       9526197                   1525   \n",
       "2                             10       9526198                   1224   \n",
       "\n",
       "                                        patent_title patent_type patent_year  \\\n",
       "0          Multi-purpose spade having detachable tip     utility        2016   \n",
       "1                            Force-transmitting unit     utility        2016   \n",
       "2  Suspension unit for agricultural implement, ag...     utility        2016   \n",
       "\n",
       "                                            pct_data  \\\n",
       "0  [{'pct_102_date': None, 'pct_371_date': None, ...   \n",
       "1  [{'pct_102_date': None, 'pct_371_date': None, ...   \n",
       "2  [{'pct_102_date': None, 'pct_371_date': None, ...   \n",
       "\n",
       "                                        rawinventors  \\\n",
       "0  [{'rawinventor_first_name': 'Hyun C.', 'rawinv...   \n",
       "1  [{'rawinventor_first_name': 'Artur', 'rawinven...   \n",
       "2  [{'rawinventor_first_name': 'Henrik', 'rawinve...   \n",
       "\n",
       "                                               uspcs  \\\n",
       "0  [{'uspc_first_seen_date': '2015-06-02', 'uspc_...   \n",
       "1  [{'uspc_first_seen_date': '2015-06-02', 'uspc_...   \n",
       "2  [{'uspc_first_seen_date': '2015-06-02', 'uspc_...   \n",
       "\n",
       "                                               wipos  \n",
       "0  [{'wipo_field_id': '29', 'wipo_field_title': '...  \n",
       "1  [{'wipo_field_id': '29', 'wipo_field_title': '...  \n",
       "2  [{'wipo_field_id': '31', 'wipo_field_title': '...  \n",
       "\n",
       "[3 rows x 47 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract metadata from response\n",
    "print(\"status code:\", resp.status_code,';', \"reason:\", resp.reason)\n",
    "total_patent_count = results[\"total_patent_count\"]\n",
    "patents_per_page = results['count']\n",
    "print(\"total_patent_count:\",total_patent_count,';', \"patents_per_page:\", patents_per_page)\n",
    "\n",
    "# extract data from response\n",
    "data_resp = results['patents']\n",
    "# data_resp[0]\n",
    "raw_df = pd.DataFrame(data_resp)\n",
    "raw_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import data from bulk download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to download TSV files containing detailed patent descriptions from PatentsView \n",
    "# !wget http://data.patentsview.org/detail-description-text/detail-desc-text-2016.tsv.zip # 2016 - 3.0 GB zipped\n",
    "# !wget http://data.patentsview.org/detail-description-text/detail-desc-text-2017.tsv.zip # 2017 - 2.8 GB zipped\n",
    "# !wget http://data.patentsview.org/detail-description-text/detail-desc-text-2018.tsv.zip # 2018 - 1.6 GB zipped\n",
    "# !wget http://data.patentsview.org/detail-description-text/detail-desc-text-2019.tsv.zip # 2019 - 0.7 GB zipped\n",
    "\n",
    "# !unzip files\n",
    "# unzip detail-desc-text-2016.tsv.zip\n",
    "# unzip detail-desc-text-2017.tsv.zip\n",
    "# unzip detail-desc-text-2018.tsv.zip\n",
    "# unzip detail-desc-text-2019.tsv.zip\n",
    "\n",
    "def convert_bytes(num, suffix='B'):\n",
    "    \"\"\" convert bytes int to int in aggregate units\"\"\"\n",
    "    for unit in ['','K','M','G','T','P','E','Z']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f%s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
    "\n",
    "path = \"data/\"\n",
    "with os.scandir(path) as it:\n",
    "    for entry in it:\n",
    "        if not entry.name.startswith('.') and entry.is_file():\n",
    "            print(entry.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12.5GB'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect unzipped file sizes\n",
    "convert_bytes(os.path.getsize(\"data/detail-desc-text-2016.tsv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'13.8GB'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_bytes(os.path.getsize(\"data/detail-desc-text-2017.tsv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7.2GB'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_bytes(os.path.getsize(\"data/detail-desc-text-2018.tsv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0GB'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_bytes(os.path.getsize(\"data/detail_desc_text_2019.tsv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://my-fastai-instance.us-west2-b.c.weighty-elf-228123.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create SparkSession/SparkContext as entry point to Dataset/DataFrame API\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"data/detail-desc-text-2016.tsv\", \"data/detail-desc-text-2017.tsv\", \n",
    "         \"data/detail-desc-text-2018.tsv\", \"data/detail-desc-text-2019.tsv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = reduce(lambda x,y: x.unionAll(y), \n",
    "#             [spark.read.format('csv')\n",
    "#                        .load(f, header=\"true\", inferSchema=\"true\") \n",
    "#              for f in files])\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'path file:/home/glmack_agoralytix_com/techniche/df_2018.parquet already exists.;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o72.save.\n: org.apache.spark.sql.AnalysisException: path file:/home/glmack_agoralytix_com/techniche/df_2018.parquet already exists.;\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:114)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-da348db44752>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                \u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                .save(\"df_2018.parquet\"))\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'path file:/home/glmack_agoralytix_com/techniche/df_2018.parquet already exists.;'"
     ]
    }
   ],
   "source": [
    "df_2018 = (spark.read\n",
    "               .format(\"csv\")\n",
    "               .option(\"delimiter\", \"\\t\")\n",
    "               .option('inferSchema', \"true\")\n",
    "               .load(\"data/detail-desc-text-2018.tsv\")\n",
    "               .write\n",
    "               .format(\"parquet\")\n",
    "               .save(\"df_2018.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2018.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StructType' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-76a15ce11479>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m schema = StructType([\n\u001b[0m\u001b[1;32m      2\u001b[0m             \u001b[0mStructField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_c0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIntegerType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m             \u001b[0mStructField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_c1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStringType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             StructField(\"_c2\", IntegerType(), True)])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'StructType' is not defined"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "            StructField(\"_c0\", IntegerType(), True),\n",
    "            StructField(\"_c1\", StringType(), True),\n",
    "            StructField(\"_c2\", IntegerType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query file directly with SQL\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT _c0 FROM parquet.`data/df_2018.parquet` WHERE _c1 LIKE 'natural language' LIMIT 100\n",
    "\"\"\"\n",
    "\n",
    "df_2018_nl = spark.sql(query)\n",
    "\n",
    "df_2018_nl.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160249"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 160249 rows in 2018 dataset\n",
    "df_2018.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_c0', '_c1', '_c2']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2018.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) FileScan csv [_c0#10,_c1#11,_c2#12] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/glmack_agoralytix_com/techniche/data/detail-desc-text-2018.tsv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<_c0:string,_c1:string,_c2:string>\n"
     ]
    }
   ],
   "source": [
    "df_2018.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+------------------+\n",
      "|summary|                 _c0|                 _c1|               _c2|\n",
      "+-------+--------------------+--------------------+------------------+\n",
      "|  count|              160249|              160249|             37278|\n",
      "|   mean|1.0055075034692628E7|                null|47672.901443210474|\n",
      "| stddev|   80038.35207688982|                null|53306.603326141594|\n",
      "|    min|            10000000|#System-Minimal# ...|             10000|\n",
      "|    max|             RE47183|•Microphone• Here...|             99984|\n",
      "+-------+--------------------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2018.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_c0', 'string'), ('_c1', 'string'), ('_c2', 'string')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2018.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_171819 = df_2017.union(df_2018).union(df_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_171819.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_171819.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2018.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[count(DISTINCT _c0): bigint]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = df_2018.agg(F.countDistinct('_c0'))\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df.createOrReplaceTempView('reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(output, n=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = spark.sql(\n",
    "  \"SELECT * FROM people\")\n",
    "names = results.map(lambda p: p.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-2f6ac8d704b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misEmpty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.rdd.isEmpty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o653.save.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 42.0 failed 1 times, most recent failure: Lost task 7.0 in stage 42.0 (TID 2407, localhost, executor driver): java.lang.RuntimeException: Cannot reserve additional contiguous bytes in the vectorized reader (requested 22340181 bytes). As a workaround, you can reduce the vectorized reader batch size, or disable the vectorized reader. For parquet file format, refer to spark.sql.parquet.columnarReaderBatchSize (default 4096) and spark.sql.parquet.enableVectorizedReader; for orc file format, refer to spark.sql.orc.columnarReaderBatchSize (default 4096) and spark.sql.orc.enableVectorizedReader.\n\tat org.apache.spark.sql.execution.vectorized.WritableColumnVector.throwUnsupportedException(WritableColumnVector.java:111)\n\tat org.apache.spark.sql.execution.vectorized.WritableColumnVector.reserve(WritableColumnVector.java:92)\n\tat org.apache.spark.sql.execution.vectorized.WritableColumnVector.appendBytes(WritableColumnVector.java:468)\n\tat org.apache.spark.sql.execution.vectorized.OnHeapColumnVector.putByteArray(OnHeapColumnVector.java:497)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedPlainValuesReader.readBinary(VectorizedPlainValuesReader.java:193)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedRleValuesReader.readBinarys(VectorizedRleValuesReader.java:426)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBinaryBatch(VectorizedColumnReader.java:472)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:220)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:261)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:159)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:181)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.scan_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:232)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.sql.execution.vectorized.OnHeapColumnVector.reserveInternal(OnHeapColumnVector.java:523)\n\tat org.apache.spark.sql.execution.vectorized.WritableColumnVector.reserve(WritableColumnVector.java:90)\n\t... 27 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:167)\n\t... 32 more\nCaused by: java.lang.RuntimeException: Cannot reserve additional contiguous bytes in the vectorized reader (requested 22340181 bytes). As a workaround, you can reduce the vectorized reader batch size, or disable the vectorized reader. For parquet file format, refer to spark.sql.parquet.columnarReaderBatchSize (default 4096) and spark.sql.parquet.enableVectorizedReader; for orc file format, refer to spark.sql.orc.columnarReaderBatchSize (default 4096) and spark.sql.orc.enableVectorizedReader.\n\tat org.apache.spark.sql.execution.vectorized.WritableColumnVector.throwUnsupportedException(WritableColumnVector.java:111)\n\tat org.apache.spark.sql.execution.vectorized.WritableColumnVector.reserve(WritableColumnVector.java:92)\n\tat org.apache.spark.sql.execution.vectorized.WritableColumnVector.appendBytes(WritableColumnVector.java:468)\n\tat org.apache.spark.sql.execution.vectorized.OnHeapColumnVector.putByteArray(OnHeapColumnVector.java:497)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedPlainValuesReader.readBinary(VectorizedPlainValuesReader.java:193)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedRleValuesReader.readBinarys(VectorizedRleValuesReader.java:426)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBinaryBatch(VectorizedColumnReader.java:472)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:220)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:261)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:159)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:181)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.scan_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:232)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.sql.execution.vectorized.OnHeapColumnVector.reserveInternal(OnHeapColumnVector.java:523)\n\tat org.apache.spark.sql.execution.vectorized.WritableColumnVector.reserve(WritableColumnVector.java:90)\n\t... 27 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-a85d536ff582>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m             .save(\"df.parquet\"))\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o653.save.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 42.0 failed 1 times, most recent failure: Lost task 7.0 in stage 42.0 (TID 2407, localhost, executor driver): java.lang.RuntimeException: Cannot reserve additional contiguous bytes in the vectorized reader (requested 22340181 bytes). As a workaround, you can reduce the vectorized reader batch size, or disable the vectorized reader. For parquet file format, refer to spark.sql.parquet.columnarReaderBatchSize (default 4096) and spark.sql.parquet.enableVectorizedReader; for orc file format, refer to spark.sql.orc.columnarReaderBatchSize (default 4096) and spark.sql.orc.enableVectorizedReader.\n\tat org.apache.spark.sql.execution.vectorized.WritableColumnVector.throwUnsupportedException(WritableColumnVector.java:111)\n\tat org.apache.spark.sql.execution.vectorized.WritableColumnVector.reserve(WritableColumnVector.java:92)\n\tat org.apache.spark.sql.execution.vectorized.WritableColumnVector.appendBytes(WritableColumnVector.java:468)\n\tat org.apache.spark.sql.execution.vectorized.OnHeapColumnVector.putByteArray(OnHeapColumnVector.java:497)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedPlainValuesReader.readBinary(VectorizedPlainValuesReader.java:193)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedRleValuesReader.readBinarys(VectorizedRleValuesReader.java:426)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBinaryBatch(VectorizedColumnReader.java:472)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:220)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:261)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:159)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:181)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.scan_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:232)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.sql.execution.vectorized.OnHeapColumnVector.reserveInternal(OnHeapColumnVector.java:523)\n\tat org.apache.spark.sql.execution.vectorized.WritableColumnVector.reserve(WritableColumnVector.java:90)\n\t... 27 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:167)\n\t... 32 more\nCaused by: java.lang.RuntimeException: Cannot reserve additional contiguous bytes in the vectorized reader (requested 22340181 bytes). As a workaround, you can reduce the vectorized reader batch size, or disable the vectorized reader. For parquet file format, refer to spark.sql.parquet.columnarReaderBatchSize (default 4096) and spark.sql.parquet.enableVectorizedReader; for orc file format, refer to spark.sql.orc.columnarReaderBatchSize (default 4096) and spark.sql.orc.enableVectorizedReader.\n\tat org.apache.spark.sql.execution.vectorized.WritableColumnVector.throwUnsupportedException(WritableColumnVector.java:111)\n\tat org.apache.spark.sql.execution.vectorized.WritableColumnVector.reserve(WritableColumnVector.java:92)\n\tat org.apache.spark.sql.execution.vectorized.WritableColumnVector.appendBytes(WritableColumnVector.java:468)\n\tat org.apache.spark.sql.execution.vectorized.OnHeapColumnVector.putByteArray(OnHeapColumnVector.java:497)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedPlainValuesReader.readBinary(VectorizedPlainValuesReader.java:193)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedRleValuesReader.readBinarys(VectorizedRleValuesReader.java:426)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBinaryBatch(VectorizedColumnReader.java:472)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:220)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:261)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:159)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:181)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.scan_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:232)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.sql.execution.vectorized.OnHeapColumnVector.reserveInternal(OnHeapColumnVector.java:523)\n\tat org.apache.spark.sql.execution.vectorized.WritableColumnVector.reserve(WritableColumnVector.java:90)\n\t... 27 more\n"
     ]
    }
   ],
   "source": [
    "df = (spark.read\n",
    "            .load(\"data/*.parquet\")\n",
    "            .write\n",
    "            .format(\"parquet\")\n",
    "            .save(\"df.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o245.save.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 24 in stage 20.0 failed 1 times, most recent failure: Lost task 24.0 in stage 20.0 (TID 992, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:257)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.hadoop.fs.FSError: java.io.IOException: No space left on device\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:248)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:58)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.writeChunk(ChecksumFileSystem.java:424)\n\tat org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunks(FSOutputSummer.java:206)\n\tat org.apache.hadoop.fs.FSOutputSummer.write1(FSOutputSummer.java:124)\n\tat org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:110)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:58)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat java.io.FilterOutputStream.write(FilterOutputStream.java:97)\n\tat org.apache.parquet.hadoop.util.HadoopPositionOutputStream.write(HadoopPositionOutputStream.java:45)\n\tat org.apache.parquet.bytes.ConcatenatingByteArrayCollector.writeAllTo(ConcatenatingByteArrayCollector.java:46)\n\tat org.apache.parquet.hadoop.ParquetFileWriter.writeDataPages(ParquetFileWriter.java:460)\n\tat org.apache.parquet.hadoop.ColumnChunkPageWriteStore$ColumnChunkPageWriter.writeToFileWriter(ColumnChunkPageWriteStore.java:201)\n\tat org.apache.parquet.hadoop.ColumnChunkPageWriteStore.flushToFileWriter(ColumnChunkPageWriteStore.java:261)\n\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.flushRowGroupToStore(InternalParquetRecordWriter.java:173)\n\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.close(InternalParquetRecordWriter.java:114)\n\tat org.apache.parquet.hadoop.ParquetRecordWriter.close(ParquetRecordWriter.java:165)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.close(ParquetOutputWriter.scala:42)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.releaseResources(FileFormatDataWriter.scala:57)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.commit(FileFormatDataWriter.scala:74)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:247)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248)\n\t... 10 more\nCaused by: java.io.IOException: No space left on device\n\tat java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:246)\n\t... 36 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:167)\n\t... 32 more\nCaused by: org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:257)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.hadoop.fs.FSError: java.io.IOException: No space left on device\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:248)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:58)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.writeChunk(ChecksumFileSystem.java:424)\n\tat org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunks(FSOutputSummer.java:206)\n\tat org.apache.hadoop.fs.FSOutputSummer.write1(FSOutputSummer.java:124)\n\tat org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:110)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:58)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat java.io.FilterOutputStream.write(FilterOutputStream.java:97)\n\tat org.apache.parquet.hadoop.util.HadoopPositionOutputStream.write(HadoopPositionOutputStream.java:45)\n\tat org.apache.parquet.bytes.ConcatenatingByteArrayCollector.writeAllTo(ConcatenatingByteArrayCollector.java:46)\n\tat org.apache.parquet.hadoop.ParquetFileWriter.writeDataPages(ParquetFileWriter.java:460)\n\tat org.apache.parquet.hadoop.ColumnChunkPageWriteStore$ColumnChunkPageWriter.writeToFileWriter(ColumnChunkPageWriteStore.java:201)\n\tat org.apache.parquet.hadoop.ColumnChunkPageWriteStore.flushToFileWriter(ColumnChunkPageWriteStore.java:261)\n\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.flushRowGroupToStore(InternalParquetRecordWriter.java:173)\n\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.close(InternalParquetRecordWriter.java:114)\n\tat org.apache.parquet.hadoop.ParquetRecordWriter.close(ParquetRecordWriter.java:165)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.close(ParquetOutputWriter.scala:42)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.releaseResources(FileFormatDataWriter.scala:57)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.commit(FileFormatDataWriter.scala:74)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:247)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248)\n\t... 10 more\nCaused by: java.io.IOException: No space left on device\n\tat java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:246)\n\t... 36 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-8513f673ecd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                \u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                .save(\"df_2019.parquet\"))\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o245.save.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 24 in stage 20.0 failed 1 times, most recent failure: Lost task 24.0 in stage 20.0 (TID 992, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:257)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.hadoop.fs.FSError: java.io.IOException: No space left on device\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:248)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:58)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.writeChunk(ChecksumFileSystem.java:424)\n\tat org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunks(FSOutputSummer.java:206)\n\tat org.apache.hadoop.fs.FSOutputSummer.write1(FSOutputSummer.java:124)\n\tat org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:110)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:58)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat java.io.FilterOutputStream.write(FilterOutputStream.java:97)\n\tat org.apache.parquet.hadoop.util.HadoopPositionOutputStream.write(HadoopPositionOutputStream.java:45)\n\tat org.apache.parquet.bytes.ConcatenatingByteArrayCollector.writeAllTo(ConcatenatingByteArrayCollector.java:46)\n\tat org.apache.parquet.hadoop.ParquetFileWriter.writeDataPages(ParquetFileWriter.java:460)\n\tat org.apache.parquet.hadoop.ColumnChunkPageWriteStore$ColumnChunkPageWriter.writeToFileWriter(ColumnChunkPageWriteStore.java:201)\n\tat org.apache.parquet.hadoop.ColumnChunkPageWriteStore.flushToFileWriter(ColumnChunkPageWriteStore.java:261)\n\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.flushRowGroupToStore(InternalParquetRecordWriter.java:173)\n\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.close(InternalParquetRecordWriter.java:114)\n\tat org.apache.parquet.hadoop.ParquetRecordWriter.close(ParquetRecordWriter.java:165)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.close(ParquetOutputWriter.scala:42)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.releaseResources(FileFormatDataWriter.scala:57)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.commit(FileFormatDataWriter.scala:74)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:247)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248)\n\t... 10 more\nCaused by: java.io.IOException: No space left on device\n\tat java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:246)\n\t... 36 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:167)\n\t... 32 more\nCaused by: org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:257)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.hadoop.fs.FSError: java.io.IOException: No space left on device\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:248)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:58)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.writeChunk(ChecksumFileSystem.java:424)\n\tat org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunks(FSOutputSummer.java:206)\n\tat org.apache.hadoop.fs.FSOutputSummer.write1(FSOutputSummer.java:124)\n\tat org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:110)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:58)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat java.io.FilterOutputStream.write(FilterOutputStream.java:97)\n\tat org.apache.parquet.hadoop.util.HadoopPositionOutputStream.write(HadoopPositionOutputStream.java:45)\n\tat org.apache.parquet.bytes.ConcatenatingByteArrayCollector.writeAllTo(ConcatenatingByteArrayCollector.java:46)\n\tat org.apache.parquet.hadoop.ParquetFileWriter.writeDataPages(ParquetFileWriter.java:460)\n\tat org.apache.parquet.hadoop.ColumnChunkPageWriteStore$ColumnChunkPageWriter.writeToFileWriter(ColumnChunkPageWriteStore.java:201)\n\tat org.apache.parquet.hadoop.ColumnChunkPageWriteStore.flushToFileWriter(ColumnChunkPageWriteStore.java:261)\n\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.flushRowGroupToStore(InternalParquetRecordWriter.java:173)\n\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.close(InternalParquetRecordWriter.java:114)\n\tat org.apache.parquet.hadoop.ParquetRecordWriter.close(ParquetRecordWriter.java:165)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.close(ParquetOutputWriter.scala:42)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.releaseResources(FileFormatDataWriter.scala:57)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.commit(FileFormatDataWriter.scala:74)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:247)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248)\n\t... 10 more\nCaused by: java.io.IOException: No space left on device\n\tat java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:246)\n\t... 36 more\n"
     ]
    }
   ],
   "source": [
    "df_2019 = (spark.read\n",
    "               .format(\"csv\")\n",
    "               .option(\"delimiter\", \"\\t\")\n",
    "               .option('inferSchema', \"true\")\n",
    "               .load(\"data/detail-desc-text-2018.tsv\")\n",
    "               .write\n",
    "               .format(\"parquet\")\n",
    "               .save(\"df_2019.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0='9872055', _c1=\"The following description is merely exemplary in nature and is not intended to limit the present disclosure, application, or uses. For purposes of clarity, the same reference numbers will be used in the drawings to identify similar elements. As used herein, the term module refers to an application specific integrated circuit (ASIC), an electronic circuit, a general purpose computing device, a processor (shared, dedicated, or group) and memory that execute one or more software or firmware programs, a combinational logic circuit, and/or other suitable components that provide the described functionality. As used herein, the phrase “at least one of A, B, and C” should be construed to mean a logical (A or B or C), using a non-exclusive logical OR. It should be understood that steps within a method may be executed in different order without altering the principles of the present disclosure. The teachings of the present disclosure can be implemented in a system for communicating content to an end user or user device. Both the data source and the user device may be formed using a general computing device having a memory or other data storage for incoming and outgoing data. The memory may comprise but is not limited to a hard drive, FLASH, RAM, PROM, EEPROM, ROM phase-change memory or other discrete memory components. Each general purpose computing device may be implemented in electrical circuitry, analog circuitry, digital circuitry, system on chips or combinations thereof. Further, the computing device may include a microprocessor (processor) or microcontroller that performs instructions to carry out the steps performed by the various system components. A content or service provider is also described. A content or service provider is a provider of data to the end user. The service provider, for example, may provide data corresponding to the content such as metadata as well as the actual content in a data stream or signal. The content or service provider may include a general purpose computing device, communication components, network interfaces and other associated circuitry to allow communication with various other devices in the system. Further, while the following disclosure is made with respect to the delivery of video (e.g., television (TV), movies, music videos, etc.), it should be understood that the systems and methods disclosed herein could also be used for delivery of any media content type, for example, audio, music, data files, web pages, advertising, etc. Additionally, throughout this disclosure reference is made to data, content, information, programs, movie trailers, movies, advertising, assets, video data, etc., however, it will be readily apparent to persons of ordinary skill in the art that these terms are substantially equivalent in reference to the example systems and/or methods disclosed herein. As used herein, the term “title” will be used to refer to, for example, a movie itself and not the name of the movie. As used herein, the term “content” will be used to refer to, for example, a movie or program itself “Content identifier” refers to the data associated with content used for identifying the content. Machines may use an actual numeric or alphanumeric value unique to the content. People may use a title for identification. Various types of data may be associated with the content. For program guides and recommendations list content identifiers, a cluster identifier and other data may also be provided with the content. While the following disclosure is made with respect to example DIRECTV® broadcast services and systems, it should be understood that many other delivery systems are readily applicable to disclosed systems and methods. Such systems include wireless terrestrial distribution systems, wired or cable distribution systems, cable television distribution systems, Ultra High Frequency (UHF)/Very High Frequency (VHF) radio frequency systems or other terrestrial broadcast systems (e.g., Multi-channel Multi-point Distribution System (MMDS), Local Multi-point Distribution System (LMDS), etc.), Internet-based distribution systems, cellular distribution systems, power-line broadcast systems, any point-to-point and/or multicast Internet Protocol (IP) delivery network, and fiber optic networks. Further, the different functions collectively allocated among a service provider and integrated receiver/decoders (IRDs) as described below can be reallocated as desired without departing from the intended scope of the present patent. In the following examples, recommendations of titles of various programs are provided. The content recommendations may provide a title either graphically or alpha-numerically or a combination of both. Graphically, content posters or thumbnails may be provided. Several lists are generated, sorted and processed herein. The lists may include content or program titles or one or more alphanumeric identifiers or both. The list may not contain the actual content itself. Referring now to FIG. 1 , a satellite television broadcasting system 10 is illustrated. The satellite television broadcast system 10 includes a head end 12 that generates wireless signals 13 through an antenna 14 which are received by an antenna 16 of a satellite 18 . The wireless signals 13 , for example, may be digital. The wireless signals 13 may be referred to as an uplink signal. A transmitting antenna 20 generates downlink signals 26 that are directed to various receiving systems including stationary systems such as those in the home, as well as mobile receiving systems. One example of a receiving unit is a set top box. Another device is a gateway device 22 . The gateway device 22 is in communication with a respective antenna 24 . Each antenna 24 receives downlink signals 26 from the transmitting antenna 20 of the satellite 18 . The gateway devices and set top boxes may be referred to as satellite television receiving devices. The gateway device 22 may be connected within a building 28 such as a house, multi-dwelling unit or commercial building. A local area network 30 may be used to connect gateway device 22 with client devices 32 and other IP devices such as mobile devices 34 and fixed devices 36 . The local area network 30 may be a wireless network or a wired network. The interconnection of the gateway device 22 with the client devices 32 allow for multi-room viewing of content as well as coordinated content recording. The gateway device 22 may also be referred to as a server device. The gateway device 22 may be used to distribute content, channels, programs and other data to each of the client devices 32 . Examples of a mobile device 34 include but are not limited to mobile phones, tablet devices and portable computers. Fixed devices 36 may include desktop computers, video game systems, printers or other IP devices that are not meant to be transported on a regular basis. The gateway device 22 may also be in communication with a set top box 38 . The set top box 38 and the gateway device 22 may also be connected through the local area network 30 . Coordination of recordings may also be provided between the set top box 38 and the gateway device 22 . The set top box 38 may also include a DVR that communicates content to the gateway device 22 . The gateway device 22 may also communicate content to the set top box 38 . The client device 32 may be connected to the gateway device 22 through a DIRECTV® Ethernet to coaxial module DECA module 40 . The DECA module 40 converts Ethernet content to content suitable for communication through a coaxial network. One or more of the client devices 32 may have a DECA module 40 associated therewith. The local area network 30 may also have network area storage 42 associated therewith. The network area storage (NAS) 42 may be used for storing content and other data files from the mobile devices 34 , the fixed devices 36 or from the gateway device 22 . The network area storage 42 may be accessible from one or more of the devices in communication with the local area network 30 . The gateway device 22 may also be in communication with the head end 12 through a wide area network 50 . The wide area network 50 may be one type of network or multiple types of networks. The network 36 may, for example, be a public switched telephone network, the internet, a mobile telephone network or other type of network. The gateway device 22 , the client devices 32 and the set top box 38 may all be in communication with a display 44 used for displaying content. Although a separate display is not illustrated for the mobile devices 34 and the fixed devices 36 , it is understood that displays may be incorporated into those devices as well. The displays 44 may be a monitor or television, or other screen device used for displaying content, program guides, descriptions, graphical user interfaces and the like. The graphical user interfaces may be selected using the input 125 . The input 125 may be a remote control or other pointing device used to make a selection of choices set forth within a graphical user interface. A cellular phone tower 52 is also illustrated. A cellular phone tower 52 may provide access to a mobile telephone network and to the wide area network 50 . In the case where a gateway device 22 does not have access to the wide area network through a service such as a digital subscriber line or cable modem, a mobile device 54 may be incorporated into the system. The mobile device 54 may receive signals from the gateway device 22 that are intended for the wide area network 50 and communicate the signals through the cellular tower 52 to the wide area network. The mobile device 54 may allow the head end 12 to communicate with the gateway device 22 directly. Examples of suitable uses for the gateway device 22 to connect with the head end 12 may be for call back signals that are generated when ordering on-demand or pay-per-view content. The gateway device 22 may also be in communication with the wireless area network through the cellular tower 52 to provide various types of internet content, analytics and the like to and from the gateway device 22 . The head end 12 may be in communication with an external content source 60 . The external content source 60 may contain a progressive download source 62 , a streaming source 64 and a broadband source. The content source 60 may provide content for distribution through the satellite 18 to the gateway device 22 . The external content source 60 may also provide content to the gateway device 22 through the progressive download source 62 and the streaming source 64 through the wide area network 50 . The broadband source 66 communicates broadband content to the gateway device 22 . A cable television source 68 may also be coupled to the gateway device 22 directly through a cable or through the wide area network 50 . The cable television source 68 provides cable content to the head end 12 or to the gateway device 22 through a wired or wireless connection. The system 10 may also include an analytics module 70 that includes a content analytics module 72 and a cloud system function analytics module 74 . The content analytics module 72 may be used to analyze the content browsed by devices connected to the gateway device 22 . The content analytics module 72 may use intelligence to derive customer behavior and improve targeting of customer needs in relation to content browsed through the gateway device 22 . For example, the content analytics module 72 may monitor third party video websites and keep track of content watched or browsed so that improved recommendations may be provided to customers when browsing through a user interface at the gateway device 22 . The content analytics provided by the content analytics module 72 may also be used to adjust the advertisements provided to the gateway device 22 . A content analytics module 72 may also provide data so that content relevant to the user is prepositioned at the gateway device 22 . Details of the operation of the content analytics module 72 are provided below. The cloud system function analytics module 74 provides an external analytical engine for analyzing data provided from the gateway device 22 . Data may be communicated to the cloud system function analytics module 74 directly or indirectly from a monitoring module that may be internal or external to the gateway device 22 . The cloud system function analytics module 74 may act as a real-time monitoring module that extracts low level information and stores it in a database. The cloud system function analytics module 74 may provide real-time visualization of various parameters of operation of the gateway device 22 . The operation of the cloud system function analytics module 74 will be described in further detail below. Referring now to FIG. 2 , the client device 32 is illustrated in further detail. The client device 32 may include various component modules for use within the local area network 30 and for displaying content signals. The display of content signals may take place by rendering signals provided from the network. It should be noted that the client device 32 may comprise various different types of devices or may be incorporated into various types of devices. For example, the client device 32 may be a standalone device that is used to intercommunicate through a local area network to the gateway device 22 illustrated in FIG. 1 . Communication to the gateway device 22 may be wired or wireless. The client device 32 may also be incorporated into various types of devices such as a television, a video gaming system, a hand-held device such as a phone or personal media player, a computer, or any other type of device capable of being networked. The client device 32 may include various component modules such as those illustrated below. It should be noted that some of the components may be optional components depending on the desired capabilities of the client device and the overall system. The client device 32 includes an interface module 160 . The interface module 160 may control communication between the local area network and the client device 32 . Digital Transmission Content Protocol Volume 1 (DTCP) may be used to encrypt the communication signals between the gateway and client device as a form of digital rights management. As mentioned above, the client device 32 may be integrated within various types of devices or may be a standalone device. The interface module 160 communicates with a rendering module 162 . The rendering module 162 receives formatted signals through the local area network that are to be displayed on the display. The rendering module 162 merely places pixels in locations as instructed by the formatted signals. Rendering may also take place using vector graphics commands that instruct a group of pixels to be formed by the client based on simple instructions. By not including a decoder, the rendering module 162 will allow consistent customer experiences at various client devices. The rendering module 162 communicates rendered signals to the display of the device or an external display. The rendered signals may include but are not limited to a program guide and guide banners or row ads associated therewith, a playlist, a pay-per-view list, and a recommendations list. A recommendation list may contain recommended programs or movies or both. Recommended lists may also include content or movies that are stored within the gateway device 22 . A boot-up acquisition module 164 may provide signals through the interface module 160 during boot-up of the client device 32 . The boot-up acquisition module 164 may provide various data that is stored in memory 166 through the interface module 160 . The boot-up acquisition module 164 may provide a make identifier, a model identifier, a hardware revision identifier, a major software revision, and a minor software revision identifier. Also, a download location for the server device to download a boot image may also be provided. A unique client device identifier for each client device may also be provided. The gateway device 22 may receive the client device identifier in communication signals from each client device. The client device identifier may be used in various communications between the client device and the gateway device 22 . For example, when requesting a playlist, a pay-per-view list, a set top box guide, a movie list or recommendations, a listing request may come with a client device identifier so that the particular client device may be identified by the gateway device 22 . Also, when viewing requests, such as channel tuning, content selection and the like are performed, the client device identifier may be communicated from the client device. The gateway device 22 may correlate the watched content and the client device so that recommendations, ordered playlists, pay-per-view list, guide banners and movie recommendations may be personalized for the various client devices. The boot-up acquisition module 164 may obtain each of the above-mentioned data from memory 166 . Communications may take place using HTTP client module 170 . The HTTP client module 170 may provide formatted HTTP signals to and from the interface module 160 . A remote user interface module 172 allows client devices associated with the client device 32 to communicate remote control commands and status to the server device. The remote user interface module 172 may be in communication with the receiving module 174 . The receiving module 174 may receive the signals from a remote control or input 125 through input signal 178 associated with the display and convert them to a form usable by the remote user interface module 172 . The remote user interface module 172 allows the server device to send graphics and audio and video to provide a full featured user interface within the client. Screen displays may be generated based on the signals from the server device. Thus, the remote user interface module 172 may also receive data through the interface module 160 . It should be noted that modules such as the rendering module 162 and the remote user interface module 172 may communicate and render both audio and visual signals. The receiving module 174 may receive input signals from the input 125 . The input 125 may be a visual input signal that may include, but is not limited to, a graphical input signal such as a stylus signal or a gesture signal, a mouse signal, or a pointer signal. The data received through the receiving module 174 may be communicated directly to the interface module 160 and ultimately the server device with very little processing because very little processing power may be included within a client device 32 . The receiving module 174 may convert the signals input into electrical signals for transmission or communication to the server device. For example, the raw voice signals may be communicated to the server device through the interface module 160 . A clock 180 may communicate with various devices within the system so that the signals and the communications between the server device and client are synchronized and controlled. Referring now to FIG. 3 , the head end 12 is illustrated in further detail. The head end 12 may include a billing system 310 . The billing system 310 may include various types of account information including the billing address, the subscriptions that the user subscribes to, authorizations, conditional access identifiers and the like. The billing system 310 is in communication with an authentication system 312 . The authentication system 312 may authenticate devices that are in communication with the head end 12 , including the gateway device 22 and the various user devices in communication with the head end through the wide area network. The authentication system 312 may also be in communication with an encryption system 314 used to encrypt content. Various encryption keys may be used to encrypt content for particular users. By encrypting content in the encryption system 314 , content is prevented from use by unauthorized users. Encryption may take place for individual content or may take place for broadcasted content used by a plurality of users. Authentication may take place in the authentication system 312 by using a password or through a conditional access system located in the individual devices. The conditional access system may have logic designed to decrypt the content. The head end 12 may communicate content to the gateway device 22 of FIG. 1 using a content module 330 . The content module 330 may process the content into appropriate formats for broadcasting or point-to-point delivery. Transcoding, frequency translation, multiplexing and amplification may all be performed within the content module 330 . The type of processing is at least in part based on the mechanism of delivery. Different versions of content may be housed in a content repository 332 . The content source 60 may provide content to the content module 330 . Content may be provided in a tape or recordable disk as well as through cable, RF or satellite. An advanced program guide (APG) module 334 communicates program guide data to various users of the system. For example, the APG module 334 may communicate guide date to the gateway device 22 of FIG. 1 through the satellite 18 . The content source 60 may provide guide data to the APG module 334 . However, a separate data source 336 may also provide the advance program guide module 334 with data. The data corresponding to content may include titles, posters, actor data, descriptions, parental ratings, user ratings, studio data, credits data, genre data, subgenre data, content type and the like. The head end 12 may also include a recommendation authoring system 340 . The recommendation authoring system 340 generates an external recommendation list that is ultimately communicated to the gateway device 22 . The external recommendation list may include recommended content titles, related content to the recommended content and the strength of similarity score between the related content and recommended content. In this case both the recommended content and the related content refer to content titles for the specific programming. The external recommendation list may include other types of metadata such as the channel, actors, ratings, the start and end times, the date and information as to whether the content will be broadcasted at a different time. In the present example, the external recommendation list may be the same for different gateway devices 22 . As will be described below the external recommendation list may be communicated to and modified by the gateway devices 22 . The recommendation authoring system 340 may be a combination of automated and operator controlled systems. The recommendation authoring system 340 may receive data from various sources including external sources 342 and internal sources 344 available from within the head end 12 . The external sources may, for example, be provided from one or more experts, from Nielsen® ratings, Blue Fin®, or other external sources. An example of an internal source 344 is “What's Hot”, which is a list of the most watched and talked about programming available from the system provider associated with the head end 12 . The internal source 344 may also be modified according to various marketing experts within the head end 12 . The compilation of the external sources and internal sources may be performed to obtain the external recommendation list. Both the external sources 342 and internal sources 344 may also generate the related content list. The related content list may also have the same type of metadata associated with the recommended content. An analytics module 70 ′ may also be incorporated into the head end 12 . The analytics module 70 ′ may perform the same functions as the analytics module 70 illustrated in FIG. 1 . In FIG. 1 , the analytics module 70 is a separate web connected module. However, the analytics module 70 ′ may be incorporated into the head end 12 . The analytics module 70 ′ may incorporate the functions of the content analytics module 72 and the cloud system function and the analytics module 74 described above. They have not been illustrated as a separate device for simplicity in FIG. 3 . Referring now to FIG. 4 , a gateway device 22 is illustrated in further detail. Although, a particular configuration of the gateway device 22 is illustrated, it is merely representative of various electronic devices with an internal controller used as a content receiving device. The antenna 24 may be one of a number of different types of antennas that includes one or more low noise blocks. The antenna 24 may be a single antenna 24 used for satellite television reception. The gateway device 22 may be coupled to the display 44 . The display 44 may have an output driver 412 within the gateway device 22 . The gateway device 22 may be headless so that it may be placed in any location throughout the home. That is, no direct connection to a display or television is required. A controller 410 is used to coordinate and control the various functions of the gateway device 22 . The controller 410 may be a one or more general processors such as a microprocessor that cooperates with control software. In the present example, the controller comprises two processors 410 A and 410 B. The processor 410 A is a media processor module (media processor) and the processor 410 B is a network processor module (network processor for short). The media processor 410 A functions include the functions of a front end 414 . These functions may include a tuner 414 A, a demodulator 414 B, a decoder 414 C such as a forward error correction decoder and any buffer or other functions. Specific functions of the controller 410 are described in detail below. The processor 410 B may be used to control the interfacing with various networks and act as a router and may also act as a modem. The controller 410 may also include a viewer tracking module 416 , a browser tracking module 418 and a recommendation module 420 . The viewer tracking module 416 is used for tracking and logging viewing events at the gateway device 22 . Ultimately, the viewer history may be logged in a viewer tracking log as will be further described below. The viewer tracking module 416 may track a time that content was watched, a client device associated with the content, the title of the content and if the content belongs to a series. The browser tracking module 418 may track browser activity data related to content viewed on websites through a web browser. The browser activity data may include, but is not limited to, video content viewed partially or in its entirety, content trailers, and data look-ups. The recommendation module 420 is used for generating recommendations corresponding to programs currently available for viewing corresponding to previously watched content and content related to browser activity during a particular time slot. The recommendation module 420 may also generate programs or recordings that are deemed to be future or current programming that the viewer should like based on an analysis of viewing habits of the viewer. The recommendations module 420 may receive the external recommendations list which is coordinated or supplanted by recommendations based on the viewer tracking data and browser tracking data. In general, the tuner 414 A receives the signal or data from the individual channel. The tuner 414 A may receive television programming content, program guide data or other types of data. The demodulator 414 B demodulates the signal or data to form a demodulated signal or data. The decoder 414 C decodes the demodulated signal to form decoded data or a decoded signal. The controller 410 may be similar to that found in current DIRECTV® set top boxes which uses a chip-based multifunctional controller. Although only one tuner 414 A, one demodulator 414 B and one decoder 414 C are illustrated, multiple tuners, demodulators and decoders may be provided within a gateway device 22 . The controller 410 is in communication with a memory 430 . The memory 430 is illustrated as a single box with multiple boxes therein. The memory 430 may actually be a plurality of different types of memory including the hard drive, a flash drive and various other types of memory. The different boxes represented in the memory 430 may be other types of memory or sections of different types of memory. The memory 430 may be non-volatile memory or volatile memory or combinations thereof. The memory 430 may include storage for various operational data collected during operation of the gateway device 22 . One type of data storage includes the viewer tracking log 432 obtained and controlled by the viewer tracking module 416 . The viewer tracking log (VTL) 432 includes viewer tracking log data that includes data about details of programs that have been watched or played back, including what time that they were watched or played back. The data of the VTL 432 may also include how long they were watched and program details. The program details may include whether the program belongs to a series. Recording deletion data within a digital video recorder may also be included in the data of the VTL 432 . The memory 430 may also include a browser tracking log 432 B obtained and controlled by the browser tracking module 418 . The browser tracking log 432 B includes browser tracking data that includes details of browser activity that includes video content partially viewed or viewed in its entirety, content trailers viewed and data lookups. The data in the browser activity may or may not relate directly to video content activity. The amount of time browsing various websites or watching on-line content may also be tracked. Another type of memory 430 is the settings and the list information (SLI) memory 434 . The SLI memory 134 may store various types of data including set top box playlist data 436 that has the playlist for content saved within the gateway device 22 . The playlist data contains content visible to users and content currently non-visible to users. Another type of data is the favorite settings for the gateway device 22 . The favorites may be stored in a favorites memory 138 . Other types of data may also be included in the SLI memory 434 which is illustrated as an “other” data memory 440 . The other data memory 440 may include various types of data including ignored suggestions which correspond to suggestion or recommendation suggestions that were ignored. Another type of data in the other data memory 440 may include the channel subscription data, the blocked channels, adult channels, rating limits set by the gateway device 22 , current set top box language, prioritizer data, TV resolution data, to do list data, the conditional access module identifier, and a request identifier. Further, time zone data, time of day daylight savings, status data, aspect ratio data, viewing hours data, quick tune list and a zip code may all be included within the other memory 4140 of the SLI memory 434 . The memory 430 may also include advanced program guide memory 444 . The advanced program guide (APG) memory 444 may store program guide data that is received within the system. The program guide data may store various amounts of data including two or more weeks' worth of program guide data. The program guide data from the APG memory 444 may be communicated in various manners including through the satellite 18 of FIG. 1 . The program guide data may include a content or program identifiers, and various data objects corresponding thereto. The content identifier may include series data. The first 4 digits may, for example, identify the series. The program guide may include program characteristics for each program content. The program characteristic may include ratings, categories, actor, director, writer, content identifier and producer data. The data may also include various other settings. The memory 430 may also include a digital video recorder 446 . The digital video recorder 446 may be a hard drive, flash drive, or other memory device. A record of the content stored in the digital video recorder is a playlist. The playlist may be stored in the DVR 446 or a separate memory as illustrated. The memory 430 may also include a cache memory 448 . The cache memory 448 may be used to store cached content corresponding to web content under the control of a cache module 450 that will be described further below. The gateway device 22 may also include a user interface (UI) 456 . The user interface 456 may be various types of user interfaces such as a keyboard, push buttons, a touch screen, a voice activated interface or the like. The user interface 456 may be used to select a channel, select various information, change the volume, change the display appearance, or other functions. The user interface 456 may also be used for selecting recommendation and providing feedback for recommendations. The user interface 456 may thus be used for generating a selection signal. A network interface 458 may be included within the gateway device 22 to communicate various data through the local area network 30 and to and from the wide area network 50 illustrated above. The network interface 458 may include various technologies used alone or in combination such as Wi-Fi, WiMax, WiMax mobile, wireless, cellular, a cable modem, a digital subscriber line (DSL) modem or other types of communication systems. The network interface 458 may use various protocols for communication therethrough including, but not limited to, hypertext transfer protocol (HTTP). The network interface 458 may also include a Bluetooth® interface, a Zigbee® interface or other limited distance RF interface for communicating with devices such as a mobile device. The gateway device 22 provides two major functions. That is, the gateway device 22 provides video service to the building illustrated in FIG. 1 . In the present example, satellite service is provided to the building 28 . However, cable and over-the-air video service may also be communicated through the gateway device 22 . The gateway 22 is also an aggregator of in-home IP networks and wide area networks. The IP network access service provider may be the same service provider as the television provider. The network interface 458 provides a modem function that is typically a separately provided function in typical home-based systems. The modem function provided within the network interface 458 may be provided in the same housing as the gateway device. More than one type of modem may be provided within the network interface 458 . Although, not all of the modem functions may be operated. That is, when the user receives the gateway device 22 , only the system's service providers subscribed to may be provisioned or enabled. Advantageously, the gateway device 22 does not have to communicate through an external device. The gateway device 22 controls the modem functions of the network interface 458 and thus the gateway device 22 can communicate with the wide area network (internet). This reduces the amount of interfacing issues in dealing with a third party modem device. A third party device does not have to be purchased and thus the overall cost to the consumer may be reduced. As will be described below, troubleshooting and integration of functionality provides a user of the gateway device 22 with a more reliable device with increased functionality incorporated therein. The gateway device 22 may also include a clock 460 . The clock 460 may be used to keep track of a current time. The current time may be used to determine a current time slot as described below. The current time may be used to identify the times at which video content or browser content is being viewed. In a sense, the current time may be used as a time stamp for data corresponding to the use or watching of various browser and video content. The time may also be used in self-healing and diagnostics to determine when certain gateway device parameters are being used. The gateway device 22 includes an initialization module 464 . The initialization module may be used for initially setting up the gateway device and the operation of the specific network interfaces set forth therein. More than one type of modem may be incorporated into the network interface 458 . The initialization module 464 is used to initialize one or more of the modems for communicating to the wide area network. Further, the interoperability of the modem as well as a router and the front end 414 is initialized in the initialization module 464 . The gateway device 22 may include an aggregation and transformation module 466 . The aggregation and transformation module 466 aggregates content from broadband sources, video sources such as the satellite and other sources available through the wide area network. The aggregation and transformation module 466 transforms the content into formats that are usable by the various devices connected to the local area network. The gateway device 22 may also include a self-learning module 468 . The self-learning module 468 is used for self-healing and diagnostics. The self-learning module 468 is a dynamic system that adapts to customer behavior, learns their pattern of service usage, and performs self-healing and diagnostics actions using a dynamic feedback mechanism. The gateway device 22 may also comprise an interaction flow module 470 . The interaction flow module 470 performs a combination of functions at the media processor and network processor that coordinates the functions internally. Ultimately, content arriving by way of the satellite and broadband may be aggregated together. The interaction flow module 470 allows the content to be processed so that the devices coupled to the gateway device 22 may consume the content received from the various sources. The content may be conditioned for delivery within the home network. “Flow” refers to signals flowing into, out of and through the gateway device 22 . The flow signals comprise the various types of content as described above. Referring now to FIG. 5 , a block diagrammatic view of the gateway device 22 is illustrated in further detail with functions of the media processor 410 A and the network processor 410 B having the functionality blocks associated therewith. In this example, a media processor module 510 includes the media processor 410 A and the functions associated therewith. A network processor module 512 has the network processor 410 B and the functions associated therewith. The network processor 512 includes the functions of the network interface 458 of FIG. 4 in further detail. The media processor 410 A has memory associated therewith. The memory is broken out in further detail compared to that set forth in FIG. 4 . In this example, the DVR 446 is in communication with the media processor 410 A. The media processor 410 A may also have flash memory 520 associated therewith. The flash memory 520 may be used for storage of critical data, files and images needed for robust operation upon start up. The data files and images are a bare minimum set of files required to bring the system into a normal operational state. Dynamic software updates of software images may also be stored in first memory 520 . The media processor 410 A may also be in communication with a double data rate (DDR) synchronous dynamic random-access memory 522 . The DDR memory 522 may be used for operating system execution as well as execution of programs and other executables. The media processor 410 A receives content from the front end 414 . In the present example, a tuner, demodulator and decoder may be contained within the front end 414 . Also in this example, satellite content is received by the front end 414 . However, other types of over-the-air or cable-based content may be received. A triplexer 524 may be incorporated into the media processor module 510 . The triplexer 524 provides content from an external connection to the front end 414 . The triplexer 524 may also provide content to a multimedia over coaxial alliance format (MoCa®) module 530 located within the network processor module 512 . A MoCA® module 530 ′ may be located in the media processor as well. The MoCa® standard allows coaxial cabling to enable whole-home distribution of high definition video content. A MoCa® connection 532 may interconnect the triplexer 524 and the MoCa® module 530 . The media processor 410 A and the network processor 410 B may be interconnected using a connection bus 534 . The bus 534 may be high speed or low speed or combinations of both. The Ethernet or gigabit media independent interface is a high speed connection. The connection bus 534 may be an Ethernet bus or a gigabit media independent interface. A low speed network connection may also be coupled between the media processor 410 A and the network process 410 B. As will be described in more detail below, the low speed connection may comprise several lines. Of course, other types of formats may be used between the media processor 410 A and the network processor 410 B. The network processor module 512 includes a detailed view of the network interface 458 . In this example, a network interface 540 may include a first Wi-Fi module 540 A, a second Wi-Fi module 540 B, a cellular modem 540 C and a Bluetooth® module 540 D. The Wi-Fi modules 540 A and 540 B may provide a plurality of connections outside of the network processor module to other devices such as mobile devices and tablet computers. The Wi-Fi modules 540 A and 540 B may be separate so that different communication frequencies may be used. The cellular modem 540 C may be a 3G/4G LTE cellular modem used to couple the network processor to a cellular service provider so that a wide area network connection may be obtained. The network interface 540 A may also include a ZigBee® module 540 E. The ZigBee module 540 E may communicate and receive ZigBee® formatted signal to and from devices outside the gateway device 22 . The network interface 540 A may also include a Remote Control Standard for Consumer Electronics (RF4CE) interface 540 F that may be used for remote controls. Network interface 540 may also include a digital subscriber line (DSL) interface 540 G. The DSL interface 540 G may be used to provide two-way communication of the network processor module 512 and a digital subscriber line such as a telephone line. The network interface 540 may also include a small form-factor pluggable (SFP) interface 540 H. The SFP interface 540 H may also be referred to as a mini-GBIC. The SFP interface 540 H is an optical transceiver module used to interface with optical fibers to provide two-way communication therethrough. The SFP transceiver may support SONET/DSH Fast Ethernet, Gigabit Ethernet, Fiber Channel, and other communication standards. The network processor 410 B may communicate to the MoCa® module 512 and the modules of the network interface 458 and are coupled through a layer two switch 544 . The layer two switch 544 may act as a router to route signals to other devices connected to the layer two switch 544 without having to involve the network processor 410 B. This improves the overall flow. However, the layer two switch 544 may be incorporated functionally within the network processor 410 B. A router 546 within the network processor 410 B is used to route signals to the WAN from various devices such as the MoCa module 530 and the devices of the network interface 458 . The term router may be used for the router 546 alone or in combination with the layer two switch 544 . The network processor 410 B may also be in communication with a wired multi-port LAN module 550 . By way of example, four ports are illustrated. A WAN module 552 may also be provided. The WAN module may be a one-port gigabit WAN connection. The LAN module 550 allows direct wired access to the network processor 410 B. The WAN module 552 provides wired access to the internet through a wide area network connection. Both wired and wireless connections may be used. The network processor module 512 may also have flash memory 554 that is used for storage of critical sellings, configuration and data for reliable bringing up of the system as well as storage of dynamically updated software images and previous image rollback. The network processor module 512 may also have DDR memory 556 that is used for operating system and program execution. The media processor module 510 and the network processor module 512 may each have a power block 560 associated therewith. The power blocks 560 may be an individual power supply or be a connector to a power supply of the gateway device 22 . The power block 560 of the media processor module may be in communication with the power block 560 of the network processor module through a power connector 562 . Referring now to FIG. 6 , a method for the startup of the gateway device is set forth. In step 610 the gateway device is started by providing power or selecting a power or reset switch. In step 612 it is determined whether the gateway device has been provisioned. If the gateway device has not been provisioned, the gateway device is provisioned in step 614 . By provisioning the gateway device, direct communication between the gateway device and the wide area network interface is established. The direct connection bypasses an external stand-alone modem. Communication to the WAN may be performed directly through the wide area network interface. As is illustrated in FIG. 5 , a plurality of different types of network interfaces may be provided within the gateway device. One of the network interfaces provides the wide area network interface of the gateway device. Provisioning the gateway device means selecting the desired network interface for communication. After step 610 and after step 614 , step 616 begins the integrated modem startup. Modem refers to one or more of the modules of the network interface. One or more of the devices in the network interface may be used in the gateway device. In step 618 it is determined whether the modem hardware module has been detected. If the modem hardware module has not been detected, the external modem is checked in step 620 . A report may be generated in step 622 so that the user may take action. After step 618 when the hardware module of the modem has been detected, step 622 is performed. Step 622 enables the software module of the modem. Step 624 is performed after step 622 and determines whether the modem has been provisioned. If the modem has not been provisioned, step 626 provisions the modem with the head end of the internet service provider associated with the modem. After steps 626 and 624 , step 628 begins operation of the integrated modem because both the modem and the integrated modem have been provisioned as described in the preceding steps. Referring now to FIG. 7 , a method for operating the gateway device with a mobile device 54 through a cell tower 52 that is connected to a wide area network 50 is set forth. In this example, it is presumed that a wide area network interface is not incorporated into the gateway device 22 . The mobile device 54 is used for connecting the gateway device ultimately to the wide area network 50 of FIG. 1 . In step 710 the Bluetooth® or Wi-Fi is turned on at the mobile device 54 . In step 712 the gateway device pairs with the mobile device 54 . This may be performed in various methods depending upon the type of mobile device. Secret codes or passwords may be used to pair the mobile device with the gateway device. Ultimately, the phone and gateway device recognize each other so that communication signals may be exchanged. Once paired with the gateway device, the gateway device may perform an optional step of restricting the type of content or the timing of content using settings. The user, through a user interface, generates content settings or timing settings to restrict the flow of content through the mobile device. In step 716 commands are communicated to the wide area network, such as the internet, from the gateway device through the mobile device. In step 718 commands are communicated from the wide area network or internet to the gateway device through the mobile device. Steps 714 through 718 may be performed continually until the gateway device is unpaired with the mobile device. This unpairing may take place when the mobile device exceeds the maximum communication distance. Step 720 monitors the heartbeat of the connection to determine if a connection is still in place. In step 722 , when the connection is active, the gateway device and mobile device continue to use the connection through the exchange of signals. In step 722 , when the connection is determined to be inactive by the lack of a “heartbeat” signal, step 726 generates a message at the display of the screen device to notify the user that the system is disconnected or in an error state. The system may take corrective action with or without user intervention. It should be noted that in step 714 the timing may be restricted. That is, one variant of the method allows the use of a time restriction for restricting use of the mobile device. For example, from 11 pm to 4 am every day the gateway device may make use of the mobile device for communicating with the wide area network. The timing may be set in non-prime time hours so that it is less likely that the user is using the mobile device. Referring now to FIG. 8 , the high level block diagrammatic view of the aggregation and transformation module 466 of FIG. 4 is set forth. The aggregation and transformation module 466 is in communication with the satellite 18 or a cable source through the front end 414 as described above in FIG. 4 . The aggregation and transformation module 466 is also in communication with a broadband source 66 . An aggregation module 810 obtains the content and allows the content to be transformed for use throughout the local area network. A transcoder module 812 may be used to transcode the content from a first type of encoding to a second type of encoding. A shaping and conditioning module 814 may shape and condition the content electronically to meet the requirements of a receiving device within the local area network based on but not limited to the type of receiving device and the type of content. A storing and forwarding module 816 may be a combination of other devices previously described in FIGS. 4 and 5 , such as the digital video recorder or other type of memory and a router. An in-home delivery/IP module 820 may also be a combination of devices previously described. For example, the layer two switch 564 may be used for the distribution of content through the system to various user devices through the local area network. The content may also be communicated to the client devices through the in-home delivery IP module 820 . Screen displays, including playlists or content available lists may be provided. It should be noted that the content may be changed to an IP protocol which may be sent over a MoCa® format, through Wi-Fi or through an Ethernet connection. Referring now to FIG. 9 , a gateway device is coupled to a video content source such as a satellite source or a cable source in step 910 . The satellite source and cable source are broadcast sources. However, they may also be on-demand and pay-per-view sources. The gateway device is also coupled to a broadband source in step 912 . As mentioned in FIG. 8 , the broadband source and the satellite source may be in communication with an aggregation and transformation module 466 . In step 914 content is stored in a digital video recorder or other memory associated with the gateway device. In step 916 a screen display is generated that aggregates the content from the satellite source and the broadband source as well as the content that is stored within the digital video recorder. By providing content that is available from the broadband source, satellite source and video recorder, an increased amount of content is available. In step 918 a listing request for content menu is received at the gateway device from the user device within the local area network. In step 920 an identifier for the user device and/or the capabilities of the user device is received at the gateway device. In step 922 a local area network condition is determined. A condition may correspond to the amount of content being communicated through the system. That is, the overall bandwidth use at the current time may be one condition. Another condition of the local area network may be the forecasted amount of use. For example, certain actions may be scheduled to be performed. The type of traffic may be another condition. Some traffic may have higher priority than other traffic. For example, the network signals for current streaming video traffic may have a higher priority over data analysis communication to a remote site. The resources of the gateway are reserved in response to the network condition, the gateway conditions and the request for content. This may be performed by the media processor sending a request “reservation signal” to the network processor in step 924 . The network processor may send a response signal, an acknowledge signal (ACK) or a not acknowledge signal (NAK), depending upon the real time availability of bandwidth to satisfy the request. After step 924 , a response signal is generated by the network processor. In step 928 it is determined whether the response signal is an acknowledge signal (ACK). When the response signal is a not acknowledge signal (NAK), step 930 adjusts the media type delivered to lower the bandwidth. Referring back to step 928 , when the response signal is an acknowledge signal (ACK), step 932 allocates or reserves the gateway resources. For example, bandwidth may be reserved at the gateway device as an allocated resource for communicating the requested content based upon the condition. As mentioned above, various conditions may allow for increased or decreased bandwidth allocation for a particular piece of content. The reservation of bandwidth may be immediate or for future bandwidth. Other allocated or reserved resources include hardware queues, software queues, runtime memory, ingress/egress buffer resources, hardware accelerator processing queues and CPU timeslices. After steps 930 and 932 , step 934 shapes and conditions the communication of the content at the gateway device. The conditioning may be done based on the content type. For example, movie content may be conditioned differently than internet news updates. The amount of bandwidth used by content may be allocated. However, when bandwidth becomes available, content bandwidth allocations may be increased for other content. The content may also be shaped. Shaping the content means changing or restricting some flow signals through the gateway device while allowing other flow signals to maintain maximum speed. In step 936 the content may be converted into an IP protocol. In step 938 the content in the IP protocol is communicated through the local area network to the requesting device. The content may be stored or displayed at the requesting device in step 940 . Referring now to FIG. 10 , the request for content or service through the local area network may be formatted in a particular format. A request message 1010 may include an originator ID 1012 that corresponds to a unique identifier for the user device. A time stamp 1014 may also be provided within the service request message. The time stamp may correspond to the time that the message was communicated. A message type 1016 may also be included within the service request message 1010 . The message type may include various types of messages including a request for content from the satellite device, a request for content from the browser, a request for information or other types of requests. The service request message may also include a message 1018 that includes various data that triggers a response at the gateway device. For example, a request for content may include a content identifier within the message 1018 . Referring now to FIG. 11 , a block diagrammatic view of the layer two switch or router in relation to the media processor module 510 is set forth. The router 546 is in communication with TR-069 module 1110 and an XMPP module 1112 . The TR-069 module 1110 defines an application layer protocol for remote management of end user devices. The TR-069 module 1110 provides communication between the router and various configuration servers. The XMPP module 1112 is an extensible markup language for communications protocol that is message-oriented. It is used for various types of signaling and the like. The TR-069 module 1110 and the XMPP module 1112 represent communication to various devices outside of the gateway device 22 through the wide area network 50 . The router 546 communicates to external devices using a remote interface handler module 1120 . The router 546 communicates to the media processor 410 A through a local interface handler module 1122 . The local interface handler module 1122 communicates through a shared interface module such as a shared memory/local IPC module 1124 . The shared memory may be one of the types of memories described above relative to FIG. 4 or 5 . The shared memory/local IPC module 1124 represents a shared communication channel between the router module 546 and the media processor module 410 A. The router 546 also includes a gateway abstraction module 1126 . The gateway abstraction module 1126 is used for abstracting signals that are communicated through the remote interface handler module 1120 and the local interface handler module 1122 . The gateway abstraction module 1126 is in communication with the router control plane 1130 . The router control plane 1130 is in communication with the router data plane 1132 . The gateway abstraction module 1126 is also in communication with a system management module 1134 . The system management module is in communication with a configuration interface 1136 . The router control plane 1130 includes a protocol serializer/deserializer module 1140 and a validation module 1142 . The protocol serializer/deserializer module 1140 deserializes the control signals from the gateway abstraction module 1126 in route to the router data plane 1132 . The protocol serializer/deserializer module 1140 serializes the signals from the router data plane en route to the gateway abstraction module 1126 . The system management module 1134 and the configuration interface module 1136 are used for managing the gateway abstraction module 1126 and controlling the routing of signals to the remote interface handler module 1120 or the local interface handler module 1122 . The media processor 410 A may include DVR modules 1150 and gateway application modules 1152 . The DVR modules 1150 store various media content as described above. The DVR modules 1150 may include the DVR itself as well as the control for storing data within the DVR module 1150 and retrieving the data for the DVR module 1150 . The application modules 1152 provide various controls or perform various functions for the gateway device 22 . Various types of application modules 1152 may be provided depending upon the functions desired in the gateway device 22 . Applications may be used to provide information to the gateway device 22 or may be used to obtain information from the gateway device 22 . A gateway local interface 1154 communicates between the shared memory/local IPC module 1124 and the DVR modules 1150 and the application modules 1152 . In general, the DVR modules 1150 and the application modules 1152 request various services such as the creation of quality of service flows using a quality of service flow signal, teardown of quality of service flows, dynamic session creation and session deletion signals, removal of firewall entities for certain protocols, query for internet connectivity, providing TR-069 data to the router 546 for consolidation, varying router/networking parameters, providing notification to the router 546 of DLNA/streaming events (creation and deletion of streams) and providing notification to the router of RVU session and creation/deletion. The service request signals may have a timestamp for bookkeeping purposes. The shared memory/local IPC module 1124 receives serialized communication signals from the protocol serializer/deserializer module 1140 by way of the local interface handler/module 1122 . The gateway local interface 1154 communicates serialized signals from the DVR modules 1150 and application modules 1152 through the local interface handler module 1122 , the gateway abstraction module 1126 and the router control plane 1130 . In particular, the serialized signals from the media processor 410 A are deserialized at the protocol serializer/deserializer module 1140 . The gateway local interface 1154 includes a protocol serializer/deserializer module 1156 that serializes and deserializes the content between the media processor 410 A and the router 546 . As mentioned above, the commands and data from the DVR modules 1150 and the application modules 1152 are serialized to form serialized request signals for communication to the router module 546 . The media processor 410 A may perform various functions based on the response signals from the router. A screen display with data received from the router 546 may be generated. A pair of IP protocol stacks 1160 , 1162 may be bypassed when communication signals are exchanged between the media processor 410 A and the network processor 410 B and more specifically the router 546 , although two IP protocol stacks 1160 , 1162 are illustrated. Referring now to FIG. 12 , a signal flow between the media processor 410 A and the router 546 are set forth. The application module 1152 , in this example, generates a serialized service request or query 1210 that is serialized at the protocol serializer/deserializer 1156 . In the following example, a service request signal is provided. The service request signal may include an identifier such as an originator identifier to uniquely identify the originator of the request. Of course, a query request signal may also be substituted for the service request. The protocol serializer/deserializer 1156 generates a serialized request signal which is a binary request that is communicated to the local shared memory/IPC module 1124 . The local IPC module 1124 communicates the content to the local interface handler 1122 . Because the signal is intended for use within the gateway device 22 , the remote interface handler 1120 is bypassed. The shared memory/local IPC module 1124 transports the serialized binary request signal 1212 through a local secure shared memory as indicated by the arrow 1214 . Step 1216 hands off the protocol from the local interface handler 1122 to a protocol serializer/deserializer module 1140 . The protocol serializer/deserializer 1140 deserializes the signal. The validation module 1142 may validate the deserialized signal. The deserialized signal is sent from the validation module 1142 to the gateway abstraction module 1126 which obtains the desired data. The gateway abstraction module queries or services requests through interaction with the underlying router components such as the router data plane 1132 . This is represented by the arrow 1222 . Ultimately, a response signal 1226 is generated at the gateway abstraction module 1126 . The gateway abstraction module 1126 returns the response signal 1226 to the local interface handler 1122 as represented by the arrow 1228 . The response signal is then communicated from the local interface handler 1122 to the shared memory/local IPC module 1124 as represented by arrow 1230 . The protocol deserializer 1156 receives the signal from the local IPC module 1124 as represented by arrow 1232 . The local protocol serializer/deserializer 1156 deserializes the signal and communicates the deserialized signal to the application module 1152 as represented by arrow 1234 . As mentioned above, one of the DVR modules 1150 may be substituted for the application module 1152 . Both the DVR modules 1150 and application modules 1152 may request data from sources internal or external to the gateway device through the router 546 . In response to the response signal a screen display may be generated in the gateway device from the media processor 410 A. As is described above, the router 546 and the media processor 410 A provide bidirectional communication using a sessionless request-response protocol that is serialized into a binary format over the communication channel between the two entities of the gateway device 22 . By providing bidirectional communication, the information between the router 546 and the media processor 410 A allow better quality of services, since all of the communication is controlled within the gateway device 22 . The transport of the signals is performed using a shared memory which bypasses an IP protocol stack of the gateway device 22 which allows more efficient communication. The message is serialized into a compact binary format. As mentioned above in FIG. 11 , the time stamp of the origination allows a recipient device to process the messages in a given window, and thus discarded stale messages outside of the window are not processed. The system management module 1134 determines whether the systems fall outside a pre-established window for processing. By providing both a remote interface handler module 1120 and a local interface handler module 1122 , messages may be easily and quickly processed depending upon the desired direction and origin. The gateway abstraction module may also communicate an asynchronous response 1236 to the media processor 410 A. The responses may come to the media processor after a significant delay related to a request and events that occur. Referring now to FIG. 13 , the cache module 450 of FIG. 4 is set forth in further detail. The cache module 450 has access to all the packet flows through the gateway device 22 . Statistics may be collected for a variety of browsing habits based upon the use of the satellite tuner and front end as well as non-video or non-satellite, such as web browsing. Ultimately, the cache module 450 may be used for a variety of purposes including, but not limited to, the caching of audio, video and web pages to enhance the user's experience and reduce latency. The cache module 450 may also be used to pre-position video advertisements (ads) within the gateway device to reduce the wide area network traffic when using specific applications. For example, ads may be requested at a mobile device when the user is within certain applications. By pre-positioning advertisement content within the gateway device, the reliance on the interface with the wide area network is reduced. Further, by providing an improved user profile that includes both web browsing and video content use, a more complete user profile is obtained. Further, a reduced amount of caching compared to caching everything may be obtained. The cache module 450 includes a packet engine 1310 that is used to analyze the packets being exchanged within the gateway device 22 . The packet engine 1310 monitors the packets leaving and entering the gateway device 22 from the wide area network. The packet engine 1310 also receives satellite television packets to determine the content being watched. Local area network packets may also be monitored. The packet engine 1310 provides raw data to an analysis engine 1312 . The analysis engine 1312 may analyze the content and generate a viewer tracking log 432 A and the browser tracking log 432 B. The analysis engine 1312 may also be in communication with a user profile generator 1320 A or 1320 B. The user profile generator 1320 A may be located external to the gateway device 22 through the WAN. The user profile generator 1320 B may be located within the cache module 450 or another part of the gateway 22 . The functions of the user profile generator 1320 A, 1320 B may be similar and thus referred to as a user profile generator 1320 in the following description. The user profile generator 1320 may generate a user profile based upon the viewer browsing and viewer watching profiles. By analyzing the packets into and out of the gateway device 22 , the user profiles may be generated. User profiles may be generated in various ways, including storing searches from a web browser, storing searches within a video streaming service, storing identifiers from streaming video services, and storing web page identifiers for browsed web pages. User profiles may be generated for individual users or more generally for the overall users of the system. The profiles may be time based, as mentioned above. Predetermined time periods may correspond to different periods of use, thus different types of content may be browsed or different users may view different types of programming. For example, time periods may be every hour, half hour or a multi-hour time block. The analysis engine is in communication with a cache engine 1330 . The cache engine 1330 acts upon content that is cached and pre-positions content within the gateway device 22 . The cache engine 1330 may store content within the memory 430 of the gateway device as illustrated in FIG. 4 . The cache engine 1330 may cache web pages. For example, if a user browses FoxNews.com after arriving home between 6 pm and 6:30 pm on a weeknight, the cache engine 1330 may cache the web content just prior to the typical content use time. In another example, a particular website such as an internet mail website may be browsed to. The gateway device 22 may obtain the data from the webmail site and store the data within the gateway device 22 . The cache engine 1330 may control the obtaining of the cached data based upon user profiles, which in turn are based upon the viewer tracking log and/or the browser tracking log. The cached data provides an improved user experience since the data is available. The data may be obtained in the background, and the overall user experience may be improved by instantly providing content and not interrupting or reducing the speeds of other services to obtain the data. The cache engine 1330 may also be used to cache content from the satellite or other video providing services. By using the viewer tracking log and the browser tracking log, the user profile generator 1320 may generate a profile that is used by the cache engine to monitor the programming and store programming within the digital video recorder of the gateway device 22 . The cache engine 1330 may also be used for caching advertisements that are to be displayed during broadcast, during stored video content playback, or during web browsing. Pop-up windows and the like may be generated for advertisements that may be populated by advertisement content stored within the memory of the gateway device. The analysis engine 1312 may also be in communication with the recommendation module 420 illustrated in FIG. 4 . The recommendation module 420 may use the viewer tracking log 432 , the browser tracking log 432 B, the user profile from the user profile generator 1320 and analysis thereof to generate content recommendations for the user. Recommendations may be generated in various ways, including providing posters on a recommendation screen display or providing a simple list of recommended content titles. A short description or selector selection buttons for obtaining descriptions may also be displayed. Recommendations may be performed by reviewing the program guide data and the data stored within the digital video recorder of the gateway device 22 . The content recommendations are tailored to the user of the gateway device 22 through the analysis engine. Relevant data may best be provided to users regarding recommended video content. The cache module 450 may also include an in-memory cache 1332 . The in-memory cache 1332 may increase the performance of the gateway by holding frequently-requested data in the memory and therefore reducing the need for database queries or other queries to obtain the data. The cache module 450 may also include a secondary disk cache 1336 . The secondary disk cache 1336 may be used to improve the time it takes to read or write to the digital video recorder (hard disk). A primary disk cache may be included within the digital video recorder. By providing a secondary disk cache 1336 , overall system performance may be improved. The cache module 450 may also include a cleanup module 1338 . The cleanup module 1338 may perform heuristics based on cleanup of cache content when a maximum amount of content (watermark) is reached. Referring now to FIG. 14 , a method for generating content recommendations is set forth. In step 1410 television content, LAN content and wide area network (WAN) content are communicated through a packet engine of the gateway device 22 . In step 1412 , television viewing history is stored at the viewer. This may be performed in the viewer tracking log. In step 1414 the browser history is recorded at the browser tracking log of the gateway device. The browser history may be both of the wide area network and the local area network. The browsing tracking log and the viewer tracking log may be analyzed in the analysis engine to form a user profile in step 1416 . The user profile may be determined within the gateway device 22 or may be exported to an external profile generator that is communicated to through the wide area network. In step 1418 the gateway device performs an action in response to the user profile. In this example, a first action is set forth in steps 1420 to 1424 . In step 1420 a request signal for content that corresponds to the user profile is generated. The content may be cached in step 1422 . In step 1424 the user may select the content and play back the content at the gateway device. The content obtained and played back in steps 1420 - 1424 corresponds to video-on-demand content that can be requested at any time. Steps 1410 through 1418 may also be used to obtain broadcasted content. Broadcasted content is content that is broadcasted on a predetermined channel at a predetermined time. A second example of an action is set forth in steps 1430 to 438 . Step 1430 compares program guide data that corresponds to broadcasted content to the user profile. In step 1432 a recommendation list may be provided in response to the step of comparing. In step 1434 content may be cached that corresponds to recommendations or the user profile. The cached list of content may be displayed in step 1436 . After step 1436 , step 1438 allows the content to be selected and the content played back. That is, the list of content may be displayed at the gateway device or at a requesting user device or client device. The content selection signal may be generated at the client or other user device and communicated to the gateway device. The gateway device may retrieve the content for playback from within the memory of the gateway device. The content may be processed according to the capabilities of the requesting device. The content may be displayed on a display associated with the requesting device. Referring now to FIG. 15 , a method for generating a browser profile is set forth. In step 1510 the search terms, website identifiers or other identifiers are entered in to a web browser or other wide area network interface. In step 1512 content may be viewed on a website through the gateway device. In step 1514 packet flow signals are monitored through the gateway device. Step 1516 provides television content to step 1514 so that flow signals of television content and content on websites are monitored. In step 1516 content may be cached according to the user profiles developed in response to browser activity or television viewing activity or both. Cached content is stored in the memory of the gateway device in step 1516 . Step 1518 communicates the packet data to an analysis engine which analyzes the data. In step 1520 the analysis engine may communicate the analyzed data to the profile generator. The profile generator may be web based or may be gateway device based. In step 1522 a user profile for the gateway device is generated. In step 1524 the user profile is communicated to the recommendations engine. In step 1526 a screen display may be generated for recommendations that comprise a recommendations list. Broadcast content may also be recommended as well as stored content. Broadcast content may also be recommended for future content. Referring back to step 1522 , step 1530 may provide the user profile to a cache engine. The cache engine may obtain content such as browser content or other video content based upon the user profile in step 1532 . Web based video content may also be stored within the gateway device in response to the user profile. The web based content may also appear in the recommendations list. Referring now to FIG. 16 , a screen display for a web browser 1610 for a webpage “Benssnowcones.com” is illustrated. The web browser 1610 is displaying a web page (browser display) for the business Ben's Snow Cones. Ben's Snow Cones may also have a cached advertising window 1612 . The cached advertising window 1612 may present a cached advertisement that was prestored by the cache engine. The cached advertising may also be related to the subject matter of the website. For example, the cached advertising window 1612 may be related to food or food services. Ben's Snow Cones is a food service related website. Referring now to FIG. 17 , the cached advertising content may be communicated in a video signal 1710 . The video signal 1710 may include video information 1712 and a trigger 1714 . The trigger may cause the gateway device 22 to retrieve cached advertisement content from the memory of the gateway device. The cached advertisement is inserted in the advertisement area 1716 of the video signal that is to be displayed on the display. The video then resumes at 1718 after the advertisement has played out. Referring now to FIG. 18 , user interface 1810 having a current broadcast portion 1812 entitled “What's On” and a recommended recording portion 1814 . The content in the portion 1814 is content that is either fully or partially recorded within the gateway device 22 . As mentioned above in FIG. 4 , the recommended recordings may be stored in a user partition of the DVR 446 of gateway device 22 , a network partition of the DVR 446 of gateway device 22 or both. Recording indicators 1816 may be used to indicate the content is stored in the gateway device 22 . The content in the “What's On” portion 1812 , as described above, is what is currently broadcasting in the present time slot and is recommended by the recommendation engine. For example, the “What's On” portion 1812 displays “ABC Nightly News”, “Sesame Street”, “Mythbusters”, and “Star Trek: The Next Generation”. In the recommended recording portions 1814 , “Up All Night”, “Louie”, “Doctor Who” and “Everybody Loves Raymond” is provided in a sorted content list. It should be noted that the recommended recording portion 1814 may be scrolled using the arrow indicator 1820 . A recommendations list may include the “What's On” portion or the recommended recordings, or both. Referring now to FIG. 19 , a movie recommendation screen display 1910 is illustrated displaying a recommended movie content list or at least part of a list based on the recommendation engine and the user profile. The recommendation engine may be used for recommending various types of content. “Movies” are only one specific type of content that may be provided. “Movie content” can easily be replaced with “Sports” or “Television Shows” as the specific type. In this example, a movie recommendations list 1912 is illustrated by using posters illustrating recommended movies. The list includes “Star Wars-Episode 1”, “Star Trek”, “The Matrix”, “Alien” and “Star Wars-Episode 2”. Referring now to FIG. 20 , a detailed block diagrammatic view of the connection bus 534 between the media processor 410 A and the network processor 410 B is set forth. In this example, a high speed connection 2010 is in communication with an IP interface 2012 of the media processor 410 A. An IP interface 2014 of the network processor 410 B is also in communication with the high speed connection 2010 . A low speed connection 2020 is also used to connect the media processor and the network processor. The low speed connection 2020 in this example has a message buffer line 2022 , a first reset line 2024 , a second reset line 2026 and an LED control connection 2028 . The message buffer line 2022 may be an RS-232 serial bus that communicates heartbeat messages between the media processor 410 A and the network processor 410 B. The heartbeat messages are communicated from a hardware watchdog module 2030 of the media processor 410 A. Heartbeat messages from the network processor 410 B originate from a hardware watchdog module 2032 . Heartbeat signals may be communicated from the hardware watchdog module 2030 to the hardware watchdog module 2032 and vice versa. That is, the network processor 410 B may monitor the condition of the media processor 410 A and the media processor 410 A may monitor the operation of the network processor 410 B. The message buffer 2022 in this example provides 100 kbps speed therethrough. However, other amounts of data may be communicated therethrough. Boot time messages may be communicated through the message buffer 2022 . However, notifications between the processors are the main function. The heartbeat signal communicated through the message buffer 2022 may include, but is not limited to, the CPU load, an interface load data, temperature data for various components, such as the Wi-Fi chip temperature, the hard disk drive temperature, the status of the high speed connection 2010 , the amounts of memory being used and other desirable processing data. Ultimately, one watchdog module waits to receive a heartbeat signal from the other watchdog module. The receiving processor enters a state/state machine transition that performs certain heuristics before deciding the other processor is in a non-recoverable state. The one processor that is still operating may trigger a hardware reset through one of the reset lines 2024 , 2026 . A reset module 2034 is located within the media processor 410 A to generate the reset signal from the media processor to the network processor 410 B. A reset module 2036 communicates a reset module to the media processor 410 A through the reset line 2026 . The LED control connection 2028 may be used to illuminate LEDs 2038 and 2040 associated with the respective media processor 410 A and network processor 410 B. Referring now to FIG. 21 , a simplified method for operating the low speed connection between the media processor 410 A and the network processor 410 B is set forth. In step 2110 content is communicated between the network processor 410 B and the media processor 410 A through the high speed connection 2010 as illustrated in FIG. 20 . In step 2112 , boot time is communicated between the media processor 410 A and the network processor 410 B at boot up time in step 2112 . This is performed using the low speed connection 2020 . In particular, the message buffer 2022 may be used for communicating the boot time data therethrough. In step 2114 a first heartbeat signal and a second heartbeat signal are generated by the respective media processor 410 A and the network processor 410 B. The hardware watchdog 2030 generates the first heartbeat signal at the media processor 410 A and the hardware watchdog 2032 generates the second heartbeat signal at the network processor 410 B. In step 2116 the first heartbeat signal is communicated from the media processor to the network processor through the message buffer interface 2022 . In step 2118 the first heartbeat signal is processed at the network processor. The lack of a heartbeat signal may also be processed at the network processor. That is, the network processor may be expecting a heartbeat signal after a predetermined amount of time. In step 2120 it is determined whether the media processor is in a non-recoverable state. If the media processor is not in a non-recoverable state, the system returns to the beginning or to step 2114 in step 2122 . When the media processor is in a non-recoverable state in step 2120 , a reset signal is generated at the network processor at the reset module 2036 and communicated through the reset line 2026 to the media processor. The reset signal is communicated in step 2124 . The media processor 410 A is then reset in step 2126 . Referring back to step 2114 , after the second heartbeat signal is generated step 2130 communicates the second heartbeat signal from the network processor to the media processor through the message buffer interface 2022 . The second heartbeat signal is processed at the media processor. In step 2134 it is determined whether the network processor is in a non-recoverable state. When the network processor is not in a non-recoverable state, the system returns in step 2136 . Returning may mean returning to step 2114 or step 2110 , depending on the conditions. In a normal monitoring mode step 2114 is repeated. In step 2134 when the network process is in a non-recoverable state, step 2136 is performed. In step 2136 a reset signal is communicated to the network processor from the media processor. In step 2138 the gateway processor is reset. By providing a hardware reset, the other processor can control the data and the resetting of the other processor. When one processor fails, the user is not required to repower, restart or reset the entire gateway device 22 . Referring now to FIG. 22 , a method for gateway multiprocessor collaboration is set forth for reducing or shaping flows in the gateway device 22 . The components in FIG. 5 are also referenced. The components of interest are the network processor 410 B, a first Wi-Fi module 540 A and a second Wi-Fi module 540 B. The first Wi-Fi module 540 may be referred to as a first communication system or first modem, and the second Wi-Fi module 540 B may be referred to as a second communication system or second modem. The first Wi-Fi module 540 A communicates data to the network processor 410 B as indicated by the arrow 2210 . The data signals may be referred to as first network signals or WAN signals. The second Wi-Fi module 540 B communicates data to the network processor 410 B as indicated by the arrow 2212 . Again, the data signals may be referred to as second network signals. Various types of data may be communicated to the network processor 410 B. Some of the data is intended for the wireless area network module 552 . Both of the Wi-Fi modules 540 A and 540 B may be referred to as a system-on-chip. Both of the Wi-Fi modules 540 A, 540 B attempt to buffer packets when system congestion is detected. In the following example, a process for collaborative notification is set forth in which the network processor notifies the first Wi-Fi module 540 A and the second Wi-Fi module 540 B so that the conditions may be throttled. Further, a network signal identifier may also be communicated to or from the network processor 410 B to the first Wi-Fi module 540 A. Individual streams or network signals may be reduced in speed. Thus, certain network signals may be reduced and other network signals not reduced. That is, lower priority network signals may be reduced in speed while high priority network signals may not be reduced in speed. This is referred to as shaping of a stream or streams. Advantageously, this allows the Wi-Fi modules to react to the notification rather than transmitting at a high speed and throttling independently. Performing independent throttling results in a sawtooth effect where the Wi-Fi chips increase speed, then quickly decrease speed, then increase speed. This results in a perceived poor behavior by the users. This behavior is reduced or eliminated. A second congestion notification request signal 2216 is communicated to the second Wi-Fi module 540 B. The first congestion notification signal 2214 and the second congestion notification request signal 2216 are generated in view of each other. That is, the network processor 410 B presents an overall strategy to provide improved system performance. A first response signal 2220 is received by the network processor 410 B from the Wi-Fi module 540 A. A second response signal 2222 is received at the network processor 410 B from the second Wi-Fi module 540 B. In response to the congestion notification signals, a first data signal 2228 is not modified, but a second data signal 2230 from the second Wi-Fi module 540 B is reduced. This is illustrated by the thickness of the arrow representations of the data signals 2228 and 2230 . By coordinating the individual network signals or streams from different Wi-Fi modules, the network processor 410 B can improve the overall performance of the system during heavy load conditions. The network processor 410 B can thus improve the performance through the local area network as well as to the wide area network. Referring now to FIG. 23 , a detailed method for operating a network processor 410 B and Wi-Fi modules 540 A and 540 B is set forth. In step 2310 network signals are communicated between the network processor 410 B and a first Wi-Fi module and a second Wi-Fi module. Each Wi-Fi module may buffer content in step 2312 . That is, each Wi-Fi module may buffer output data that is destined for the network processor 410 B. However, as mentioned above, individual buffering may not be advantageous to the overall system performance. Therefore, the present example provides a method for coordinated control of the Wi-Fi systems using the network processor and the quality of service requirements set forth therein. In step 2314 a first congestion notification request signal is communicated to the first Wi-Fi system with notification data. As mentioned above, the notification data may limit individual network signals or streams of a plurality of streams being received from the first Wi-Fi module. In response to the notification data, step 2316 throttles and shapes lower priority network signals based on the notification data at the first Wi-Fi module. First modified network signals are generated and communicated in response to throttling and shaping. In step 2318 a second congestion notification request signal is communicated to the second Wi-Fi module with notification data. In step 2320 the second Wi-Fi module has its network signals throttled and shaped based on the notification data. Throttled and shaped streams are then communicated from the first Wi-Fi module to the network processor 410 B in step 2322 . In step 2324 throttled and shaped network signals are transmitted from the second Wi-Fi module to the network processor 410 B. Second modified network signals are generated and communicated in response to throttling and shaping. The process may be repeated as the network processor 410 B monitors the overall system streams and the amount of data flowing therethrough. Referring now to FIG. 24 , the self-learning module 468 of FIG. 4 is illustrated in further detail. The self-learning module 468 may monitor various parameters of the gateway device 22 . A network traffic monitor 2410 monitors network signals, flows or streams through the gateway device 22 . The network traffic may include the upstream traffic, the downstream traffic and the wide area network to local area network traffic. A processor usage monitor 2412 may monitor the amount of processor processing usage being used at a particular time period. A local network traffic monitor 2414 is used to monitor local IP traffic, including LAN to LAN streams and Wi-Fi streams. A video from satellite source monitor 2416 monitors the video from the satellite source. The amount of tuners being used and the overall processing of the satellite signals may be monitored. In this example, a satellite signal is used. However, if the system is implemented in another type of system, such as an optical fiber or cable system, the video may be monitored from the cable or optical fiber source. A recordings monitor 2418 monitors the use of recordings. The recordings monitor 2418 may monitor the saving or storing of content from various sources to a digital video recorder. The recordings monitor may also monitor the playback of content from the digital video recorder. A remote control activity monitor 2420 may monitor the activity from an input 125 as illustrated in FIGS. 2 and 4 . The remote control activity monitor provides an indication whether the system is being used and how much burden is being placed upon the system. A power save mode monitor 2422 may also be included in the self-learning module 468 . The power save mode monitor 2422 monitors the activation of a power save mode of the gateway device 22 . The power save mode may correspond to a period of inactive use. The activation of the power save mode is inverse to the amount of data retrieved from the other monitors. That is, when the power save mode is activated, there is little or no use of the gateway device 22 . A trend module 2430 keeps track of various customer behaviors and activity by monitoring the various monitors 2410 through 2422 . The trend module 2430 may determine a high water mark, a low water mark and an average water mark for each of the monitor parameters. The trend module 2430 is in communication with a time slot monitor module 2432 . The time slot monitor may keep track of the activity within certain time slots. Time slots may correspond to a predetermined time period. For example, a day may be broken down into 24 time periods representing 24 hours. Other time periods may also be used, such as half day or eighths of a day. A self-healing and diagnostics module 2434 is in communication with the trend module 2430 and the time slot monitor module 2432 . The self-healing and diagnostics module 2434 is used for self-healing and/or diagnostics by correcting various operational issues within the gateway device 22 . Operational issues within the gateway device may include system slowdown due to software code memory leaks, erroneous programming that results in system sluggishness over time, inconsistent state machines, system configuration, coding bugs, etc. that require various action or software resets. Other operational issues include non-terminating processes of the network processor or the media processor. The self-healing and diagnostics module 2434 may also determine a timing of self-healing or diagnostics by monitoring the various times and usage patterns. The times for performing self-healing and diagnostics may change depending upon the usage. Various customers may have different usage patterns and thus the timing for diagnostics and self-healing may vary between customers. Diagnostics may initiate an analysis of the parameters or may result in the communication of parameters to an analytics module. Referring now to FIG. 25 , the amount of local traffic, network traffic and video from satellite source traffic is illustrated in a plot having 35 time slots. The time slots correspond to a week of time slots. Each day is therefore broken into 5 time slots in this example. As mentioned above, various numbers of time slots may be used depending upon the desired resolution. As can be seen between time periods 3 and 5, most of the activity appears to be performed. This would likely be an inopportune time to perform self-healing. It should be noted that only three parameters are illustrated in FIG. 25 . All or more than the parameters set forth in FIG. 24 may be monitored and plotted. A plot may be provided within the system so that a determination may be made as to when self-healing or diagnostics should be performed. Referring now to FIG. 26 , one of the parameters, such as network traffic is illustrated in a pattern matrix. The network traffic corresponds to the peak use parameters. However, other parameters may also be monitored, such as the low water mark, high water mark or average use. Various time slot graphs for all the parameters and all the averages, low and high water marks, may be determined. By monitoring all the parameters, a customer usage pattern will be evident. This pattern allows a decision to perform self-healing through the self-healing and diagnostics module 2434 of FIG. 24 . Referring now to FIG. 27 , a method for performing self-healing is set forth. In step 2710 the gateway device is started. The self-learning module 468 is started in step 2712 . In step 2714 various parameters, such as those set forth in FIG. 24 , are monitored. In step 2716 various trends are monitored and stored by the trend module 2430 and the time slot monitor 2432 of FIG. 24 . Both the trend information and the current information from the various parameters set forth in FIG. 24 are provided to step 2718 . Step 2718 determines whether or not to allow self-healing. The system may also start diagnostics or self-healing at block 2720 . Block 2720 initiates diagnostic self-healing at step 2718 . In step 2718 it is determined whether or not to perform diagnostics or self-healing. If diagnostics are performed, step 2726 performs self-healing or diagnostics. After step 2726 , step 2728 is performed. Step 2728 may also be performed after step 2718 and a decision not to allow diagnostics or self-healing is performed. In step 2728 a next potential time slot for self-healing or diagnostics is determined. The process set forth in FIGS. 24 through 27 may be performed periodically on a regular basis or on an irregular basis. Referring now to FIG. 28 , a method for applying quality of service (QoS) to communication signals in devices not having deep packet inspection capabilities is set forth. The present example applies to one device with deep packet inspection and another device with non-deep packet inspection. In this example, a gateway device 22 is the deep packet inspection device and the set top box 38 , without deep packet inspection, is set forth. However, those skilled in the art will recognize that this applies to various systems outside gateways and set top boxes. The gateway device 22 may include the router 546 illustrated in FIG. 1 , a dynamic host configuration protocol (DHCP) server 2820 and a universal plug and play (UPnP) listener 2822 . The DHCP server 2820 serves clients on the local area network side. A DHCP client module 2821 is used to receive IP addresses from the internet service provider on the wide area network side. The router 546 includes a deep packet inspection module 2824 . The deep packet inspection module 2824 may be used to look into communications packets passing through the router 546 . Various layers may be inspected, for example, layers 2-7. Useful data may be obtained from the inspected packets, such as a packet flow type, a device identifier corresponding to where the flow is generated or consumed, the bandwidth needed for the flow and characteristics of the flow. Enhanced services and application of quality of service policy to the stream may be performed. The deep packet inspection process may be performed automatically. The router 546 may also include a quality of service (QoS) module 2826 . The QoS module 2826 applies a quality of service policy to the communication signals passing therethrough. Communication signals that are en route to or from the first device (STB) may have the QoS policy applied thereto. The second device, using deep packet inspection, identifies the communication signals en route to the first device and applies the QoS policy because the first device is unable to perform deep packet inspection. For example, a media stream en route to the first device may be identified as a media stream and give a higher priority by applying QoS policy. The set top box 38 may include a DHCP client 2830 , a universal plug and play broadcaster 2832 and a media generator 2834 . The exchange of data through a network 2836 is set forth. The set top box 38 is not a deep packet inspection device. A routing and forward module 2828 may also be included within the gateway 22 . The routing and forwarding module may be used to route content based on a routing table to its desired destination. The forwarding aspect of the forwarding module 2828 allows the gateway 22 to forward content not destined for the gateway system. The forward and routing module 2822 thus also forwards content toward its desired destination. Referring now to FIG. 29 , flow identification between a deep packet inspection device and a non-deep packet inspection device is set forth by way of example through the gateway device 22 and the set top box 38 illustrated in FIG. 28 . In this example, standard DHCP address allocation takes place in step 2910 . The DHCP steps illustrated in FIG. 29 include standard steps including a DHCP discover request signal that is communicated from the client to the server, a DHCP offer signal that is generated in response to the DHCP discover request signal from the service to the DHCP server 2820 to the DHCP client 2830 . A DHCP request is communicated from the DHCP client 2830 to the DHCP server 2820 . A DHCP acknowledge signal is communicated from the DHCP server 2820 to the DHCP client 2830 . In step 2912 , a DHCP signal is communicated with a vendor-specific extension (VSE) that encapsulates information about the flow signals between the devices. An IP address and a port identifier describing the flow signal may be communicated in the signal illustrated in FIG. 2912 . The DHCP server 2820 applies quality of service rules of the quality of service policy to the DHCP signal with the vendor-specific extension in step 2914 . The application of quality of service rules is performed in the router 546 . The router is a peer device. The router 546 interprets the DHCP signal with the VSE and maps the flow signals to the quality of service policy. Because all devices are not routers, the two devices perform UPnP broadcasts informing each other about their flows and the description. In step 2916 a UPnP broadcast is communicated from the UPnP broadcaster 2832 to the UPnP listener 2822 . The IP address and flow signal descriptions of the consumer or set top box 38 are provided in the data of the UPnP broadcast signal. The UPnP listener 2822 of the gateway device 22 communicates the UPnP broadcast signal with data to the router where quality of service flows are determined in step 2918 . In response to the UPnP broadcast signal received at the UPnP listener 2822 , the UPnP listener 2822 communicates a UPnP acknowledgement signal 2920 to the UPnP broadcaster. After the UPnP acknowledgement in step 2920 , the router 546 applies quality of service treatment to signals exchanged between the router and the media generator 2834 . The signal 2930 represents a modified media stream by the router whose parameters have been modified so that deep packet inspection may be performed and the quality of service, typically not available at the set top box, is accomplished. Referring now to FIG. 30 , a detailed method for allowing quality of service to be applied to a device not capable of deep packet inspection is set forth. In step 3010 , a first device is booted up. In this case, the first device corresponds to the set top box. In step 3012 a broadcast discover request is generated and communicated to the DHCP server of the gateway device. In step 3014 , a discover request signal is received at the second or gateway device. In step 3016 and IP address, a port identifier and a flow signal description for the first device is communicated to the second device. That is, the IP address for the second device is communicated to the first gateway device. In step 3020 the first device, such as the set top box, communicates a DHCP request signal with a vendor-specific extension attached thereto to the second device. The vendor specific extension may include an IP address of the first device, a communication port for communicating to the first device and a flow or communication signal description. The second device, such as the gateway device, sends an acknowledge signal to the first device in step 3022 . Quality of service policy is applied to the flow signal in the second device or gateway device. That is, the quality of service policy is applied to the communication signal communicated to the first device through the second device in step 3024 . Because all devices are not routers, a UPnP broadcast signal is also broadcasted from the first device to the gateway device (second device). The UPnP broadcast signal has vendor-specific data coupled thereto. The vendor-specific extension data may be the same data as was communicated in step 3020 . That is, the UPnP broadcast signal may include an IP address, a port identifier and a flow or communication signal description of the communication signal of the first device in step 3026 . The UPnP broadcast signal also informs other devices about the communication signal description due to the broadcast structure of the UPnP signal. Devices that are hosts as well as routers can see the broadcast on the subnet. The UPnP broadcast signal is used to map the data to the communication signals in step 3028 . In step 3030 the communication signal is identified. A quality of service policy is applied to the communication signal to form a modified communication signal. In step 3030 modified communication signals destined for the first device are communicated with the router of the second device with an applied quality of service policy applied thereto. The modified signals are processed with the second device and a function is performed in response to the modified communication signals in step 3034 . The function may be displaying the content corresponding to the communication signals or displaying a web page. Referring now to FIG. 31 , a simplified view of the gateway device 22 is illustrated with the media processor 410 A and the network processor 410 B. The gateway device 22 is shown relative to a monitoring module 3110 . The monitoring module 3110 is shown as an external monitoring module. However, the monitoring module 3110 may optionally be implemented within the network processor 410 B of gateway device 22 , as indicated by monitoring module 3110 ′. The monitoring module 3110 includes a real time monitor 3112 and an analytics/visualization module 3114 . The analytics/visualization module 3114 is in communication with a display 3116 . The real time monitor 3112 may be in communication with the cloud system function analytics module 74 . The cloud analytics module may also have a display 3122 . If the monitoring module 3110 is outside the gateway device, a secure shell (SSH) connection 3130 is used to connect the real time monitor 3112 and the media processor 410 A. When the monitoring module 3110 is outside of the gateway device 22 , a secure shell connection 3132 is formed between the real time monitor 3112 and the network processor 410 B. The real time monitor 3112 extracts low level data and stores it in a database 3134 . The analytics and visualization module 3114 is in communication with the database 3134 and provides real time visualization of multiple parameters at the display 3116 . By monitoring various parameters, the details of the information and the relation of the data may be visualized. In general, the monitoring module 3110 has low overhead in terms of a memory footprint and processor resource consumption does not result or skew the behavior of the gateway device 22 during monitoring. A number of network interfaces may be monitored, as well as individual flow or network signals within the gateway device 22 . The containers and the container process data, and the threads associated with the memory, processor and bandwidth connection may also be monitored. The secure shell connections 3130 and 3132 create a single session with the multiprocessor imbedded system. The low level interfaces of the gateway device 22 are used to collect targeted data. The monitoring module 3110 may be used to obtain a one-time collection of data or provide a repeated collection of data even down to every few seconds. It should be noted that the analytics and visualization module 3114 generally provides the data to the display 3116 in real time. Analytics may also be pushed to the cloud system function analytics module 74 or displayed on a display 3122 . Functions monitored by the monitoring module 3110 may include, but are not limited to, the network processor functions, the media processor functions, the Wi-Fi and MoCa® interfacing, the packet processing and DVR communication. Referring now to FIG. 32 , the operation of the monitoring module 3110 relative to the gateway device 22 is set forth. In step 3210 , a secure shell (SSH) connection is formed between the real time monitor and the media processor. An SSH connection may also be formed between the real time monitor and the network processor if the real time monitor is located outside the network processor. In step 3212 data about low level interfaces is requested by the real time monitor using a request signal. In step 3214 data is communicated from the gateway device 22 to the real time monitor and ultimately to the database 3134 of the monitoring module. In step 3216 the analytic/visualization module is notified of the new data within the database 3134 . This may be performed with little delay so data may ultimately be displayed in real time. In step 3218 a screen display generating a visualization engine with past data and new data is displayed. By displaying both past data and new data, trends may be monitored. In step 3220 data is displayed at a display associated with the monitoring module. An optional step, step 3222 , may be performed in which data is communicated to an external analytics module. The analytics module 74 may be a cloud based analytics module. Another optional step, step 3224 , displays data on a screen associated with the analytics module. Referring now to FIG. 33 , one example of a system view of the flow signals through the gateway device displayed by the display associated with the monitoring module is set forth. A gateway device 22 is shown with various input and output signals. FIG. 33 represents a screen display illustrated that is used for monitoring the flows. Input/output devices 3310 are illustrated at the left side and right side of FIG. 33 . The input/output devices 3310 have flow signals that enter or leave the gateway device 22 . The signals enter or leave through interface devices 3312 . The interface devices 3312 may include, but are not limited to, the network interface module 540 , the LAN module 550 and the WAN module 552 of FIG. 5 . The input/output devices 3310 illustrate the external devices to which the gateway device 22 is coupled. For example, various mobile devices and other interface or internet are represented. The interfaces 3312 may be labelled with actual interfaces, such as “LAN2” or may include a MAC address. In an actual example, the lines between the interfaces 3312 and the input/output devices 3310 may be color coded. In this example, the real-time and aggregate bandwidth of the flows through the system are indicated by differently shaped lines. Thus, higher bandwidth flows, such as those illustrated at the top of the document, are distinguished from lower bandwidth flows through the lower part of the figure. The interface devices 3312 may be virtual or physical devices. As described above, the interfaces may include WiFi, Ethernet, software, loopback and other virtual interface constructs. Referring now to FIG. 34 , another example of a screen display illustrating system views of accelerated and NAT flows through the network processor is set forth. Various devices and the flow signals associated therewith are monitored. Input/output devices 3410 are illustrated in communication with the network processor module 512 . The interface devices 3412 illustrate the interfaces in communication with the network processor module 512 , such as the local area network module 550 and wide area network module 552 . For example, LAN0, LAN3 and LAN5 are illustrated in some of the boxes. WAN0 is also illustrated. On the screen display, the devices may be selectable so that the actual speeds therethrough may be illustrated, such as those set forth below in FIG. 35 . Again, FIG. 35 also illustrates the various interfaces that may include, but are not limited to, WiFi, Ethernet, software, loopback and other virtual interface constructs. The different constructions of the line may correspond to different speeds therethrough. Referring now to FIG. 35 , a plurality of dials for measuring the bandwidth of various devices visible on a user interface from the analytical engine is set forth. Four speed displays 3510 are illustrated for different interfaces. In this example, eth1, eth2, WL1 and WL0 are displayed. However, speed displays for other interfaces may be set forth. For example, the loopback interface, coaxial interface, a bridge interface, software virtual interface, IPv6 tunnel interface, a chip specific virtual interface, GigE interface, a WiFi interface, a WAN interface and an Ethernet interface may all be displayed with similar data. The speed displays 3510 may be operator selectable, so that different speeds through different interfaces may be displayed. Each of the displays 3510 may include various packet data, including the receive speed (Rx), a transmit (Tx), a bits-per-second receiving and transmitting, errors receiving and transmitting, dropped packets received or transmitted, overruns, collisions and the number of packets through the system. Referring now to FIG. 36A , a screen display illustrating trended data from a near-real time database is set forth. The data illustrated in FIG. 36 corresponds to the CPU usage over time, memory usage over time, disk space used over time, the number of interrupts over time, system load averages over time, packet drops per interface over time, and chip specific accelerator packet stats over time. The MAC addresses set forth correspond to those that exist in the gateway device. A storage percent meter 3610 is illustrated showing the amount of storage used. A memory usage meter 3612 shows the amount of memory used. A CPU usage plot 3614 illustrates the amount of CPU usage (processor consumption) over a particular period of time. A memory usage plot 3616 illustrates an amount of memory used over a selected period of time. In FIG. 36B a current data area 3620 illustrates bandwidth consumption by showing the bytes received, the transmitted bytes, the received bit rate, the transmit bit rate, the received packets, the transmitted packets, the transmitted and received errors, the received dropped packets, the transmitted dropped packets and the MAC address of the monitor devices. Of course, various components may be monitored, such as those described above in paragraphs FIGS. 35 and 35 . Those skilled in the art can now appreciate from the foregoing description that the broad teachings of the disclosure can be implemented in a variety of forms. Therefore, while this disclosure includes particular examples, the true scope of the disclosure should not be so limited since other modifications will become apparent to the skilled practitioner upon a study of the drawings, the specification and the following claims.\", _c2=None),\n",
       " Row(_c0='9872054', _c1=\"In the following detailed description, reference is made to the accompanying drawings which form a part hereof wherein like numerals designate like parts throughout, and in which is shown by way of illustration embodiments that may be practiced. It is to be understood that other embodiments may be utilized and structural or logical changes may be made without departing from the scope of the present disclosure. Therefore, the following detailed description is not to be taken in a limiting sense, and the scope of embodiments is defined by the appended claims and their equivalents. Various operations may be described as multiple discrete actions or operations in turn, in a manner that is most helpful in understanding the claimed subject matter. However, the order of description should not be construed as to imply that these operations are necessarily order dependent. In particular, these operations may not be performed in the order of presentation. Operations described may be performed in a different order than the described embodiment. Various additional operations may be performed and/or described operations may be omitted in additional embodiments. For the purposes of the present disclosure, the phrase “A and/or B” means (A), (B), or (A and B). For the purposes of the present disclosure, the phrase “A, B, and/or C” means (A), (B), (C), (A and B), (A and C), (B and C), or (A, B and C). The description may use the phrases “in an embodiment,” or “in embodiments,” which may each refer to one or more of the same or different embodiments. Furthermore, the terms “comprising,” “including,” “having,” and the like, as used with respect to embodiments of the present disclosure, are synonymous. As used herein, the term “logic” and “module” may refer to, be part of, or include an Application Specific Integrated Circuit (ASIC), an electronic circuit, a processor (shared, dedicated, or group) and/or memory (shared, dedicated, or group) that execute one or more software or firmware programs, a combinational logic circuit, and/or other suitable components that provide the described functionality. Referring now to FIG. 1 , an arrangement for content distribution and consumption, in accordance with various embodiments, is illustrated. As shown, in embodiments, arrangement 100 for distribution and consumption of content may include a number of content consumption devices 108 coupled with one or more content aggregator/distributor servers 104 via one or more networks 106 . Content aggregator/distributor servers 104 may be configured to aggregate and distribute content to content consumption devices 108 for consumption, via one or more networks 106 . In embodiments, as shown, content aggregator/distributor servers 104 may include encoder 112 , storage 114 and content provisioning 116 (referred to as “streaming engine” in FIG. 1 ), which may be coupled to each other as shown. Encoder 112 may be configured to encode content 102 from various content providers, and storage 114 may be configured to store encoded content. Content provisioning 116 may be configured to selectively retrieve and provide encoded content to the various content consumption devices 108 in response to requests from the various content consumption devices 108 . Content 102 may be media content of various types, having video, audio, and/or closed captions, from a variety of content creators and/or providers. Examples of content may include, but are not limited to, movies, TV programming, user created content (such as YouTube video, iReporter video), music albums/titles/pieces, and so forth. Examples of content creators and/or providers may include, but are not limited to, movie studios/distributors, television programmers, television broadcasters, satellite programming broadcasters, cable operators, online users, and so forth. In various embodiments, for efficiency of operation, encoder 112 may be configured to encode the various content 102 , typically in different encoding formats, into a subset of one or more common encoding formats. However, encoder 112 may be configured to nonetheless maintain indices or cross-references to the corresponding content in their original encoding formats. Similarly, for flexibility of operation, encoder 112 may encode or otherwise process each or selected ones of content 102 into multiple versions of different quality levels. The different versions may provide different resolutions, different bitrates, and/or different frame rates for transmission and/or playing. In various embodiments, the encoder 112 may publish, or otherwise make available, information on the available different resolutions, different bitrates, and/or different frame rates. For example, the encoder 112 may publish bitrates at which it may provide video or audio content to the content consumption device(s) 108 . Encoding of audio data may be performed in accordance with, e.g., but are not limited to, the MP3 standard, promulgated by the Moving Picture Experts Group (MPEG). Encoding of video data may be performed in accordance with, e.g., but are not limited to, the H264 standard, promulgated by the International Telecommunication Unit (ITU) Video Coding Experts Group (VCEG). Encoder 112 may include one or more computing devices configured to perform content portioning, encoding, and/or transcoding, such as described herein. Storage 114 may be temporal and/or persistent storage of any type, including, but are not limited to, volatile and non-volatile memory, optical, magnetic and/or solid state mass storage, and so forth. Volatile memory may include, but are not limited to, static and/or dynamic random access memory. Non-volatile memory may include, but are not limited to, electrically erasable programmable read-only memory, phase change memory, resistive memory, and so forth. In various embodiments, content provisioning 116 may be configured to provide encoded content as discrete files and/or as continuous streams of encoded content. Content provisioning 116 may be configured to transmit the encoded audio/video data (and closed captions, if provided) in accordance with any one of a number of streaming and/or transmission protocols. The streaming protocols may include, but are not limited to, the Real-Time Streaming Protocol (RTSP). Transmission protocols may include, but are not limited to, the transmission control protocol (TCP), user datagram protocol (UDP), and so forth. Networks 106 may be any combinations of private and/or public, wired and/or wireless, local and/or wide area networks. Private networks may include, e.g., but are not limited to, enterprise networks. Public networks, may include, e.g., but is not limited to the Internet. Wired networks, may include, e.g., but are not limited to, Ethernet networks. Wireless networks, may include, e.g., but are not limited to, Wi-Fi, or 3G/4G networks. It would be appreciated that at the content distribution end, networks 106 may include one or more local area networks with gateways and firewalls, through which content aggregator/distributor server 104 communicate with content consumption devices 108 . Similarly, at the content consumption end, networks 106 may include base stations and/or access points, through which content consumption devices 108 communicate with content aggregator/distributor server 104 . In between the two ends may be any number of network routers, switches and other networking equipment of the like. However, for ease of understanding, these gateways, firewalls, routers, switches, base stations, access points and the like are not shown. In various embodiments, as shown, a content consumption device 108 may include player 122 , display 124 and user input device 126 . Player 122 may be configured to receive streamed content, decode and recover the content from the content stream, and present the recovered content on display 124 , in response to user selections/inputs from user input device 126 . In embodiments, player 122 may include decoder 132 , presentation engine 134 and user interface engine 136 . Decoder 132 may be configured to receive streamed content, decode and recover the content from the content stream. Presentation engine 134 may be configured to present the recovered content on display 124 , in response to user selections/inputs. In embodiments, decoder 132 and/or presentation engine 134 may be configured to present audio and/or video content to a user that has been encoded using varying encoding control variable settings in a substantially seamless manner. Thus, in various embodiments, the decoder 132 and/or presentation engine 134 may be configured to present two portions of content that vary in resolution, frame rate, and/or compression settings without interrupting presentation of the content. User interface engine 136 may be configured to receive the user selections/inputs from a user, and to selectively render a menu interface as described herein. While shown as part of a content consumption device 108 , display 124 and/or user input device(s) 126 may be stand-alone devices or integrated, for different embodiments of content consumption devices 108 . For example, and as depicted in FIGS. 2-7 , for a television arrangement, display 124 may be a stand-alone television set, Liquid Crystal Display (LCD), Plasma and the like, while player 122 may be part of a separate set-top set, and user input device 126 may be a separate remote control, gaming controller, keyboard, or another similar device. Similarly, for a desktop computer arrangement, player 122 , display 124 and user input device(s) 126 may all be separate stand-alone units. On the other hand, for a mobile arrangement, such as a tablet computing device, display 124 may be a touch sensitive display screen that includes user input device(s) 126 , and player 122 may be a computing platform with a soft keyboard that also includes one of the user input device(s) 126 . Further, display 124 and player 122 may be integrated within a single form factor. Similarly, for a smartphone arrangement, player 122 , display 124 and user input device(s) 126 may be likewise integrated. Referring now to FIG. 2 , a player 122 in the form of a set-top box, or “console,” (configured with applicable portions of the present disclosure) may be operably coupled to a display 124 , shown here in the form of a flat panel television. In FIG. 2 , presentation engine 134 and/or user interface engine 136 of player 122 may render underlying media content (not shown) on display 124 . In various embodiments, the media content may be provided to player 122 by content aggregator/distributor server 104 . In various embodiments, the media content may come from one or more media content sources, such as the one or more providers of content 102 in FIG. 1 . Referring to FIG. 3 , an example video content 340 may include a plurality of frames 342 . In various embodiments, the plurality of frames 342 may be presented, e.g., by presentation engine 134 of player 122 , in the order depicted by the “time” arrow shown in FIG. 1 , although this is not required. For instance, in a “rewind” mode, plurality of frames 342 may be presented in reverse (i.e., right to left), at various speeds. In various embodiments, player 122 may receive, e.g., from user input device 126 , a first command to suspend continued presentation of video content 340 . For example, a user may issue a “pause” command to a remote control and/or a touch screen of player 122 . Rather than simply freezing the continued presentation of video content 340 on the frame(s) that was/were being presented at the time the first command was received, in various embodiments, presentation engine 134 of player 122 may be configured to repeatedly present a multi-frame segment 344 (also referred to as a “vignette” or “clip”) of video content 340 . In various embodiments, presentation engine 134 may continue this repeated presentation until receipt of a second command, e.g., from user input device 126 , to resume presentation of video content 340 . For example, a user may issue a second pause command, or a “play” command, e.g., using a remote control and/or touch screen. Rather than resuming presentation from a single frozen frame—which may be confusing if the user has disengaged from video content mentally, e.g., by being away for a prolonged period—the user may instead be presented with multi-frame segment 344 of video content 340 . This may provide the user with at least some context of a narrative of video content 340 at the time of issuance of the command to suspend presentation. Presentation engine 134 may implement the repeated playback of multi-frame segment 344 in various ways. In some embodiments, presentation engine 134 may be configured to preserve a pause point 346 in video content 340 for which presentation coincides temporally with receipt of the command to suspend presentation. In various embodiments, this point may be preserved as a frame, a pointer to a frame, etc. For example, in FIG. 3 , presentation engine 134 may store multi-frame segment 344 in a frame buffer 348 . In various embodiments, frame buffer 348 may be a portion of various types of memory having a predetermined size. In various embodiments, frame buffer 348 may be a first in/first out (“FIFO”) buffer, and may be continuously-shifted along with presentation of video content 340 . In various embodiments, preserved point 346 may correspond to a last frame, or “pause” frame 350 , within frame buffer 348 at the time the command to suspend presentation is received by player 122 . Repeated playback of multi-frame segment 344 may include starting at a “loopback,” or “LB” frame 352 at one end of frame buffer 348 and playing through frames until reaching pause frame 350 , and then repeating (until a command to resume presentation is received). In various embodiments, including the one depicted in FIG. 3 , multi-frame segment 344 of video content 340 may be a multi-frame segment of video content 340 that precedes the preserved pause point 346 , though this is not required. In various embodiments, multi-frame segment 344 may have a predetermined length and/or number of frames. In various embodiments, the length or number of frames may be adjustable by a user, e.g., by altering the size of frame buffer 348 . In some embodiments, a default length of multi-frame segment 344 may be 5 seconds, 10 seconds, 15 seconds, 20 seconds, and so forth. Techniques other than a frame buffer may be employed. For example, in some embodiments, presentation engine 134 may be configured to maintain a current pointer and a time-shifted pointer that is shifted by a predetermined number of frames or seconds relative to the current pointer. In various embodiments, presentation engine 134 may begin the repeated presentation of multi-frame segment 344 at the time-shifted pointer. During normal presentation of video content 340 , the time shifted pointer may be shifted one frame forward for each frame of video content 340 presented. In some embodiments, count-back frame references may also be used. In some embodiments, if, during the repeated playback of multi-frame segment 344 , sound is on (e.g., if a mute button has not been pressed, or a user setting toggled), the user may be presumed to not be in the immediate vicinity of player 122 . For instance, the user may have left the room to make a snack or use the restroom. Otherwise, if the user were in the immediate vicinity, the repeated sound could be distracting and potentially annoying, especially if the user is attempting to do something else, such as taking a phone call. In such a situation, when the user issues a command to resume presentation, the user may be more likely to be reengaging with video content 340 after some mental and/or physical absence (e.g., bathroom break, grabbing a snack). Simply resuming presentation of video content 340 at the exact point at which it was suspended, as is done with existing media playing devices, may cause the user to be disoriented or confused with regard to a narrative of video content 340 upon resumption of presentation. Allowing multi-frame segment 344 to play out from a point in time at which the resume command is received by presentation engine 134 may avoid or reduce such confusion or disorientation. The user may become reacquainted with the context/narrative of the portion of video content 340 that was being presented when the user issued the command to suspend presentation (e.g., “pause”). Conversely, if, during the repeated playback, sound is off (e.g., a mute button has been pressed, or a user setting toggled), the user may be presumed to be in the same vicinity as player 122 , or at least within audible range of speakers controlled by player 122 . The sound may be off because the user is trying to do something else, like take a phone call or explain a plot point to another viewer, in which case leaving the sound on could be distracting and/or annoying. In such case, the user's reengagement with video content 340 may not be as difficult as if the user had left the room for a period of time. In such case, a better user experience may be achieved by resuming presentation of video content 340 from the preserved pause point 346 (e.g., pause frame 350 ) being presented when the user issued the command to suspend presentation, as the user may be ready to pick up immediately where she left off. In some embodiments, presentation engine 134 may be configured by default to mute sound when the command to suspend presentation is received. Accordingly, in various embodiments, presentation engine 134 may be configured to, upon receipt of the command to resume presentation, selectively resume presentation of the video content from various points of multi-frame segment 344 of video content 340 based on whether the repeated playback of multi-frame segment 344 is accompanied by sound. For example, in various embodiments, presentation engine 134 may be configured to resume presentation from preserved point 346 , e.g., in response to a determination that repeated presentation of multi-frame segment 344 is not accompanied by sound. On the other hand, if the repeated presentation of multi-frame segment 344 is accompanied by sound, then presentation engine 134 may resume presentation of video content 340 from another point in multi-frame segment 344 . For instance, on receipt of the second command, presentation engine 134 may simply allow multi-frame segment 344 to run out, in spite of the fact that the user may have already consumed all or a portion of multi-frame segment 344 . As noted above, this may allow a user to reacquaint themselves with a narrative of video content 340 , e.g., by completing consumption of the repeating vignette. Commands described herein may be issued by users to player 122 from user input devices 126 of various types. In embodiments where content consumption device 108 is a set-top box or media player, such as in FIG. 2 , commands may be issued by users using a remote control device (not shown). In embodiments where content consumption device 108 and/or player 122 is a mobile computing device such as a smartphone or tablet computer, the aforementioned commands may be issued by a user through one or more gestures made on or near a touch screen, or by using buttons or knobs and/or other actuators. FIG. 4 depicts an example process 400 that may be implemented by various computing devices, such as content consumption device 108 , in accordance with various embodiments. At operation 402 , video content 340 may be presented, e.g., by presentation engine 134 on display 124 . At operation 404 , if a command to suspend presentation of video content 340 has not been received by presentation engine 134 , then method 400 may proceed back to 402 , and presentation may continue. However, if at operation 404 , a command to suspend presentation has been received by presentation engine 134 , the process 400 may proceed to operation 406 . At operation 406 , preserved pause point 346 (e.g., a pointer or pause frame 350 ) may be preserved, e.g., by presentation engine 134 . At operation 408 , multi-frame segment 344 of video content 340 may be presented, e.g., by presentation engine 134 on display 124 . After presentation of multi-frame segment 344 , at operation 410 , if a command to resume presentation of video content 340 (e.g., a “play” or “resume”) command has not been received, e.g., by presentation engine 134 , then process 400 may proceed back to operation 408 , and multi-frame segment 344 may be presented again. However, if at operation 410 , a command to resume presentation of video content 340 has been received, then method may proceed to operation 412 . At operation 412 , a determination may be made of whether presentation of the multi-frame segment 344 at block 408 was accompanied by sound. If the answer is yes, then at operation 414 , presentation of video content 340 may be resumed, e.g., by presentation engine 134 , from preserved pause point 346 at operation 414 , and method may proceed back to operation 402 . However, if the answer is no, then presentation of video content 340 may be resumed from another point in multi-frame segment 344 at operation 416 , and process 400 may proceed back to operation 402 . For example, presentation engine 134 may simply allow presentation of multi-frame segment 344 to continue from the point of multi-frame segment 344 that was being presented when the resume command was received, and then to transition smoothly into resumed presentation of video content 340 after preserved point 346 . Referring now to FIG. 5 , an example computer suitable for use for various components of FIG. 1 , such as content consumption device 108 , is illustrated in accordance with various embodiments. As shown, computer 500 may include one or more processors or processor cores 502 , and system memory 504 . For the purpose of this application, including the claims, the terms “processor” and “processor cores” may be considered synonymous, unless the context clearly requires otherwise. Additionally, computer 500 may include mass storage devices 506 (such as diskette, hard drive, compact disc read only memory (CD-ROM) and so forth), input/output devices 508 (such as display, keyboard, cursor control, remote control, gaming controller, image capture device, and so forth) and communication interfaces 510 (such as network interface cards, modems, infrared receivers, radio receivers (e.g., Bluetooth), and so forth). The elements may be coupled to each other via system bus 512 , which may represent one or more buses. In the case of multiple buses, they may be bridged by one or more bus bridges (not shown). Each of these elements may perform its conventional functions known in the art. In particular, system memory 504 and mass storage devices 506 may be employed to store a working copy and a permanent copy of the programming instructions implementing the operations associated with content consumption device 108 , e.g., operations shown in FIG. 4 . The various elements may be implemented by assembler instructions supported by processor(s) 502 or high-level languages, such as, for example, C, that can be compiled into such instructions. The permanent copy of the programming instructions may be placed into permanent mass storage devices 506 in the factory, or in the field, through, for example, a distribution medium (not shown), such as a compact disc (CD), or through communication interface 510 (from a distribution server (not shown)). That is, one or more distribution media having an implementation of the agent program may be employed to distribute the agent and program various computing devices. The number, capability and/or capacity of these elements 510 - 512 may vary, depending on whether computer 500 is used as a content aggregator/distributor server 104 or a content consumption device 108 (e.g., a player 122 ), as well as whether computer 500 is a stationary computing device, such as a set-top box or desktop computer, or a mobile computing device such as a tablet computing device, laptop computer or smartphone. Their constitutions are otherwise known, and accordingly will not be further described. FIG. 6 illustrates an example non-transitory computer-readable storage medium 602 having instructions configured to practice all or selected ones of the operations associated with content aggregator/distributor servers 104 or content consumption devices 108 , earlier described, in accordance with various embodiments. As illustrated, non-transitory computer-readable storage medium 602 may include a number of programming instructions 604 . Programming instructions 604 may be configured to enable a device, e.g., computer 500 , in response to execution of the programming instructions, to perform, e.g., various operations of process 400 of FIG. 4 . In alternate embodiments, programming instructions 604 may be disposed on multiple non-transitory computer-readable storage media 602 instead. Referring back to FIG. 5 , for one embodiment, at least one of processors 502 may be packaged together with computational logic 522 configured to practice aspects of process 400 of FIG. 4 . For one embodiment, at least one of processors 502 may be packaged together with computational logic 522 configured to practice aspects of process 400 of FIG. 4 to form a System in Package (SiP). For one embodiment, at least one of processors 502 may be integrated on the same die with computational logic 522 configured to practice aspects of process 400 of FIG. 4 . For one embodiment, at least one of processors 502 may be packaged together with computational logic 522 configured to practice aspects of process 400 of FIG. 4 to form a System on Chip (SoC). For at least one embodiment, the SoC may be utilized in, e.g., but not limited to, a mobile computing device such as a computing tablet or smartphone. The following paragraphs describe examples of various embodiments. Example 1 may include at least one computer-readable medium including instructions that, in response to execution of the instructions by a media player computing device, cause the media player computing device to, on receipt of a first command to suspend continued presentation of video content, repeatedly present a multi-frame segment of the video content until receipt of a second command to resume presentation of the video content. Example 2 may include one computer-readable medium of example 1, wherein the instructions, in response to execution by the media player computing device, further cause the media player computing device to preserve a pause point in the video content for which presentation coincides temporally with receipt of the first command. Example 3 may include he at least one computer-readable medium of example 2, wherein the instructions, in response to execution by the media player computing device, further cause the media player computing device to selectively resume presentation of the video content from the preserved pause point or another point in the repeatedly presented multi-frame segment of the video content. Example 4 may include he at least one computer-readable medium of example 3, wherein the selectively resume presentation includes resume presentation from the another point in response to a determination that the repeated presentation of the multi-frame segment of the video content is accompanied by sound. Example 5 may include the at least one computer-readable medium of example 3, wherein the selectively resume presentation includes resume presentation from the preserved pause point in response to a determination that the repeated presentation of the multi-frame segment of the video content is not accompanied by sound. Example 6 may include the at least one computer-readable medium of any one of examples 1-5, wherein the first and/or second command is received wirelessly from a remote control. Example 7 may include he at least one computer-readable medium of example 2, wherein the multi-frame segment of the video content includes a multi-frame segment of the video content that precedes the preserved pause point. Example 8 may include the at least one computer-readable medium of example 7, wherein the multi-frame segment has a predetermined length or number of frames that is adjustable by a user. Example 9 may include the at least one computer-readable medium of any one of examples 1-5, wherein the repeated presentation of the multi-frame segment is selectively accompanied by sound based on a user command and/or a user preference. Example 10 may include the at least one computer-readable medium of any one of examples 1-5, wherein the multi-frame segment is stored in a frame buffer that is continuously-shifted along with presentation of the video content. Example 11 may include the at least one computer-readable medium of example 10, wherein the frame buffer is a first in/first out (“FIFO”) buffer. Example 12 may include the at least one computer-readable medium of any one of examples 1-5, wherein the instructions, in response to execution by the media player computing device, further cause the media player computing device to maintain a current pointer and a time-shifted pointer that is shifted by a predetermined number of frames relative to the current pointer, and wherein the repeated presentation of the multi-frame segment begins at the time-shifted pointer. Example may include a computer-implemented method. The method may include receiving, by a media player computing device, a first command to suspend continued presentation of video content. The method may further include, in response to receiving the first command, repeatedly presenting, by the media-player computing device, a multi-frame segment of the video content. The method may further include receiving, by the media player computing device, a second command to resume presentation of the video content. The method may further include, in response to receiving the second command, ceasing, by the media player computing device, the repeated presenting of the multi-frame segment and resuming presentation of the video content. Example 14 may include the computer-implemented method of example 13, further including preserving, by the media player computing device, a pause point in the video content for which presentation coincides temporally with receipt of the first command. Example 15 may include the computer-implemented method of example 14, further including selectively resuming presentation of the video content from the preserved pause point or another point in the repeatedly presented multi-frame segment of the video content. Example 16 may include the computer-implemented method of example 15, wherein the selectively resuming includes resuming, by the media player computing device, presentation from the another point in response to a determination that the repeated presentation of the multi-frame segment of the video content is accompanied by sound. Example 17 may include the computer-implemented method of example 15, wherein the selectively resuming includes resuming, by the media player computing device, presentation from the preserved pause point in response to a determination that the repeated presentation of the multi-frame segment of the video content is not accompanied by sound. Example 18 may include the computer-implemented method of any one of examples 13-17, wherein receiving the first and/or second command includes receiving the first and/or second command wirelessly from a remote control. Example 19 may include the computer-implemented method of example 14, wherein the multi-frame segment of the video content includes a multi-frame segment of the video content that precedes the preserved pause point. Example 20 may include the computer-implemented method of example 19, wherein the multi-frame segment has a predetermined length or number of frames that is adjustable by a user. Example 21 may include the computer-implemented method of any one of examples 13-17, wherein the repeated presentation of the multi-frame segment is selectively accompanied by sound based on a user command and/or a user preference. Example 22 may include the computer-implemented method of any one of examples 13-17, wherein the multi-frame segment is stored in a frame buffer that is continuously-shifted along with presentation of the video content. Example 23 may include the computer-implemented method of example 22, wherein the frame buffer is a first in/first out (“FIFO”) buffer. Example 24 may include the computer-implemented method of any one of examples 13-17, further including maintaining, by the media player computing device, a current pointer and a time-shifted pointer that is shifted by a predetermined number of frames relative to the current pointer, and wherein the repeatedly presenting includes repeatedly presenting the multi-frame segment beginning at the time-shifted pointer. Example 25 may include an apparatus. The apparatus may include one or more processors. The apparatus may also include a presentation engine coupled with the one or more processors and configured to, on receipt of a first command to suspend continued presentation of video content, repeatedly present a multi-frame segment of the video content until receipt of a second command to resume presentation of the video content. Example 26 may include the apparatus of example 25, wherein the presentation engine is further to preserve a pause point in the video content for which presentation coincides temporally with receipt of the first command. Example 27 may include the apparatus of example 26, wherein the presentation is further to selectively resume presentation of the video content from the preserved pause point or another point in the repeatedly presented multi-frame segment of the video content. Example 28 may include the apparatus of example 27, wherein the presentation engine is further to resume presentation from the another point in response to a determination that the repeated presentation of the multi-frame segment of the video content is accompanied by sound. Example 29 may include the apparatus of example 27, wherein the presentation engine is further to resume presentation from the preserved pause point in response to a determination that the repeated presentation of the multi-frame segment of the video content is not accompanied by sound. Example 30 may include the apparatus of any one of examples 25-29, wherein the first and/or second command is received wirelessly from a remote control. Example 31 may include the apparatus of example 26, wherein the multi-frame segment of the video content includes a multi-frame segment of the video content that precedes the preserved pause point. Example 32 may include the apparatus of example 31, wherein the multi-frame segment has a predetermined length or number of frames that is adjustable by a user. Example 33 may include the apparatus of any one of examples 25-29, wherein the repeated presentation of the multi-frame segment is selectively accompanied by sound based on a user command and/or a user preference. Example 34 may include the apparatus of any one of examples 25-29, wherein the multi-frame segment is stored in a frame buffer that is continuously-shifted along with presentation of the video content. Example 35 may include the apparatus of example 34, wherein the frame buffer is a first in/first out (“FIFO”) buffer. Example 36 may include the apparatus of any one of examples 25-29, wherein the presentation engine is further to maintain a current pointer and a time-shifted pointer that is shifted by a predetermined number of frames relative to the current pointer, and to begin the repeated presentation of the multi-frame segment at the time-shifted pointer. Example 37 may include an apparatus. The apparatus may include means for receiving a first command to suspend continued presentation of video content. The apparatus may also include means for repeatedly presenting a multi-frame segment of the video content in response to the means for receiving the first command. The apparatus may also include means for receiving a second command to resume presentation of the video content. The apparatus may also include means for ceasing the repeated presenting of the multi-frame segment and resuming presentation of the video content in response to the means for receiving the second command. Example 38 may include the apparatus of example 37, further including means for preserving a pause point in the video content for which presentation coincides temporally with receipt of the first command. Example 39 may include he apparatus of example 38, further including means for selectively resuming presentation of the video content from the preserved pause point or another point in the repeatedly presented multi-frame segment of the video content. Example 40 may include the apparatus of example 39, wherein the means for selectively resuming include means for resuming presentation from the another point in response to a determination that the repeated presentation of the multi-frame segment of the video content is accompanied by sound. Example 41 may include the apparatus of example 39, wherein the means for selectively resuming include means for resuming presentation from the preserved pause point in response to a determination that the repeated presentation of the multi-frame segment of the video content is not accompanied by sound. Example 42 may include the apparatus of any one of examples 37-41, wherein the means for receiving the first and/or second command includes means for receiving the first and/or second command wirelessly from a remote control. Example 43 may include the apparatus of example 38, wherein the multi-frame segment of the video content includes a multi-frame segment of the video content that precedes the preserved pause point. Example 44 may include the apparatus of example 43, wherein the multi-frame segment has a predetermined length or number of frames that is adjustable by a user. Example 45 may include the apparatus of any one of examples 37-41, wherein the repeated presentation of the multi-frame segment is selectively accompanied by sound based on a user command and/or a user preference. Example 46 may include the apparatus of any one of examples 37-41, wherein the multi-frame segment is stored in a frame buffer that is continuously-shifted along with presentation of the video content. Example 47 may include the apparatus of example 46, wherein the frame buffer is a first in/first out (“FIFO”) buffer. Example 48 may include the apparatus of any one of examples 37-41, further including means for maintaining a current pointer and a time-shifted pointer that is shifted by a predetermined number of frames relative to the current pointer, wherein the means for repeatedly presenting include means for repeatedly presenting the multi-frame segment beginning at the time-shifted pointer. Computer-readable media (including non-transitory computer-readable media), methods, apparatuses, systems and devices for performing the above-described techniques are illustrative examples of embodiments disclosed herein. Additionally, other devices in the above-described interactions may be configured to perform various disclosed techniques. Although certain embodiments have been illustrated and described herein for purposes of description, a wide variety of alternate and/or equivalent embodiments or implementations calculated to achieve the same purposes may be substituted for the embodiments shown and described without departing from the scope of the present disclosure. This application is intended to cover any adaptations or variations of the embodiments discussed herein. Therefore, it is manifestly intended that embodiments described herein be limited only by the claims. Where the disclosure recites “a” or “a first” element or the equivalent thereof, such disclosure includes one or more such elements, neither requiring nor excluding two or more such elements. Further, ordinal indicators (e.g., first, second or third) for identified elements are used to distinguish between the elements, and do not indicate or imply a required or limited number of such elements, nor do they indicate a particular position or order of such elements unless otherwise specifically stated.\", _c2=None),\n",
       " Row(_c0='9872053', _c1='MODE FOR CARRYING OUT THE INVENTION Hereinafter, modes of the present technology (hereinafter, referred to as embodiments) will be described. The description will be presented in the following order. 1. Current System of Multiplexing Audio Stream of High Definition-Serial Digital Interface (HD-SDI) 2. Embodiment of Present Technology 3. Modified Example 1. CURRENT SYSTEM OF MULTIPLEXING AUDIO STREAM OF HD-SDI A current system of multiplexing an audio stream into a data stream (hereinafter, simply, also referred to as an HD-SDI) defined in the HD-SDI format of 1.485 Gbps is defined in SMPTE 299-1. In addition, the format of an audio stream multiplexed into the HD-SDI is defined by audio engineering society (AES) 3. FIG. 1 illustrates the data structure of an audio data packet defined in SMPTE 299-1. One audio data packet includes: an auxiliary data flag ADF; a data ID DID; a data block number DBN; a data count DC; an audio clock phase data CLK; audio data of a total of four channels CH 1 to CH 4 ; an error correction code configured by ECC 0 to ECC 5 ; and a check sum CS. The range from the ADF to the CH 4 is protected by the error correction code. In addition, the length of one audio data packet is 31 bytes. Here, one byte is 10 bits, and, hereinafter, one byte will be referred to also as one word. In each of the channels CH 1 to CH 4 , one sample of audio data (hereinafter, referred to as an audio sample) is included. In a case where the sampling rate of an audio stream is 32 kHz, 44.1 kHz, or 48 kHz, audio data of four channels can be multiplexed into one audio data packet. In other words, audio samples of respective two channels (Channels 1 and 2 ) of an AES sub frame 1 and an AES sub frame 2 , which are continuous, can be multiplexed into one audio data packet. On the other hand, in a case where the sampling rate of the audio stream is 96 kHz, audio data of two channels can be multiplexed into one audio data packet. In other words, a first audio sample and a second audio sample of Channel 1 of an AES sub frame 1 and an AES sub frame 2 , which are continuous, can be multiplexed into one audio data packet. FIG. 2 illustrates usable areas of the audio data packet defined in SMPTE 299-1. In addition, FIG. 2 illustrates usable areas of an audio data packet in video data of 60I of 2,200 samples×1,125 lines that is defined in SMPTE 292-1. The audio data packet can be multiplexed into a horizontal auxiliary data area acquired by excluding areas of start of active video SAV, end of active video EAV, a line number LN, and cyclic redundancy check CRCC from a horizontal blanking area of a data stream of a Cb/Cr series (hereinafter, referred to as a C-series data stream). However, the audio data packet cannot be multiplexed into a horizontal auxiliary data area of the next horizontal line of a horizontal line including a switching point (hereinafter, simply, referred to also as a line). However, in a case where the sampling rate of the audio stream is 48 kHz, the average number of times of sampling the audio per one line of video data is about 1.42 (=48 kHz/30 Hz/1,125 lines) times. Thus, the number of times of sampling per one line is two times at the ratio of once or less for two lines, and it is necessary to transmit an audio data packet at a maximum of two times for one line. Meanwhile, since the horizontal auxiliary data area is 268 bytes, two packet streams each including four audio data packets of 31 bytes can be multiplexed into one line. Thus, in a case where the sampling rate of the audio stream is 48 kHz, since audio data of four channels is included in one audio data packet, audio data of a maximum of 16 channels can be transmitted. On the other hand, in a case where the sampling rate of the audio stream is 96 kHz, since audio data of two channels is included in one audio data packet, audio data of a maximum of eight channels can be transmitted. 2. EMBODIMENT OF PRESENT TECHNOLOGY Next, embodiments of the present technology will be described with reference to FIGS. 3 to 9 . [Example of Configuration of Signal Transmission System 1 ] FIG. 3 is a block diagram that illustrates a signal transmission system 1 according to an embodiment to which the present technology is applied. The signal transmission system 1 is configured to include: n broadcast cameras 11 - 1 to 11 - n ; and a camera control unit (CCU) 12 . The broadcast cameras 11 - 1 to 11 - n are connected to the CCU 12 respectively through optical fiber cables 13 - 1 to 13 - n. Hereinafter, in a case where the broadcast cameras 11 - 1 to 11 - n do not need to be individually discriminated from each other, each thereof will be simply referred to as a broadcast camera 11 . In addition, hereinafter, in a case where the optical fiber cables 13 - 1 to 13 - n do not need to be individually discriminated from each other, each thereof will be simply referred to as an optical fiber cable 13 . The broadcast camera 11 is used as a signal transmission apparatus that transmits a signal (data stream) by using an SDI used for transmitting video data of which the frame rate is 100 Hz or 120 Hz (hereinafter, referred to as a 100 Hz-SDI or a 120 Hz-SDI). In addition, the CCU 12 is used as a signal reception apparatus that receives a signal (data stream) by using the 100 Hz-SDI or the 120 Hz-SDI. The signal transmission system 1 acquired by combining the broadcast cameras 11 and the CCU 12 is used as a signal transmission system that transmits a signal (data stream) by using the 100 Hz-SDI or the 120 Hz-SDI. Each broadcast camera 11 transmits video data (2K video data) of 2,200 samples×1,125 lines or 2,640 samples×1,125 lines, which is acquired by a photographing process, to the CCU 12 through the optical fiber cable 13 . This video data is configured as video data of a progressive system of which the frame rate is 100 Hz or 120 Hz (video data of 100P or 120P) or video data of which the field rate is 200 Hz or 240 Hz (video data of 200I or 240I). The CCU 12 controls each broadcast camera 11 , receives video data from each broadcast camera 11 , and transmits video data (return data) used for displaying a video on a monitor of each broadcast camera 11 which is in the middle of the photographing process using the other broadcast cameras 11 . [Example of Circuit Configuration of Broadcast Camera 11 ] FIG. 4 is a block diagram that illustrates an example of the configuration of a part of the broadcast camera 11 that relates to an embodiment of the present technology. The broadcast camera 11 a is configured to include: an imaging device 101 ; an audio input unit 102 ; a signal processing unit 103 ; and a transmission control unit 104 . In addition, the signal processing unit 103 is configured to include: a video signal processing/adding unit 111 ; a mapping unit 112 ; an audio multiplexing unit 113 ; a multiplexing unit 114 ; a scrambler 115 ; and a parallel/serial (P/S) conversion unit 116 . The imaging device 101 , for example, is configured by a CMOS image sensor, a CCD image sensor, or the like. The imaging device 101 supplies video data of 100P, 120P, 200I, or 240I acquired as a result of a photographing process to the video signal processing/adding unit 111 of the signal processing unit 103 . The format of the video data generated by the imaging device 101 is not particularly limited. For example, the format of a multi-color space such as an RGB space or a YCbCr space can be employed. In addition, for example, in the case of the YCbCr space, an arbitrary system among 4:4:4, 4:2:2, 4:1:1, 4:2:0, and the like can be employed. Hereinafter, a case will be described as an example in which the 4:2:2 system of YCbCr is employed as the format of the color space of the video data. The audio input unit 102 , for example, is configured by a microphone, an audio processing device, and the like. The audio input unit 102 collects audios of a plurality of channels in parallel with a photographing process executed by the imaging device 101 . In addition, the audio input unit 102 generates an audio stream including digital audio data of the plurality of channels by sampling the collected audios of the plurality of channels at a predetermined sampling rate and supplies the generated audio stream to the mapping unit 112 of the signal processing unit 103 . The sampling rate, for example, is set to be in the range of 32 kHz to 96 kHz (for example, 32 kHz, 44.1 kHz, 48 kHz, 96 kHz, or the like). The video signal processing/adding unit 111 executes video processing such as a flaw correction, a gamma correction, a color space conversion, and the like for the video data. In addition, the video signal processing/adding unit 111 generates a plurality of data streams acquired by multiplexing video data. In other words, the video signal processing/adding unit 111 multiplexes video data into a plurality of data streams of a predetermined format. In addition, the video signal processing/adding unit 111 adds predetermined information to the plurality of data streams. Then, the video signal processing/adding unit 111 supplies the plurality of data streams that have been generated to the audio multiplexing unit 113 . The audio multiplexing unit 113 multiplexes an audio stream into at least one of the plurality of data streams supplied from the video signal processing/adding unit 111 . The audio multiplexing unit 113 supplies a plurality of data streams including the data stream into which the audio stream is multiplexed to the multiplexing unit 114 . The multiplexing unit 114 generates one data stream by multiplexing a plurality of data streams by using a predetermined method. The multiplexing unit 114 supplies the generated data stream to the scrambler 115 . The scrambler 115 executes a scrambling process of a predetermined system for the data stream and supplies the data stream after the scrambling process to the parallel/serial conversion unit 116 . The parallel/serial conversion unit 116 executes a parallel/serial conversion of a data stream and supplies the data stream after the conversion to the transmission control unit 104 . The transmission control unit 104 controls the transmission of the data stream to the CCU 12 . [Example of Circuit Configuration of CCU 12 ] FIG. 5 is a block diagram that illustrates an example of the configuration of a part of the CCU 12 that relates to an embodiment of the present technology. The CCU 12 is configured to include: a reception control unit 151 ; a signal processing unit 152 ; a video processing unit 153 ; and an audio processing unit 154 . In addition, the signal processing unit 152 is configured to include: a serial parallel (S/P) conversion unit 161 ; a descrambler 162 ; a separation unit 163 ; and a reproduction unit 164 . The reception control unit 151 controls the reception of a data stream supplied from each broadcast camera 11 . The reception control unit 151 supplies the received data stream to the serial/parallel conversion unit 161 of the signal processing unit 152 . The serial/parallel conversion unit 161 executes a serial/parallel conversion of the data stream and supplies the data stream after the conversion to the descrambler 162 . The descrambler 162 executes a descrambling process, which is an inverse process of the process executed by the scrambler 115 of the broadcast camera 11 , for a data stream and supplies the data stream after the descrambling process to the separation unit 163 . The separation unit 163 separates a data stream into a plurality of data streams by executing an inverse process of the process executed by the multiplexing unit 114 of the broadcast camera 11 and supplies the plurality of data streams to the reproduction unit 164 . The reproduction unit 164 extracts data of each pixel that is multiplexed into a plurality of data streams, restores original video data including the extracted pixel data, and supplies the restored original video data to the video processing unit 153 . In addition, the reproduction unit 164 extracts an audio sample multiplexed into the plurality of data streams, restores an original audio stream including the extracted audio sample, and supplies the restored original audio stream to the audio processing unit 154 . The video processing unit 153 is configured by a device that executes various processes for video data and executes a predetermined process for video data supplied from the reproduction unit 164 . For example, the video processing unit 153 is configured by a display displaying a video that is based on video data, a storage device storing video data, and the like. The audio processing unit 154 is configured by a device that executes various processes for audio data and executes a predetermined process for audio data supplied from the reproduction unit 164 . For example, the audio processing unit 154 is configured by a speaker outputting an audio that is based on the audio data, a storage device storing the audio data, and the like. In addition, depending on a content to be processed, the video processing unit 153 and the audio processing unit 154 may be configured by using one device. [Signal Transmission Process] Next, the signal transmission process executed by the broadcast camera 11 will be described with reference to a flowchart represented in FIG. 6 . This process illustrates a process of a case where video data corresponding to one frame is transmitted, and, in a case where video data of a plurality of frames is transmitted, the process is repeatedly executed. In Step S 1 , the video signal processing/adding unit 111 executes video signal processing and addition of SAV and the like. More specifically, the video signal processing/adding unit 111 executes video processing such as a flaw correction, a gamma correction, and a color space conversion for video data supplied from the imaging device 101 . In addition, the video signal processing/adding unit 111 , for example, divides Y-series data and C-series (CbCr) data of video data into two data streams having a configuration similar to that defined in SMPTE 274 and multiplexes the data streams. Furthermore, the video signal processing/adding unit 111 adds the same SAV, EAV, LN, and CRCC as those represented in the specification of SMPTE 274 to the two data streams that have been generated. The video signal processing/adding unit 111 supplies the two data streams that have been generated to the audio multiplexing unit 113 . In Step S 2 , the mapping unit 112 executes mapping of an audio stream. More specifically, the mapping unit 112 maps the audio stream supplied from the audio input unit 102 into a format defined in AES3. The mapping unit 112 supplies the audio stream after mapping to the audio multiplexing unit 113 . In Step S 3 , the audio multiplexing unit 113 multiplexes the audio stream into a data stream. More specifically, the audio multiplexing unit 113 multiplexes the audio stream into one data stream (for example, the C-series data stream) of the two data streams. For example, in a case where the sampling rate of the audio stream is 48 kHz, and the frame rate of the video data is 120 Hz (120P or 240I), the average number of times of sampling of an audio per one line of video data is about 0.36 times (=48 kHz/120 Hz/1,125 lines). On the other hand, in a case where the sampling rate of the audio stream is 48 kHz, and the frame rate of the video data is 100 Hz (100P or 200I), the average number of times of sampling of an audio per one line of video data is about 0.43 times (=48 kHz/100 Hz/1,125 lines). Accordingly, in a case where the frame rate of the video data is 120 Hz or 100 Hz, the audio is sampled at the interval of once or less for two lines of the video data. More specifically, the audio data is sampled at a shortest interval of once for two lines and at a longest interval of once for three lines, whereby an audio sample is generated. In addition, for a next line of a line for which an audio sample is generated through the sampling of the audio data (hereinafter, referred to as a sampling line), sampling is not executed, and accordingly, any audio sample is not generated. Thus, the audio multiplexing unit 113 , basically, in a horizontal auxiliary data area of lines of two rows that are continuous from the next line of the sampling line, as illustrated in FIG. 7 , multiplexes a maximum of two packet streams each including a maximum of four units of audio packet data having a configuration similar to that defined in SMPTE 299-1 each time. In one audio data packet, since audio samples corresponding to four channels are included, audio samples corresponding to 16 channels per one packet stream is included. In addition, every time when sampling of audio data is executed once, a maximum of 16 audio data packets of 8×two rows are multiplexed into a data stream, and accordingly, audio data of a maximum of 64 channels can be multiplexed into a data stream. In addition, in a case where the sampling rate of the audio data is 96 kHz, audio data of a maximum of 32 channels, which correspond to a half of that of the case of 48 kHz, can be multiplexed into a data stream. Furthermore, similar to the specification of SMPTE 299-1, the multiplexing of an audio data packet into the horizontal auxiliary data area of the next line of the switching point is prohibited. For this reason, as illustrated in FIGS. 8 and 9 , due to the influence of a switching line, there are cases where the multiplexing position of the audio data packet is shifted to the rear side. More specifically, FIG. 8 illustrates a case where audio data A to C is sampled, and a switching point is included in the sampling line of the audio data B. In addition, a multiplexing position flag mpf represented in the lowermost level in the figure is a flag that illustrates the relation between a multiplexing position of an audio data packet and a sampling line of corresponding audio data and is set to a predetermined position of CLK of the audio data packet. In the audio data A, a sampling line and the next line do not include a switching point. Accordingly, packet streams A 1 to A 4 corresponding to the audio data A are not influenced by the switching point but are multiplexed into a horizontal auxiliary data area of lines of two rows continuous from the next line of the sampling line of the audio data A. In addition, the value of the multiplexing position flag of an audio data packet included in each of the packet streams A 1 to A 4 is set to “0” representing that the audio data packet is not influenced by the switching point but is multiplexed into a line of a normal position. Also in the audio data C, similarly, a sampling line and the next line do not include a switching point. Accordingly, packet streams C 1 to C 4 corresponding to the audio data C are not influenced by the switching point but are multiplexed into a horizontal auxiliary data area of lines of two rows continuous from the next line of the sampling line of the audio data C. In addition, the value of the multiplexing position flag of an audio data packet included in each of the packet streams C 1 to C 4 is set to “0”. On the other hand, in the audio data B, a sampling line includes a switching point, and an audio data packet cannot be multiplexed into a horizontal auxiliary data area of the next line. Accordingly, packet streams B 1 to B 4 corresponding to the audio data B are multiplexed into a horizontal auxiliary data area of lines of two rows continuous from a line positioned two rows after the sampling line by skipping the horizontal auxiliary data area of the next line of the sampling line of the audio data B. In addition, the value of the multiplexing position flag of an audio data packet included in each of the packet streams B 1 to B 4 is set to “1” representing that the audio data packet is multiplexed into a line shifted to a further rear side than a normal case due to the influence of the switching point. FIG. 9 illustrates a case where audio data A to C is sampled, and a switching point is included in the next line of a sampling line of the audio data A. In the audio data A, the next line of a sampling line includes a switching point, and audio data packets cannot be multiplexed into a horizontal auxiliary data area of lines positioned two rows after the sampling line. Accordingly, while packet streams A 1 and A 2 corresponding to the audio data A are multiplexed into the horizontal auxiliary data area of the next line of the sampling line of the audio data A, packet streams A 3 and A 4 are multiplexed into a horizontal auxiliary data area of lines positioned three rows after the sampling line by skipping one row. In addition, the value of the multiplexing position flag of an audio data packet included in each of the packet streams A 1 and A 2 is set to “0”. On the other hand, the value of the multiplexing position flag of an audio data packet included in each of the packet streams A 3 and A 4 is set to “1” representing that the audio data packet is multiplexed into a line shifted to a further rear side than a normal case due to the influence of the switching point. Furthermore, the packet streams B 1 to B 4 corresponding to the audio data B are multiplexed into a horizontal auxiliary data area of lines shifted to a further rear side than the normal case by one line due to the shift of the multiplexing positions of the packet streams A 3 and A 4 to the rear side by one line. Accordingly, the value of the multiplexing position flag of an audio data packet included in each of the packet streams B 1 to B 4 is set to “1” representing that the audio data packet is multiplexed into a line shifted to a further rear side than the normal case due to the influence of the switching point. In addition, the multiplexing positions of audio data packets corresponding to the audio data C and the values of the multiplexing position flags are as illustrated in the example of FIG. 8 . Furthermore, the audio multiplexing unit 113 multiplexes an audio control packet similar to that defined in SMPTE 299-1 in a horizontal auxiliary data area of a data stream (for example, a data stream of a Y series) different from the data stream into which the audio stream is multiplexed. In addition, the audio multiplexing unit 113 uses a value acquired by measuring a position of an audio sample from the first word of the EAV by a formatter by using a clock of 297 MHz, 297/1.110 MHz, or the like as an audio clock phase. Then, the audio multiplexing unit 113 supplies a data stream acquired by multiplexing an audio stream and an audio control packet to the multiplexing unit 114 . In Step S 4 , the multiplexing unit 114 multiplexes the data stream. In other words, the multiplexing unit 114 multiplexes two data streams into one data stream in units of words. The multiplexing unit 114 supplies the multiplexed data stream to the scrambler 115 . In Step S 5 , the scrambler 115 scrambles the data stream by using a method similar to that defined in SMPTE 292-1 and supplies a resultant data stream to the parallel/serial conversion unit 116 . In Step S 6 , the parallel/serial conversion unit 116 executes a parallel/serial conversion of the data stream and supplies the converted data stream to the transmission control unit 104 . In Step S 7 , the transmission control unit 104 transmits the data stream to the CCU 12 . Thereafter, the signal transmission process ends. [Signal Reception Process] Next, a signal reception process executed by the CCU 12 in correspondence with the signal transmission process executed by the broadcast camera 11 illustrated in FIG. 6 will be described with reference to a flowchart represented in FIG. 10 . This process represents a process of a case where video data corresponding to one frame is received, and this process is repeatedly executed in a case where video data of a plurality of frames is received. In Step S 51 , the reception control unit 151 receives a data stream transmitted from the broadcast camera 11 . The reception control unit 151 supplies the received data stream to the serial/parallel conversion unit 161 . In Step S 52 , the serial/parallel conversion unit 161 executes a serial/parallel conversion of the data stream through an inverse process of the process executed by the parallel/serial conversion unit 116 of the broadcast camera 11 . The serial/parallel conversion unit 161 supplies the data stream after the conversion to the descrambler 162 . In Step S 53 , the descrambler 162 descrambles the data stream through an inverse process of the process executed by the scrambler 115 of the broadcast camera 11 . The descrambler 162 supplies the data stream after the descrambling to the separation unit 163 . In Step S 54 , the separation unit 163 separates data streams through an inverse process of the process executed by the multiplexing unit 114 of the broadcast camera 11 . The separation unit 163 supplies the two data streams after the separation to the reproduction unit 164 . In Step S 55 , the reproduction unit 164 reproduces the data stream. In other words, the reproduction unit 164 extracts data of each pixel that is multiplexed into the two data streams and restores original video data including the extracted pixel data. In addition, the reproduction unit 164 extracts audio samples multiplexed into the two data streams and restores an original audio stream including the extracted audio samples. Then, the reproduction unit 164 supplies the restored video data to the video processing unit 153 and supplies the restored audio data to the audio processing unit 154 . The video processing unit 153 executes predetermined processing such as display of a video that is based on the video data, recording of the video data, and the like for the video data. The audio processing unit 154 executes predetermined processing such as outputting of an audio that is based on the audio data, recording of the audio data, and the like for the audio data. Thereafter, the signal reception process ends. In this way, an SDI capable of efficiently transmitting audio data together with video data of which the frame rate is 100 Hz or 120 Hz can be realized. In other words, an SDI that is compliant with SMPTE 299-1, which is a current audio signal multiplexing system of the HD-SDI, and is capable of transmitting audio data of channels of which the number is a maximum of four times that defined in SMPTE 299-1 together with video data can be realized. 3. MODIFIED EXAMPLE Hereinafter, modified examples of the embodiment according to the present technology described above will be described. The range to which the present technology is applied is not limited to the frame rates of the video data and the sampling rates of the audio described above but can be applied to a case where the audio is sampled at the interval of once or less for lines of n1 (here, n1≧2) rows of the video data. In such a case, the generated audio sample may be multiplexed into a horizontal auxiliary data area of lines of n2 (here, 2≦n2≦n1) rows after the sampling line in which the audio sample is generated. In this way, compared to a case where an audio sample is multiplexed into a horizontal auxiliary data area of one line, audio data of more number of channels can be multiplexed into a data stream and be transmitted. More specifically, for example, it may be considered to set the frame rate of the video data to 96 Hz. In addition, the multiplexing position of the audio data packet does not necessarily start from the next line of a sampling line of a corresponding audio sample but, for example, may start from a line after two or more rows. Furthermore, audio samples generated by executing sampling once may be intermittently multiplexed into a plurality of lines instead of being multiplexed into a plurality of continuous lines. In addition, the format of the video data is not particularly limited but may be a format that can be multiplexed into a data stream of an SDI. Similarly, as the format of the audio data, any format other than the AES3 may be employed, as long as it can be multiplexed into a data stream of the SDI. Furthermore, the frame size of the video data is not limited to that described in the example described above but may be set to a difference value. [Example of Configuration of Computer] A series of the processes described above can be performed either by hardware or by software. In a case where the series of the processes is performed by software, a program configuring the software is installed to a computer. Here, the computer includes a computer that is built in dedicated hardware, a computer such as a general-purpose personal computer that can execute various functions by installing various programs thereto, and the like. FIG. 11 is a block diagram that illustrates an example of the hardware configuration of a computer that executes the series of processes described above by using a program. In the computer, a CPU (Central Processing Unit) 301 , a ROM (Read Only Memory) 302 , and a RAM (Random Access Memory) 303 are interconnected through a bus 304 . In addition, an input/output interface 305 is connected to the bus 304 . An input unit 306 , an output unit 307 , a storage unit 308 , a communication unit 309 , and a drive 310 are connected to the input/output interface 305 . The input unit 306 is configured by a keyboard, a mouse, a microphone, and the like. The output unit 307 is configured by a display, a speaker, and the like. The storage unit 308 is configured by a hard disk, a non-volatile memory, and the like. The communication unit 309 is configured by a network interface and the like. The drive 310 drives a magnetic disk, an optical disc, a magneto-optical disk, or a removable medium 311 such as a semiconductor memory. In the computer configured as above, the CPU 301 , for example, loads a program stored in the storage unit 308 into the RAM 303 through the input/output interface 305 and the bus 304 and executes the loaded program, thereby executing the series of the processes described above. The program executed by the computer (the CPU 301 ), for example, may be provided with being recorded on a removable medium 311 as a package medium or the like. In addition, the program may be provided through a wired or wireless transmission medium such as a local area network, the Internet, or digital satellite broadcast. In the computer, by loading the removable medium 311 into the drive 310 , the program can be installed to the storage unit 308 through the input/output interface 305 . In addition, the program may be received by the communication unit 309 through a wired or wireless transmission medium and be installed to the storage unit 308 . Furthermore, the program may be installed to the ROM 302 or the storage unit 308 in advance. In addition, the program executed by the computer may be a program that executes the processes in a time series along the sequence described in this specification or a program that executes the processes in a parallel manner or at necessary timing such as at the timing of being called. In this specification, a system represents a set of a plurality of constituent elements (an apparatus, a module (component), and the like), and all the constituent elements do not need to be disposed in a same casing. Thus, a plurality of apparatuses that are housed in separate casings and are connected through a network and one apparatus in which a plurality of modules are housed in one casing are systems. In addition, the present technology is not limited to the embodiments described above, and various changes can be made therein in a range not departing from the concept of the present technology. For example, each step described in each flowchart described above may be either executed by one apparatus or executed by a plurality of apparatuses in a shared manner. Furthermore, in a case where a plurality of processes are included in one step, the plurality of processes included in the one step may be either executed by one apparatus or executed by a plurality of apparatuses in a shared manner. In addition, for example, the present technology may take configurations as below. (1) A signal processing apparatus including: an audio multiplexing unit that multiplexes an audio sample into a data area of horizontal lines of n2 (here, 2≦n2≦n1) rows after a first horizontal line in which the audio sample is generated, which is a predetermined data area arranged within a horizontal blanking area disposed for each horizontal line of video data in a data stream defined in a format of an SDI (serial digital interface) used for transmitting the video data, in a case where an audio is sampled at an interval of once or less for horizontal lines of n1 (here, n1≧2) rows of the video data of a predetermined frame rate. (2) The signal processing apparatus according to (1), wherein the audio multiplexing unit multiplexes the audio sample into the data area of horizontal lines of n2 rows continuous from a second horizontal line that is a next line of the first horizontal line. (3) The signal processing apparatus according to (2), wherein the frame rate of the video data is in a range of 96 Hz to 120 Hz, a sampling rate of the audio is in a range of 32 kHz to 96 kHz, and the audio multiplexing unit multiplexes the audio sample into the data area of horizontal lines of two rows continuous from the second horizontal line. (4) The signal processing apparatus according to (3), wherein the audio multiplexing unit multiplexes a maximum of eight audio data packets including a maximum of four audio samples into one data area. (5) The signal processing apparatus according to any of (2) to (4), wherein the audio multiplexing unit multiplexes the audio sample with the data area of the next horizontal line of a horizontal line including a switching point being skipped. (6) The signal processing apparatus according to (5), wherein the audio multiplexing unit sets a flag that represents whether or not the horizontal line into which the audio sample is multiplexed is shifted to a rear side due to an influence of the switching point in the data area of the horizontal line. (7) The signal processing apparatus according to any of (1) to (6), further including a video signal processing unit that generates the data stream into which the video data is multiplexed. (8) A signal processing method including: multiplexing an audio sample into a data area of horizontal lines of n2 (here, 2≦n2≦n1) rows after a first horizontal line in which the audio sample is generated, which is a predetermined data area arranged within a horizontal blanking area disposed for each horizontal line of video data in a data stream defined in a format of an SDI (serial digital interface) used for transmitting the video data, in a case where an audio is sampled at an interval of once or less for horizontal lines of n1 (here, n1≧2) rows of the video data of a predetermined frame rate by using a signal processing apparatus. (9) A program causing a computer to execute: multiplexing an audio sample into a data area of horizontal lines of n2 (here, 2≦n2≦n1) rows after a first horizontal line in which the audio sample is generated, which is a predetermined data area arranged within a horizontal blanking area disposed for each horizontal line of video data in a data stream defined in a format of an SDI (serial digital interface) used for transmitting the video data, in a case where an audio is sampled at an interval of once or less for horizontal lines of n1 (here, n1≧2) rows of the video data of a predetermined frame rate. (10) A signal processing apparatus including: a reproduction unit that restores an audio stream including an audio sample by extracting the audio sample from a data stream in which the audio sample is multiplexed into a data area of horizontal lines of n2 (here, 2≦n2≦n1) rows after a first horizontal line in which the audio sample is generated, which is a predetermined data area arranged within a horizontal blanking area disposed for each horizontal line of video data in the data stream defined in a format of an SDI (serial digital interface) used for transmitting the video data by sampling an audio at an interval of once or less for horizontal lines of n1 (here, n1≧2) rows of the video data of a predetermined frame rate. (11) A signal processing method including: restoring, by a signal processing apparatus, an audio stream including an audio sample by extracting the audio sample from a data stream in which the audio sample is multiplexed into a data area of horizontal lines of n2 (here, 2≦n2≦n1) rows after a first horizontal line in which the audio sample is generated, which is a predetermined data area arranged within a horizontal blanking area disposed for each horizontal line of video data in the data stream defined in a format of an SDI (serial digital interface) used for transmitting the video data by sampling an audio at an interval of once or less for horizontal lines of n1 (here, n1≧2) rows of the video data of a predetermined frame rate. (12) A program causing a computer to execute: restoring an audio stream including an audio sample by extracting the audio sample from a data stream in which the audio sample is multiplexed into a data area of horizontal lines of n2 (here, 2≦n2≦n1) rows after a first horizontal line in which the audio sample is generated, which is a predetermined data area arranged within a horizontal blanking area disposed for each horizontal line of video data in the data stream defined in a format of an SDI (serial digital interface) used for transmitting the video data by sampling an audio at an interval of once or less for horizontal lines of n1 (here, n1≧2) rows of the video data of a predetermined frame rate. (13) A signal transmission system including: a signal transmission apparatus including: an audio multiplexing unit that multiplexes an audio sample into a data area of horizontal lines of n2 (here, 2≦n2≦n1) rows after a first horizontal line in which the audio sample is generated, which is a predetermined data area arranged within a horizontal blanking area disposed for each horizontal line of video data in a data stream defined in a format of an SDI (serial digital interface) used for transmitting the video data, in a case where an audio is sampled at an interval of once or less for horizontal lines of n1 (here, n1≧2) rows of the video data of a predetermined frame rate; and a transmission control unit that controls transmission of the data stream into which the audio sample is multiplexed; and a signal reception apparatus including: a reception control unit that controls reception of the data stream; and a reproduction unit that extracts the audio sample from the data stream and restores an audio stream including the audio sample. REFERENCE SIGNS LIST 1 Signal transmission system 11 - 1 to 11 - n Broadcast camera 12 CCU 101 Imaging device 102 Audio input unit 103 Signal processing unit 104 Transmission control unit 111 Video signal processing/adding unit 112 Mapping unit 113 Audio multiplexing unit 114 Multiplexing unit 115 Scrambler 116 Parallel/serial conversion unit 151 Reception control unit 152 Signal processing unit 161 Serial/parallel conversion unit 162 Descrambler 163 Separation unit 164 Reproduction unit', _c2=None)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2019.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_2016' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-b3eb4b842d9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_2016\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_2016' is not defined"
     ]
    }
   ],
   "source": [
    "df_2016.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o457.save.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 24 in stage 32.0 failed 1 times, most recent failure: Lost task 24.0 in stage 32.0 (TID 1900, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:257)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.hadoop.fs.FSError: java.io.IOException: No space left on device\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:248)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:58)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.writeChunk(ChecksumFileSystem.java:424)\n\tat org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunks(FSOutputSummer.java:206)\n\tat org.apache.hadoop.fs.FSOutputSummer.write1(FSOutputSummer.java:124)\n\tat org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:110)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:58)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat java.io.FilterOutputStream.write(FilterOutputStream.java:97)\n\tat org.apache.parquet.hadoop.util.HadoopPositionOutputStream.write(HadoopPositionOutputStream.java:45)\n\tat org.apache.parquet.bytes.ConcatenatingByteArrayCollector.writeAllTo(ConcatenatingByteArrayCollector.java:46)\n\tat org.apache.parquet.hadoop.ParquetFileWriter.writeDataPages(ParquetFileWriter.java:460)\n\tat org.apache.parquet.hadoop.ColumnChunkPageWriteStore$ColumnChunkPageWriter.writeToFileWriter(ColumnChunkPageWriteStore.java:201)\n\tat org.apache.parquet.hadoop.ColumnChunkPageWriteStore.flushToFileWriter(ColumnChunkPageWriteStore.java:261)\n\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.flushRowGroupToStore(InternalParquetRecordWriter.java:173)\n\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.close(InternalParquetRecordWriter.java:114)\n\tat org.apache.parquet.hadoop.ParquetRecordWriter.close(ParquetRecordWriter.java:165)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.close(ParquetOutputWriter.scala:42)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.releaseResources(FileFormatDataWriter.scala:57)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.commit(FileFormatDataWriter.scala:74)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:247)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248)\n\t... 10 more\nCaused by: java.io.IOException: No space left on device\n\tat java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:246)\n\t... 36 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:167)\n\t... 32 more\nCaused by: org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:257)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.hadoop.fs.FSError: java.io.IOException: No space left on device\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:248)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:58)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.writeChunk(ChecksumFileSystem.java:424)\n\tat org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunks(FSOutputSummer.java:206)\n\tat org.apache.hadoop.fs.FSOutputSummer.write1(FSOutputSummer.java:124)\n\tat org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:110)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:58)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat java.io.FilterOutputStream.write(FilterOutputStream.java:97)\n\tat org.apache.parquet.hadoop.util.HadoopPositionOutputStream.write(HadoopPositionOutputStream.java:45)\n\tat org.apache.parquet.bytes.ConcatenatingByteArrayCollector.writeAllTo(ConcatenatingByteArrayCollector.java:46)\n\tat org.apache.parquet.hadoop.ParquetFileWriter.writeDataPages(ParquetFileWriter.java:460)\n\tat org.apache.parquet.hadoop.ColumnChunkPageWriteStore$ColumnChunkPageWriter.writeToFileWriter(ColumnChunkPageWriteStore.java:201)\n\tat org.apache.parquet.hadoop.ColumnChunkPageWriteStore.flushToFileWriter(ColumnChunkPageWriteStore.java:261)\n\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.flushRowGroupToStore(InternalParquetRecordWriter.java:173)\n\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.close(InternalParquetRecordWriter.java:114)\n\tat org.apache.parquet.hadoop.ParquetRecordWriter.close(ParquetRecordWriter.java:165)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.close(ParquetOutputWriter.scala:42)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.releaseResources(FileFormatDataWriter.scala:57)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.commit(FileFormatDataWriter.scala:74)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:247)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248)\n\t... 10 more\nCaused by: java.io.IOException: No space left on device\n\tat java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:246)\n\t... 36 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-a6cf4f82f8a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                \u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                .save(\"data/df_2016.parquet\"))\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o457.save.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 24 in stage 32.0 failed 1 times, most recent failure: Lost task 24.0 in stage 32.0 (TID 1900, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:257)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.hadoop.fs.FSError: java.io.IOException: No space left on device\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:248)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:58)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.writeChunk(ChecksumFileSystem.java:424)\n\tat org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunks(FSOutputSummer.java:206)\n\tat org.apache.hadoop.fs.FSOutputSummer.write1(FSOutputSummer.java:124)\n\tat org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:110)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:58)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat java.io.FilterOutputStream.write(FilterOutputStream.java:97)\n\tat org.apache.parquet.hadoop.util.HadoopPositionOutputStream.write(HadoopPositionOutputStream.java:45)\n\tat org.apache.parquet.bytes.ConcatenatingByteArrayCollector.writeAllTo(ConcatenatingByteArrayCollector.java:46)\n\tat org.apache.parquet.hadoop.ParquetFileWriter.writeDataPages(ParquetFileWriter.java:460)\n\tat org.apache.parquet.hadoop.ColumnChunkPageWriteStore$ColumnChunkPageWriter.writeToFileWriter(ColumnChunkPageWriteStore.java:201)\n\tat org.apache.parquet.hadoop.ColumnChunkPageWriteStore.flushToFileWriter(ColumnChunkPageWriteStore.java:261)\n\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.flushRowGroupToStore(InternalParquetRecordWriter.java:173)\n\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.close(InternalParquetRecordWriter.java:114)\n\tat org.apache.parquet.hadoop.ParquetRecordWriter.close(ParquetRecordWriter.java:165)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.close(ParquetOutputWriter.scala:42)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.releaseResources(FileFormatDataWriter.scala:57)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.commit(FileFormatDataWriter.scala:74)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:247)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248)\n\t... 10 more\nCaused by: java.io.IOException: No space left on device\n\tat java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:246)\n\t... 36 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:167)\n\t... 32 more\nCaused by: org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:257)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.hadoop.fs.FSError: java.io.IOException: No space left on device\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:248)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:58)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.writeChunk(ChecksumFileSystem.java:424)\n\tat org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunks(FSOutputSummer.java:206)\n\tat org.apache.hadoop.fs.FSOutputSummer.write1(FSOutputSummer.java:124)\n\tat org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:110)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:58)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat java.io.FilterOutputStream.write(FilterOutputStream.java:97)\n\tat org.apache.parquet.hadoop.util.HadoopPositionOutputStream.write(HadoopPositionOutputStream.java:45)\n\tat org.apache.parquet.bytes.ConcatenatingByteArrayCollector.writeAllTo(ConcatenatingByteArrayCollector.java:46)\n\tat org.apache.parquet.hadoop.ParquetFileWriter.writeDataPages(ParquetFileWriter.java:460)\n\tat org.apache.parquet.hadoop.ColumnChunkPageWriteStore$ColumnChunkPageWriter.writeToFileWriter(ColumnChunkPageWriteStore.java:201)\n\tat org.apache.parquet.hadoop.ColumnChunkPageWriteStore.flushToFileWriter(ColumnChunkPageWriteStore.java:261)\n\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.flushRowGroupToStore(InternalParquetRecordWriter.java:173)\n\tat org.apache.parquet.hadoop.InternalParquetRecordWriter.close(InternalParquetRecordWriter.java:114)\n\tat org.apache.parquet.hadoop.ParquetRecordWriter.close(ParquetRecordWriter.java:165)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.close(ParquetOutputWriter.scala:42)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.releaseResources(FileFormatDataWriter.scala:57)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.commit(FileFormatDataWriter.scala:74)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:247)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248)\n\t... 10 more\nCaused by: java.io.IOException: No space left on device\n\tat java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:246)\n\t... 36 more\n"
     ]
    }
   ],
   "source": [
    "df_2016 = (spark.read.format(\"csv\")\n",
    "               .option(\"delimiter\", \"\\t\")\n",
    "               .option(\"header\", \"true\")\n",
    "               .option('inferSchema', \"true\")\n",
    "               .load(\"data/detail-desc-text-2016.tsv\")\n",
    "               .write\n",
    "               .format(\"parquet\")\n",
    "               .save(\"data/df_2016.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2016.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark.read.format(\"csv\")\n",
    "           .option(\"delimiter\", \",\")\n",
    "           .infer\n",
    "           .load(\"data/df.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'path file:/home/glmack_agoralytix_com/techniche/df_2017.parquet already exists.;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o594.save.\n: org.apache.spark.sql.AnalysisException: path file:/home/glmack_agoralytix_com/techniche/df_2017.parquet already exists.;\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:114)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-9fbc5f244656>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                \u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                .save(\"df_2017.parquet\"))\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'path file:/home/glmack_agoralytix_com/techniche/df_2017.parquet already exists.;'"
     ]
    }
   ],
   "source": [
    "df_2017 = (spark.read.format(\"csv\")\n",
    "               .option(\"delimiter\", \"\\t\")\n",
    "               .option(\"header\", \"true\")\n",
    "               .option('inferSchema', \"true\")\n",
    "               .load(\"data/detail-desc-text-2017.tsv\")\n",
    "               .write\n",
    "               .format(\"parquet\")\n",
    "               .save(\"df_2017.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2018 = spark.read.parquet(\"data/df_2018.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2018.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2018.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2018.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2018.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subset dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset dataframe - comment/uncomment to include fields\n",
    "df = raw_df[['patent_number', \n",
    "         'patent_date', \n",
    "         'patent_title',\n",
    "         'patent_abstract', \n",
    "         'patent_firstnamed_assignee_id',\n",
    "         'patent_firstnamed_assignee_location_id',\n",
    "         'patent_firstnamed_assignee_latitude',\n",
    "         'patent_firstnamed_assignee_longitude',\n",
    "         'patent_firstnamed_assignee_city',\n",
    "         'patent_firstnamed_assignee_state',\n",
    "         'patent_firstnamed_assignee_country', \n",
    "         'patent_firstnamed_inventor_id',\n",
    "         'patent_firstnamed_inventor_location_id',\n",
    "         'patent_firstnamed_inventor_latitude',\n",
    "         'patent_firstnamed_inventor_longitude',\n",
    "         'patent_firstnamed_inventor_city',\n",
    "         'patent_firstnamed_inventor_state',\n",
    "         'patent_firstnamed_inventor_country',\n",
    "         'patent_year', \n",
    "         'patent_type', \n",
    "         'patent_kind',\n",
    "         'inventors'\n",
    "#          'patent_processing_time', \n",
    "#          'patent_num_us_application_citations', \n",
    "#          'patent_num_us_patent_citations', \n",
    "#          'patent_num_foreign_citations', \n",
    "#          'patent_num_combined_citations', \n",
    "#          'patent_num_claims', \n",
    "#          'patent_num_cited_by_us_patents',\n",
    "#          'detail_desc_length'\n",
    "            ]]\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 561 different assignees\n",
    "len(df.patent_firstnamed_assignee_id.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new column that combines the patent title and the patent abstract columns into a single string\n",
    "df['patent_title_abstract'] = df.patent_title + ' ' + df.patent_abstract\n",
    "df.patent_title_abstract.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TODO (Lee) Partition data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.patent_number.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=['patent_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = df.patent_title_abstract.tolist()\n",
    "text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partition data\n",
    "len(text_data)\n",
    "text_train = text_data[:round(len(text_data)*.8)]\n",
    "text_test = text_data[round(len(text_data)*.8):]\n",
    "print(len(text_data), len(text_train), len(text_test), len(text_train)+len(text_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to download stop words from nltk and language package from spacy\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# !python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct pipeline using Spacy Language object and associated pipeline/components\n",
    "nlp = spacy.load(\"en\")\n",
    "pprint(nlp.pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = []   \n",
    "\n",
    "# process patent documents in pipeline\n",
    "for doc in nlp.pipe(text_train, n_threads=4, batch_size=100):\n",
    "   \n",
    "    ents = doc.ents  # Named entities.\n",
    "\n",
    "    # Keep only words (no numbers, no punctuation).\n",
    "    # Lemmatize tokens, remove punctuation and remove stopwords.\n",
    "    doc = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
    "\n",
    "    # Remove common words from a stopword list.\n",
    "    doc = [token for token in doc if token not in stop_words]\n",
    "\n",
    "    # Add named entities, but only if they are a compound of more than word.\n",
    "    doc.extend([str(entity) for entity in ents if len(entity) > 1])\n",
    "    \n",
    "    processed_docs.append(doc)\n",
    "\n",
    "processed_docs[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[token.text for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = set([w.label_ for w in doc.ents]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in labels: \n",
    "    entities = [cleanup(e.string, lower=False) for e in document.ents if label==e.label_] \n",
    "    entities = list(set(entities)) \n",
    "    print(label,entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processed_docs = []\n",
    "for doc in nlp.pipe(docs, n_threads=4, batch_size=100):\n",
    "    # Process document using Spacy NLP pipeline.\n",
    "    \n",
    "    ents = doc.ents  # Named entities.\n",
    "\n",
    "    # Keep only words (no numbers, no punctuation).\n",
    "    # Lemmatize tokens, remove punctuation and remove stopwords.\n",
    "    doc = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
    "\n",
    "    # Remove common words from a stopword list.\n",
    "    #doc = [token for token in doc if token not in STOPWORDS]\n",
    "\n",
    "    # Add named entities, but only if they are a compound of more than word.\n",
    "    doc.extend([str(entity) for entity in ents if len(entity) > 1])\n",
    "    \n",
    "    pre_processed_docs.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize documents\n",
    "\n",
    "def tokenize_docs(docs):\n",
    "    tokenized_docs = []\n",
    "    for doc in docs:\n",
    "        tokenized_docs.append(word_tokenize(doc))\n",
    "    return tokenized_docs\n",
    "\n",
    "tokenized_docs = tokenize_docs(text_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean punctuation\n",
    "def clean_docs(tokenized_docs):\n",
    "    clean_docs = []\n",
    "    for doc in tokenized_docs:\n",
    "       clean_docs.append([word for word in doc if word.isalpha()])  \n",
    "    return clean_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = clean_docs(tokenized_docs)\n",
    "cleaned_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to lowercase\n",
    "def lower_words(docs):\n",
    "    lowered_words = []\n",
    "    for doc in docs:\n",
    "        lowered_words.append([word.lower() for word in doc])\n",
    "    return lowered_words\n",
    "\n",
    "lowered_data = lower_words(cleaned_data)\n",
    "lowered_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_stopwords(docs):\n",
    "    filtered_docs = []\n",
    "    for doc in docs:\n",
    "       filtered_docs.append([word for word in doc if word not in stop_words])\n",
    "    return filtered_docs\n",
    "\n",
    "# remove stopwords\n",
    "filtered_data = filter_stopwords(lowered_data)\n",
    "filtered_data\n",
    "# TODO (Lee) - resolve un-lowered stopwords \"A\" and \"An\", 'By', 'The'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct bigrams and trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train bigram phrases model\n",
    "bigram_model = Phrases(filtered_data, min_count=1, threshold=1)\n",
    "\n",
    "# train trigram phrases model\n",
    "trigram_model = Phrases(bigram_model[filtered_data], threshold=100)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigrams\n",
    "def bigrams(docs):\n",
    "    \"\"\"create bigrams\"\"\"\n",
    "    return [bigram_model[doc] for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize bigram and trigram models\n",
    "bigram_model = gensim.models.phrases.Phraser(bigram_model)\n",
    "trigram_model = gensim.models.phrases.Phraser(trigram_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams(filtered_data)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigrams(docs):\n",
    "    \"\"\"create trigrams\"\"\"\n",
    "    return [trigram_model[bigram_model[doc]] for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams(filtered_data)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stem and Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_docs(docs, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"lemmatize documents\"\"\"\n",
    "    lemmatized_docs = []\n",
    "    for doc in docs: \n",
    "        lemmatized_docs.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return lemmatized_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Lee)\n",
    "\n",
    "lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)\n",
    "# for doc in cleaned_data:\n",
    "#     for token in doc:\n",
    "#         token.lemma_\n",
    "\n",
    "# uncomment to use\n",
    "# download english model with \"python -m spacy download en\"\n",
    "\n",
    "# for token in doc:\n",
    "#     print(token, token.lemma, token.lemma_)\n",
    "\n",
    "# TODO (Lee) - lemmatize_docs(cleaned_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create corpus and dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using spacy pipeline components\n",
    "# build dictionary\n",
    "id_to_word = corpora.Dictionary(processed_docs)\n",
    "\n",
    "# build corpus\n",
    "texts = processed_docs\n",
    "\n",
    "# apply term document frequency\n",
    "# converts documents in corpus to bag-of-words format, a list of (token_id, token_count) tuples\n",
    "corpus = [id_to_word.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # build dictionary\n",
    "id_to_word = corpora.Dictionary(filtered_data)\n",
    "\n",
    "# build corpus\n",
    "texts = filtered_data\n",
    "\n",
    "# apply term document frequency\n",
    "# converts documents in corpus to bag-of-words format, a list of (token_id, token_count) tuples\n",
    "corpus = [id_to_word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view formatted corpus (term-doc-frequency)\n",
    "[[(id_to_word[id], freq) for id, freq in text] for text in corpus][:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model - model #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Lee) - deprecation warnings\n",
    "# construct LDA model\n",
    "model_lda = LdaModel(corpus=corpus,\n",
    "                     id2word=id_to_word,\n",
    "                     num_topics=25, \n",
    "                     random_state=100,\n",
    "                     update_every=1,\n",
    "                     chunksize=100,\n",
    "                     passes=10,\n",
    "                     alpha='auto',\n",
    "                     per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print keywords in n topics\n",
    "pprint(model_lda.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print top 10 keywords that comprise topic with index of 0\n",
    "pprint(model_lda.print_topic(24))\n",
    "# the most import keywords, and the respective weight, that form topic 0 are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print top 10 keywords that comprise topic with index of 1\n",
    "pprint(model_lda.print_topic(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Lee) - infer topic from keywords?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate - model #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate perplexity metrics\n",
    "perplexity = model_lda.log_perplexity(corpus)\n",
    "perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Lee) - confirm that filtered_data is indeed the correct dataset to pass to texts param\n",
    "# calculate coherence metric\n",
    "coherence = CoherenceModel(model=model_lda, texts=processed_docs, dictionary=id_to_word, coherence='c_v')\n",
    "coherence_1 = coherence.get_coherence()\n",
    "coherence_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Lee) - confirm that filtered_data is indeed the correct dataset to pass to texts param\n",
    "# calculate coherence metric\n",
    "coherence = CoherenceModel(model=model_lda, texts=filtered_docs, dictionary=id_to_word, coherence='c_v')\n",
    "coherence_1 = coherence.get_coherence()\n",
    "coherence_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate coherence metric or each of the n topicss\n",
    "coherence_1 = coherence.get_coherence_per_topic()\n",
    "coherence_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore topics\n",
    "pyLDAvis.enable_notebook()\n",
    "viz_topics_1 = pyLDAvis.gensim.prepare(model_lda, corpus, id_to_word)\n",
    "viz_topics_1\n",
    "# TODO (Lee) - salient vs relevant terms in pyLDA ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2-  Mallet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to download Mallet topic model\n",
    "# !wget http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip\n",
    "# update this path\n",
    "path_mallet = 'data/mallet-2.0.8/bin/mallet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = gensim.models.wrappers.LdaMallet(path_mallet, corpus=corpus, num_topics=25, id2word=id_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topics\n",
    "pprint(model_2.show_topics(formatted=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate coherence metric\n",
    "coherence_model_2 = CoherenceModel(model=model_2, texts=data, dictionary=id_to_word, coherence='c_v')\n",
    "coherence_model_2 = coherence_model_2.get_coherence()\n",
    "coherence_model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Lee)\n",
    "# def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "#     \"\"\"\n",
    "#     Compute c_v coherence for various number of topics\n",
    "\n",
    "#     Parameters:\n",
    "#     ----------\n",
    "#     dictionary : Gensim dictionary\n",
    "#     corpus : Gensim corpus\n",
    "#     texts : List of input texts\n",
    "#     limit : Max num of topics\n",
    "\n",
    "#     Returns:\n",
    "#     -------\n",
    "#     model_list : List of LDA topic models\n",
    "#     coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "#     \"\"\"\n",
    "#     coherence_values = []\n",
    "#     model_list = []\n",
    "#     for num_topics in range(start, limit, step):\n",
    "#         model = gensim.models.wrappers.LdaMallet(path_mallet, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "#         model_list.append(model)\n",
    "#         coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "#         coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "#     return model_list, coherence_values\n",
    "\n",
    "# model_list, coherence_values = compute_coherence_values(dictionary=id_to_word, corpus=corpus, texts=data, start=2, limit=40, step=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3 - Author topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct inventor-to-doc mapping as df from nested inventors column in json api response\n",
    "df_inventors = json_normalize(results['patents'], record_path=['inventors'], meta=['patent_number', 'patent_date'])\n",
    "df_inventors = df_inventors[['inventor_id', 'patent_number', 'patent_date']]\n",
    "df_inventors.sort_values(by=['patent_date'])\n",
    "df_inventors.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Lee) - resolve workaround\n",
    "df_idx = df\n",
    "df_idx['idx'] = df.index\n",
    "df_idx\n",
    "df_idx_1 = df_idx[['patent_number', 'idx', 'inventors']]\n",
    "df_idx_2 = df_idx_1.set_index('patent_number')\n",
    "df_idx_2.pop('inventors')\n",
    "df_idx_2\n",
    "df_pat_idx = df_idx_2.T.to_dict('records')\n",
    "for i in df_pat_idx:\n",
    "    df_pat_idx = dict(i)\n",
    "df_pat_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pat_idx = df_idx_2.T.to_dict('records')\n",
    "for i in df_pat_idx:\n",
    "    df_pat_idx = dict(i)\n",
    "df_pat_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inv_test = json_normalize(results['patents'], record_path=['inventors'], meta=['patent_number', 'patent_date'])\n",
    "df_inv_test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idx_pat_inv_map = df[['patent_number', 'inventors']]\n",
    "df_idx_pat_inv_map.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Lee) - find out how to get list of patents_view_field names from API - I did it accidentally but need to replicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.patent_title_abstract[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inventors.set_index('inventor_id').T.to_dict('list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k, v in pat2inv.items():\n",
    "#     name_dict[new_key] = name_dict.pop(k)\n",
    "#     time.sleep(4)\n",
    "\n",
    "# pprint.pprint(name_dict)\n",
    "\n",
    "# d = {'x':1, 'y':2, 'z':3}\n",
    "# d1 = {'x':'a', 'y':'b', 'z':'c'}\n",
    "\n",
    "# dict((d1[key], value) for (key, value) in d.items())\n",
    "# {'a': 1, 'b': 2, 'c': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patdf2inv = dict((df_pat_idx[key], value) for (key, value) in pat2inv.items())\n",
    "patdf2inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat2inv = {k: list(v) for k,v in df_inventors.groupby(\"patent_number\")[\"inventor_id\"]}\n",
    "pat2inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "idx_pat_map = df.patent_number.to_dict()\n",
    "idx_pat_map = {str(key): value for key, value in idx_pat_map.items()}\n",
    "idx_pat_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct author-topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct author-topic model\n",
    "model_at = AuthorTopicModel(corpus=corpus,\n",
    "                         doc2author=patdf2inv,\n",
    "                         id2word=id_to_word, \n",
    "                         num_topics=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct vectors for authors\n",
    "author_vecs = [model_at.get_author_topics(author) for author in model_at.id2author.values()]\n",
    "author_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the topic distribution for an author using use model[name] syntax\n",
    "# each topic has a probability of being expressed given the particular author, but only the ones above a certain threshold are shown.\n",
    "\n",
    "model_at['7788103-1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def show_author(name):\n",
    "#     print('\\n%s' % name)\n",
    "#     print('Docs:', model.author2doc[name])\n",
    "#     print('Topics:')\n",
    "#     pprint([(topic_labels[topic[0]], topic[1]) for topic in model[name]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate per-word bound, which is a measure of the model's predictive performance (reconstruction error?)\n",
    "\n",
    "build doc2author dictionary\n",
    "\n",
    "doc2author = atmodel.construct_doc2author(model.corpus, model.author2doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import atmodel\n",
    "doc2author = atmodel.construct_doc2author(model.corpus, model.author2doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim.models.atmodel.construct_author2doc(doc2author)\n",
    "# construct mapping from author IDs to document IDs.\n",
    "\n",
    "Parameters:\tdoc2author (dict of (int, list of str)) – Mapping of document id to authors.\n",
    "Returns:\tMapping of authors to document ids.\n",
    "Return type:\tdict of (str, list of int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim.models.atmodel.construct_doc2author(corpus, author2doc)\n",
    "construct mapping from document IDs to author IDs\n",
    "\n",
    "Parameters:\t\n",
    "corpus (iterable of list of (int, float)) – Corpus in BoW format.\n",
    "author2doc (dict of (str, list of int)) – Mapping of authors to documents.\n",
    "Returns:\t\n",
    "Document to Author mapping.\n",
    "\n",
    "Return type:\t\n",
    "dict of (int, list of str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
