{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendation System\n",
    "Collaborative filtering with implicit feedback based on latent factors. Prepare data on user-item relationships for each user-company in format that ALS can use.\n",
    "We require each unique assignee ID in the rows of the matrix, and each unique item ID in columns of matrix.\n",
    "Values of matrix should be (?) binary user-item preference * confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import ArrayType, IntegerType\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "from lightfm import LightFM\n",
    "from lightfm.datasets import fetch_movielens\n",
    "from lightfm.evaluation import precision_at_k\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from test_model import (get_patent_fields_list, get_ml_patents, \n",
    "                        create_title_abstract_col,trim_data, \n",
    "                        structure_dataframe, partition_dataframe, \n",
    "                        build_pipeline, process_docs, pat_inv_map, get_topics)\n",
    "\n",
    "from rec_system import alphanum_to_int, int_to_alphanum\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.corpora import Dictionary, mmcorpus\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models import AuthorTopicModel\n",
    "from gensim.test.utils import common_dictionary, datapath, temporary_file\n",
    "from smart_open import smart_open\n",
    "\n",
    "import spacy\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, punkt, RegexpTokenizer, wordpunct_tokenize\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
    "\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "import calendar\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.6:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1a2022cac8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.6:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data understanding - Acquire data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data understanding - Acquire data for text workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pickled dataset\n",
    "with open('/Users/lee/Documents/techniche/techniche/data/raw_data_1000', 'rb') as f:\n",
    "    raw_data_1000 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define desired keys/columns as criteria to subset dataset\n",
    "retained_keys = ['patent_number', 'patent_firstnamed_assignee_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset raw dataset by desired keys/columns\n",
    "data_1000 = trim_data(data=raw_data_1000, keys=retained_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Pandas dataframe\n",
    "df_1000 = pd.DataFrame(data_1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data understanding - Acquire data for text workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define desired keys/columns as criteria to subset dataset for text analysis workflows\n",
    "retained_keys_2 = ['patent_number', 'patent_firstnamed_assignee_id','patent_title',\n",
    "                 'patent_abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset raw dataset by desired keys/columns for text analysis workflows\n",
    "data_1000_2 = trim_data(data=raw_data_1000, keys=retained_keys_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Lee) review naming conv duplication - create item in dict by concatenating patent_title and patent_abstract\n",
    "data_1000_2 = create_title_abstract_col(data=data_1000_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Pandas dataframe\n",
    "df_1000_2 = pd.DataFrame(data_1000_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop row that contains invalid data in patent_number column\n",
    "df_1000_2[df_1000_2.patent_number.str.contains('[RE]')]\n",
    "df_1000_2 = df_1000_2.drop(df_1000_2.index[[717]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 999 entries, 0 to 999\n",
      "Data columns (total 3 columns):\n",
      "patent_firstnamed_assignee_id    972 non-null object\n",
      "patent_number                    999 non-null object\n",
      "patent_title_abstract            999 non-null object\n",
      "dtypes: object(3)\n",
      "memory usage: 31.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# drop NaNs in patent_firstnamed_assignee_id column\n",
    "df_1000_2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1000_2 = df_1000_2.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patent_firstnamed_assignee_id</th>\n",
       "      <th>patent_number</th>\n",
       "      <th>patent_title_abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>org_VU2IXnxgxGIK8A8oQrwm</td>\n",
       "      <td>10226194</td>\n",
       "      <td>Statistical, noninvasive measurement of a pati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>org_9cmRc2rH8nbl8O9VuxYL</td>\n",
       "      <td>10228278</td>\n",
       "      <td>Determining a health condition of a structure....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>org_8O8xQifxyiW5pZB2KuDx</td>\n",
       "      <td>10228693</td>\n",
       "      <td>Generating simulated sensor data for training ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  patent_firstnamed_assignee_id patent_number  \\\n",
       "0      org_VU2IXnxgxGIK8A8oQrwm      10226194   \n",
       "1      org_9cmRc2rH8nbl8O9VuxYL      10228278   \n",
       "2      org_8O8xQifxyiW5pZB2KuDx      10228693   \n",
       "\n",
       "                               patent_title_abstract  \n",
       "0  Statistical, noninvasive measurement of a pati...  \n",
       "1  Determining a health condition of a structure....  \n",
       "2  Generating simulated sensor data for training ...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1000_2.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #1 - Data preparation\n",
    "Prepare data on user-item relationships for each user-company in format that ALS can use.\n",
    "We require each unique assignee ID in the rows of the matrix, and each unique item ID in columns of matrix.\n",
    "Values of matrix should be (?) binary user-item preference * confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new rating column and assign value of 1\n",
    "df_1000['rating'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop row that contains invalid data in patent_number column\n",
    "df_1000[df_1000.patent_number.str.contains('[RE]')]\n",
    "df_1000 = df_1000.drop(df_1000.index[[717]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop NaNs in patent_firstnamed_assignee_id column\n",
    "df_1000.info()\n",
    "df_1000 = df_1000.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert patent_number column from string to int\n",
    "df_1000 = df_1000.astype({'patent_number': 'int64'})\n",
    "# uncomment to confirm\n",
    "# df_1000.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert alphanumeric patent_firstnamed_assignee_id col to int\n",
    "df_1000 = df_1000.astype({'patent_number': 'int64'})\n",
    "# s = 'org_VU2IXnxgxGIK8A8oQrwm'\n",
    "\n",
    "# code = [ord(c) for c in s]\n",
    "# code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1000.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash('org_VU2IXnxgxGIK8A8oQrwm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_1000['patent_firstnamed_assignee_id'] = df_1000['patent_firstnamed_assignee_id'].apply(hash).apply(abs)\n",
    "df_1000['patent_firstnamed_assignee_id'] = df_1000['patent_firstnamed_assignee_id'].apply(hash).apply(abs) % 65536 # 2^16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_1000['patent_firstnamed_assignee_id'] = df_1000['patent_firstnamed_assignee_id'].apply(hash).apply(abs)\n",
    "df_1000['patent_number'] = df_1000['patent_number'] % 65536 # 2^16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1000 = df_1000.astype({'patent_firstnamed_assignee_id': 'int'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1000.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preparation - create Spark dataframe from pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_df_1000 = spark.createDataFrame(df_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_df_1000.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_df_1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast columns from bigint to int\n",
    "sp_df_1000_2 = sp_df_1000.withColumn(\"patent_firstnamed_assignee_id\", sp_df_1000[\"patent_firstnamed_assignee_id\"].cast(IntegerType())).withColumn(\"patent_number\", sp_df_1000[\"patent_number\"].cast(IntegerType())).withColumn(\"rating\", sp_df_1000[\"rating\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_df_1000_2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into \n",
    "(training, test) = sp_df_1000.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model # 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the recommendation model using ALS on the training data\n",
    "# Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\n",
    "# set implicitPrefs to True to get better results b/c latent matrix \n",
    "# rank - number of latent topics- ME-10? alpha=0.01?\n",
    "# ME suggests begin with alpha=30. try alphas for domain and see if recs make sense, r\n",
    "# build ALS model\n",
    "als = ALS(maxIter=5,\n",
    "          regParam=0.01, \n",
    "          rank=10,\n",
    "          alpha=30,\n",
    "          implicitPrefs=True,\n",
    "          userCol=\"patent_firstnamed_assignee_id\", \n",
    "          itemCol=\"patent_number\", \n",
    "          ratingCol=\"rating\",\n",
    "          coldStartStrategy=\"nan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the ALS model to the training set\n",
    "model = als.fit(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #1 - Evaluation - Compare to naive baseline\n",
    "Compare model evaluation result with naive baseline model that only outputs (for explicit - the average rating (or you may try one that outputs the average rating per movie)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #1 - Optimize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = ParamGridBuilder().addGrid(als_model.regParam, [0.01,0.001,0.1]).addGrid(als_model.rank, [4,10,50]).build()\n",
    "\n",
    "\n",
    "## instantiate crossvalidator estimator\n",
    "cv = CrossValidator(estimator=als_model, estimatorParamMaps=params,evaluator=evaluator,parallelism=4)\n",
    "best_model = cv.fit(movie_ratings)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Predictions for a New User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = predictions.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_train_df = predictions.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- content-similarity\n",
    "- limits of patent space\n",
    "- TF-IDF vectorization of patents - metrics - avg distance between \n",
    "- distance between individual patents, with ranking\n",
    "- Sherry - ascent - TF-IDF vectorization - take tf-idf vector and argsort by absolute value, so you can see which features are most\n",
    "- important to this patent. Get top 20 features. While normally would do cosine distance betweel all vectors. BUT,\n",
    "- only do cosine distance between these top 20 features, for cold start patents\n",
    "- TF-IDF vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #2 - Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 972 entries, 0 to 999\n",
      "Data columns (total 3 columns):\n",
      "patent_firstnamed_assignee_id    972 non-null object\n",
      "patent_number                    972 non-null object\n",
      "patent_title_abstract            972 non-null object\n",
      "dtypes: object(3)\n",
      "memory usage: 30.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df_1000_2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2 - Data preparation - text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate TF-IDF Vectorizer using standard English stopwords\n",
    "tfidf = TfidfVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit TF-IDF matrix on text column\n",
    "tfidf_matrix = tfidf.fit_transform(df_1000_2['patent_title_abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(972, 5364)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output matrix, 972 docs, 5364 terms\n",
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3 - compute distance metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute cosine similarity matrix between docs using linear_kernel\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct reverse map of indices and movie titles\n",
    "indices = pd.Series(df_1000_2.index, index=df_1000_2['patent_title_abstract']).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "patent_title_abstract\n",
       "Statistical, noninvasive measurement of a patient's physiological state. Tools and techniques for the rapid, continuous, invasive and/or noninvasive measurement, estimation, and/or prediction of a patient's physiological state. In an aspect, some tools and techniques can estimate predict the onset of conditions intracranial pressure, an amount of blood volume loss, cardiovascular collapse, and/or dehydration. Some tools can recommend (and, in some cases, administer) a therapeutic treatment for the patient's condition. In another aspect, some techniques employ high speed software technology that enables active, long term learning from extremely large, continually changing datasets. In some cases, this technology utilizes feature extraction, state-of-the-art machine learning and/or statistical methods to autonomously build and apply relevant models in real-time.                                                                                                                                                                                                                                                                                                                        0\n",
       "Determining a health condition of a structure. The disclosure relates to structural health monitoring (SHM). In particular determining a health condition of a structure, such as a bridge, based on vibration data measured of the bridge. Measured vibration data is calibrated (410-450). Features are then extracted from the calibrated data (610-630) and a support vector machine classifier is then applied (720) to the extracted features to determine (730) the health condition of a part of the structure. Training of the support vector machine classifier by a machine learning process (910) is also described.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                1\n",
       "Generating simulated sensor data for training and validation of detection models. A scenario is defined that including models of vehicles and a typical driving environment. A model of a subject vehicle is added to the scenario and sensor locations are defined on the subject vehicle. Perception of the scenario by sensors at the sensor locations is simulated to obtain simulated sensor outputs. The simulated sensor outputs are annotated to indicate the location of obstacles in the scenario. The annotated sensor outputs may then be used to validate a statistical model or to train a machine learning model. The simulates sensor outputs may be modeled with sufficient detail to include sensor noise or may include artificially added noise to simulate real world conditions.                                                                                                                                                                                                                                                                                                                                                                                                                          2\n",
       "Hybrid parallelization strategies for machine learning programs on top of mapreduce. Parallel execution of machine learning programs is provided. Program code is received. The program code contains at least one parallel for statement having a plurality of iterations. A parallel execution plan is determined for the program code. According to the parallel execution plan, the plurality of iterations is partitioned into a plurality of tasks. Each task comprises at least one iteration. The iterations of each task are independent. Data required by the plurality of tasks is determined. An access pattern by the plurality of tasks of the data is determined. The data is partitioned based on the access pattern.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           3\n",
       "Peripheral device support with a digital assistant for operating system upgrades. A digital assistant supported across computing devices is configured to interact with an operating system (OS) upgrade system so that various user experiences, services, content, or features associated with support for peripheral devices during an OS upgrade of a computing device can be provided by the digital assistant and rendered as a native digital assistant user experience. The digital assistant is configured to surface a notification through a user interface (UI) when an OS upgrade is available for a user's computing device and recommended for installation. The OS upgrade system executes a confidence model in a machine learning system using real world crowd-sourced data to make predictions of successful post-upgrade operations of peripheral devices with an associated level of confidence. The digital assistant personalizes the OS upgrade notification to the user based on the configuration of computing and peripheral devices, applicable context, and the confidence level.                                                                                                                 4\n",
       "Initializing a workspace for building a natural language understanding system. Designing a natural language understanding (NLU) model for an application from scratch can be difficult for non-experts. A system can simplify the design process by providing an interface allowing a designer to input example usage sentences and build an NLU model based on presented matches to those example sentences. In one embodiment, a method for initializing a workspace for building an NLU system includes parsing a sample sentence to select at least one candidate stub grammar from among multiple candidate stub grammars. The method can include presenting, to a user, respective representations of the candidate stub grammars selected by the parsing of the sample sentence. The method can include enabling the user to choose one of the respective representations of the candidate stub grammars. The method can include adding to the workspace a stub grammar corresponding to the representation of the candidate stub grammar chosen by the user.                                                                                                                                                            5\n",
       "Allowing spelling of arbitrary words. Methods, systems, and apparatus, including computer programs encoded on computer storage media, for natural language processing. One of the methods includes receiving a first voice input from a user device; generating a first recognition output; receiving a user selection of one or more terms in the first recognition output; receiving a second voice input spelling a correction of the user selection; determining a corrected recognition output for the selected portion; and providing a second recognition output that merges the first recognition output and the corrected recognition output.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          6\n",
       "Leveraging content dimensions during the translation of human-readable languages. A content management system (CMS) and a translation management system (TMS) can utilize content dimensions for content items to manage and translate the content items between languages. Machine and human translations of complex dynamic content can also be improved by pre-rendering the content to remove localization-related syntax prior to machine or human translation. Content items can also be scored as to their suitability for localization prior to translation, and translation can be skipped for content items that do not have a sufficiently high score. Semantic and natural language processing (NLP) techniques can also be utilized for content categorization and routing. Translations of content items can also be continuously refined and higher quality re-translated content can be provided in an automated fashion.                                                                                                                                                                                                                                                                                       7\n",
       "Multi-language support for interfacing with distributed data. A data analysis system stores in-memory representation of a distributed data structure across a plurality of processors of a parallel or distributed system. Client applications interact with the in-memory distributed data structure to process queries using the in-memory distributed data structure and to modify the in-memory distributed data structure. The data analysis system creates uniform resource identifier (URI) to identify each in-memory distributed data structure. The URI can be communicated from one client application to another application using communication mechanisms outside the data analysis system, for example, by email, thereby allowing other client devices to interact with a particular in-memory distributed data structure. The in-memory distributed data structure can be a machine learning model that is trained by one client device and executed by another client device. A client application can interact with the in-memory distributed data structure using different programming languages.                                                                                                          8\n",
       "Using priority scores for iterative precision reduction in structured lookups for questions. An approach is provided in which a knowledge manager matches a question to multiple natural language contexts that each correspond to relations associated with entities in a structured resource. The knowledge manager identifies database queries corresponding to the multiple natural language contexts and assigns priority scores to the database queries based upon their relative specificity. In turn, the knowledge manager invokes one of the database queries based upon the assigned priority scores.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                9\n",
       "Systems and methods for generating responses to natural language queries. Computer-implemented systems and methods are provided for analyzing and responding to a query from a user. Consistent with certain embodiments, systems and methods are provided for receiving a query from the user and dividing the query into query segments based on a set of grammar rules. Further, systems and methods are provided for selecting a first segment from the query segments, receiving at least one tuple stored in association with the user, selecting a second segment from the at least one tuple. Additionally, systems and methods are provided for receiving information related to the first and second segments, and generating a response to the query based on the received information. In addition, systems and methods are provided for transmitting information to a display device for presenting the response to the user.                                                                                                                                                                                                                                                                                     10\n",
       "System for determination of automated response follow-up. Aspects include determination of automated response follow-up. A response to a question is received at a response follow-up system. The response follow-up system analyzes the response using natural language processing to identify one or more response terms. The response follow-up system determines one or more follow-up questions based on the one or more response terms. The response follow-up system modifies an aspect of a user interface based on the one or more follow-up questions.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               11\n",
       "System for generation of automated response follow-up. Aspects include generation of automated response follow-up. A response to a question is received at a response follow-up system. The response follow-up system analyzes the response using natural language processing to identify one or more response terms. The response follow-up system generates one or more follow-up questions based on the one or more response terms. The response follow-up system modifies an aspect of a user interface based on the one or more follow-up questions.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      12\n",
       "High-capacity machine learning system. The present disclosure is directed to a high-capacity training and prediction machine learning platform that can support high-capacity parameter models (e.g., with 10 billion weights). The platform implements a generic feature transformation layer for joint updating and a distributed training framework utilizing shard servers to increase training speed for the high-capacity model size. The models generated by the platform can be utilized in conjunction with existing dense baseline models to predict compatibilities between different groupings of objects (e.g., a group of two objects, three objects, etc.).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     13\n",
       "Machine learning of predictive models using partial regression trends. An input is selected from a set of inputs used by a prediction model to produce an initial predicted value of an outcome. A changed predicted value of the outcome is produced by removing the selected input from the inputs to the model. An actual value of the outcome is obtained. A label residual is computed using the actual value and the changed predicted value. A second prediction model is formed to predict a value of the selected input. A variable residual is computed using an actual value and the predicted value of the selected input. An expression is generated of a plot of the label residual and the variable residual. The selected input is transformed, to form a transformed selected input, where the model produces a second predicted value of the outcome by using the transformed selected input.                                                                                                                                                                                                                                                                                                                14\n",
       "System and method for providing follow-up responses to prior natural language inputs of a user. In certain implementations, follow-up responses may be provided for prior natural language inputs of a user. As an example, a natural language input associated with a user may be received at a computer system. A determination of whether information sufficient for providing an adequate response to the natural language input is currently accessible to the computer system may be effectuated. A first response to the natural language input (that indicates that a follow-up response will be provided) may be provided based on a determination that information sufficient for providing an adequate response to the natural language input is not currently accessible. Information sufficient for providing an adequate response to the natural language input may be received. A second response to the natural language input may then be provided based on the received sufficient information.                                                                                                                                                                                                              15\n",
       "Device-described natural language control. A remote device has an associated natural language description that includes a record of commands supported by the remote device. This record of commands includes command names, the command functions to which those names correspond, and natural language strings that are the natural language words or phrases that correspond to the command. A computing device includes a device control module that obtains the natural language description for the remote device and provides the natural language strings to a natural language assistant on the computing device. The natural language assistant monitors the natural language inputs to the computing device, and notifies the device control module when a natural language input matches one of the natural language strings. The device control module uses the natural language description to determine the command name that corresponds to the matching natural language string, and communicates the command name to the remote device.                                                                                                                                                                      16\n",
       "Contextual entity resolution. Methods and systems for resolving entities using multi-modal functionality are described herein. Voice activated electronic devices may, in some embodiments, be capable of displaying content using a display screen. Contextual metadata representing the content rendered by the display screen may describe entities having similar attributes as an identified intent from natural language understanding processing. When natural language understanding processing attempts to resolve one or more declared slots for a particular intent, matching slots from the contextual metadata may be determined, and the matching entities may be placed in an intent selected context file to be included with the natural language understanding's output data. The output data may be provided to a corresponding application for causing one or more actions to be performed.                                                                                                                                                                                                                                                                                                                18\n",
       "Scalable endpoint-dependent natural language understanding. A computer-implemented technique is described for processing a linguistic item (e.g., a query) in an efficient and scalable manner. The technique interprets the linguistic item using a language understanding (LU) system in a manner that is based on a particular endpoint mechanism from which the linguistic item originated. The LU system may include an endpoint-independent subsystem, an endpoint-dependent subsystem, and a ranking component. The endpoint-independent subsystem interprets the linguistic item in a manner that is independent of the particular endpoint mechanism. The endpoint-dependent subsystem interprets the linguistic item in a manner that is dependent on the particular endpoint mechanism. The ranking component generates final interpretation results based on intermediate results generated by the endpoint-independent subsystem and the endpoint-dependent subsystem, e.g., by identifying the most likely interpretation of the linguistic item.                                                                                                                                                                19\n",
       "Compound service performance metric framework. Techniques described herein include determining, maintaining, and applying compound service performance metrics, based on data metrics from a plurality of different services. Service-specific data metrics may be received from a plurality of different communication services offered by a service provider, for example, Internet service, voice service, video service, SMS service, etc. Different combinations, relationships, and weighting factors for the data metrics may be defined and stored for each compound performance metric. Compound performance metrics may be defined, including for example, compound customer sentiment metrics, compound customer value metrics, and/or compound customer resource usage metrics. In some cases, machine-learning and/or analytics may be performed using service-specific data metrics and corresponding customer actions, in order to determine correlations between particular combinations of data metrics and customer actions.                                                                                                                                                                                 20\n",
       "Identifying an entity associated with an online communication. An approach is described for identifying an entity associated with a communication in an online environment. An associated system may include a processor and a memory storing an application program, which, when executed on the processor, performs an operation. The operation may include receiving a communication within the online environment. The communication may include a plurality of sequential messages. The operation further may include facilitating parsing, via natural language processing, of language in the communication corresponding to an entity and one or more sentiments associated with the entity. The operation further may include determining whether the entity is unambiguously identifiable. Upon determining that the entity is not unambiguously identifiable, the operation may include identifying the entity based upon Bayesian inference. According to an embodiment, determining whether the entity is unambiguously identifiable may include determining whether the entity is among a plurality of participants in the communication.                                                                        21\n",
       "Intelligently splitting text in messages posted on social media website to be more readable and understandable for user. A method, system and computer program product for improving readability and understandability in messages posted on a social media website. The messages posted on a social media website, such as the user's social networking feed, are scanned. The scanned messages are analyzed for topics, meaning and/or tenses using natural language processing. The text in the scanned messages are split into message segments based on topic, meaning, tenses, punctuation, custom identifiers, hashtags and/or @ symbols. These message segments are then grouped based on relatedness of the topics, meaning and/or tenses. The message segments are ordered in each group of message segments, such as based on timestamps. The ordered message segments are then displayed to the user. By displaying these message segments in separate groupings in a logical order, the user will be able to view the messages posted on the user's social media website in a more readable and understandable manner.                                                                                            22\n",
       "Inferential analysis using feedback for extracting and combining cyber risk information. Various embodiments of the present technology include methods of assessing risk of a cyber security failure in a computer network of an entity. Some embodiments involve using continual or periodic data collecting to improve inferential analysis, as well as obtaining circumstantial or inferential information from social networks. Machine learning may be used to improve predicitive capabilities. Some embodiments allow for identification of an entity from circumstantial or inferential information based on the machine learning and comparative analyses.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            23\n",
       "Alignment of data captured by autonomous vehicles to generate high definition maps. A high-definition map system receives sensor data from vehicles travelling along routes and combines the data to generate a high definition map for use in driving vehicles, for example, for guiding autonomous vehicles. A pose graph is built from the collected data, each pose representing location and orientation of a vehicle. The pose graph is optimized to minimize constraints between poses. Points associated with surface are assigned a confidence measure determined using a measure of hardness/softness of the surface. A machine-learning-based result filter detects bad alignment results and prevents them from being entered in the subsequent global pose optimization. The alignment framework is parallelizable for execution using a parallel/distributed architecture. Alignment hot spots are detected for further verification and improvement. The system supports incremental updates, thereby allowing refinements of sub-graphs for incrementally improving the high-definition map for keeping it up to date.                                                                                         24\n",
       "Machine-learning approach to holographic particle characterization. Holograms of colloidal dispersions encode comprehensive information about individual particles' three-dimensional positions, sizes and optical properties. Extracting that information typically is computation-ally intensive, and thus slow. Machine-learning techniques based on support vector machines (SVMs) can analyze holographic video microscopy data in real time on low-power computers. The resulting stream of precise particle-resolved tracking and characterization data provides unparalleled insights into the composition and dynamics of colloidal dispersions and enables applications ranging from basic research to process control and quality assurance.                                                                                                                                                                                                                                                                                                                                                                                                                                                                        25\n",
       "Dynamic semantic analysis on free-text reviews to identify safety concerns. Disclosed are various embodiments for identifying safety concerns by employing dynamic semantic analysis on natural language provided in free-text reviews of products. A computing environment may encode user interface data that causes an event listener to monitor a free-text description provided in a text field in a user interface. A semantic analysis may be performed on the free-text description in real-time as the free-text description is generated. Remedial actions may be performed based on the semantics identified in the free-text description or severity levels associated with identified safety concerns.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            26\n",
       "Knowledge-based editor with natural language interface. A computer-implemented method for knowledge based ontology editing, is provided. The method receives a language instance to update a knowledge base, using a computer. The method semantically parses the language instance to detect an ontology for editing. The method maps one or more nodes for the ontology for editing based on an ontology database and the knowledge base. The method determines whether the mapped nodes are defined or undefined within the knowledge base. The method calculates a first confidence score based on a number of the defined and undefined mapped nodes. Furthermore, the method updates the knowledge base when the first confidence score meets a pre-defined threshold.                                                                                                                                                                                                                                                                                                                                                                                                                                                   27\n",
       "Abstraction of syntax in localization through pre-rendering. A content management system (CMS) and a translation management system (TMS) can utilize content dimensions for content items to manage and translate the content items between languages. Machine and human translations of complex dynamic content can also be improved by pre-rendering the content to remove localization-related syntax prior to machine or human translation. Content items can also be scored as to their suitability for localization prior to translation, and translation can be skipped for content items that do not have a sufficiently high score. Semantic and natural language processing (NLP) techniques can also be utilized for content categorization and routing. Translations of content items can also be continuously refined and higher quality re-translated content can be provided in an automated fashion.                                                                                                                                                                                                                                                                                                           28\n",
       "Question and answer system emulating people and clusters of blended people. Embodiments are directed to an information processing system for generating answers in response to questions. The system includes a memory, a processor system communicatively coupled to the memory. The processor system is configured to store in the memory data of a corpus of a predetermined entity, and receive a question comprising a natural language format. The processor circuit is further configured to analyze the data of the corpus of the predetermined entity to derive an emulated answer to the question, wherein the emulated answer includes an emulation of an actual answer that would be provided by the predetermined entity.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         29\n",
       "Hybrid natural language processor. Methods and a natural language processor for processing a natural language query are provided. The processor includes a classifier, a rule-based pre-processor, a rule-based post-processor, a named entity recognizer, and an output module. The method involves receiving a text representation of the natural language query, pre-processing the text representation, applying a classification statistical model to the text representation when pre-processing fails, applying a post-processing rule, and performing name entity recognition.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         30\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             ... \n",
       "Intelligent caching of responses in a cognitive system. Mechanism are provided for implementing an intelligent response caching engine. The mechanisms receive a set of attributes to be used to cache responses to requests in a response cache and analyze a corpus of natural language text associated with the responses to requests to identify ranges of values associated with one or more of the attributes. The mechanisms generate a plurality of caching buckets based on the identified ranges of values associated with the one or more attributes and generate, for each caching bucket in the plurality of caching buckets, a corresponding cache key. The responses are stored in the response cache in association with a cache key corresponding to a caching bucket in which the response is a member. A request is subsequently processed by retrieving a cached response from the response cache.                                                                                                                                                                                                                                                                                                        969\n",
       "Aviation field service report natural language processing. An aircraft service information handling system comprises an input module operable to collect field service narrative data. A natural language data extraction module extracts problem data and related solution data from the narrative data, and a database module populates an aircraft service information database with the extracted problem data and the related extracted solution data. The database module further searches the database for populated problem data, and retrieves the related populated solution data.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  970\n",
       "Managing credibility for a question answering system. A method and system for managing credibility of a set of search results for a search query is disclosed. The method can include determining, by a natural language processing technique configured to analyze a portion of the set of search results and a portion of the search query, a credibility factor configured to indicate similarity to a subject matter of the search query. The method can also include establishing a relevance relationship between the credibility factor and source information of a first search result of the set of search results, wherein the source information is based on the credibility factor. The method may also include computing a credibility score for the first search result of the set of search results based on the relevance relationship between the credibility factor and the source information of the set of search results.                                                                                                                                                                                                                                                                                971\n",
       "Managing credibility for a question answering system. A method and system for managing credibility of a set of search results for a search query is disclosed. The method can include determining, by a natural language processing technique configured to analyze a portion of the set of search results and a portion of the search query, a credibility factor configured to indicate similarity to a subject matter of the search query. The method can also include establishing a relevance relationship between the credibility factor and source information of a first search result of the set of search results, wherein the source information is based on the credibility factor. The method may also include computing a credibility score for the first search result of the set of search results based on the relevance relationship between the credibility factor and the source information of the set of search results.                                                                                                                                                                                                                                                                                972\n",
       "Contextual content graph for automatic, unsupervised summarization of content. A method, system and computer-usable medium are disclosed for using a contextual graph to summarize a corpus of content. Natural Language Processing (NLP) preprocessing operations are performed on text within an input corpus to form a grammatical analysis. In turn, the grammatical analysis is used to generate semantic associations between phrases in the input corpus. The resulting semantic associations are then used to determine the thematic relevance of the individual sentences in the input corpus to form a context-based ranking. In turn, the context-based ranking is used to construct a context graph, the vertices of which are represented by phrases, and the edges are represented by an aggregate score resulting from performing calculations associated with semantic similarity of the phrases. The resulting context graph is then used to generate a content summarization for the input corpus.                                                                                                                                                                                                          973\n",
       "Segmenting scenes into sematic components using neurological readings. Computer vision systems for segmenting scenes into semantic components identify a differential within the physiological readings from the user. The differential corresponds to a semantic boundary associated with the user's gaze. Based upon data gathered by a gaze tracking device, the computer vision system identifies a relative location of the user's gaze at the time of the identified differential. The computer vision system then associates the relative location of the user's gaze with a semantic boundary.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        974\n",
       "Cold start machine learning algorithm. In an example embodiment, a first plurality of images stored on a computing device is identified, each image having an indication that it depicts a first member of a social networking service. The first plurality of images is used as training data to a first machine learning algorithm to train a first machine learning algorithm model corresponding to the first member, the first machine learning algorithm model corresponding to the first member designed to calculate a member likelihood score for a candidate image. Then a second plurality of images stored on the computing device is obtained. Each image of the second plurality of images is fed to the first machine learning algorithm model corresponding to the first member, obtaining a member likelihood score for each of the second plurality of images. Then, based on the member likelihood scores for the second plurality of images, one or more member images are selected.                                                                                                                                                                                                                      975\n",
       "Interactive visualization of machine-learning performance. Methods, computer systems, computer-storage media, and graphical user interfaces are provided for visualizing a performance of a machine-learned model. An interactive graphical user interface includes an item representation display area that displays a plurality of item representations corresponding to a plurality of items processed by the machine-learned model. The plurality of item representations are arranged according to scores assigned to the plurality of items by the machine-learned model. Further, each of the plurality of item representations is visually configured to represent a label assigned to a corresponding item.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          976\n",
       "Feature processing recipes for machine learning. A first representation of a feature processing recipe is received at a machine learning service. The recipe includes a section in which groups of variables on which common transformations are to be applied are defined, and a section in which a set of transformation operations are specified. The first representation of the recipe is validated based at least in part on a library of function definitions supported by the service, and an executable version of the recipe is generated. In response to a determination that the recipe is to be executed on a particular data set, a set of provider network resources is used to implement a transformation operation indicated in the recipe.                                                                                                                                                                                                                                                                                                                                                                                                                                                                  977\n",
       "Context aware hearing optimization engine. One or more context aware processing parameters and an ambient audio stream are received. One or more sound characteristics associated with the ambient audio stream are identified using a machine learning model. One or more actions to perform are determined using the machine learning model and based on the one or more context aware processing parameters and the identified one or more sound characteristics. The one or more actions are performed.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   978\n",
       "Artificial intelligence for infrastructure management. Described are techniques for performing system management such as a system including one or more data storage systems or appliances. User input may be received by a chat processing engine. The user input may be a natural language input to perform a request. The user input may be received from a management application used to perform data storage management. The chat processing engine may perform first processing of the user input to determine a response to the request. The response may be sent to the management application. The natural language input from a user may be in the form of text or voice that is converted to text for processing by the chat processing engine.                                                                                                                                                                                                                                                                                                                                                                                                                                                                   979\n",
       "Machine learning method and machine learning apparatus learning operating command to electric motor and controller and electric motor apparatus including machine learning apparatus. A controller that makes an electric motor efficiently operate in accordance with an ambient temperature. The controller includes a machine learning apparatus learning an operating command to the electric motor. The machine learning apparatus includes a status observing part and learning part. The status observing part observes an ambient temperature of an electric motor apparatus and a cycle time of the electric motor as status variables. The learning part learns an operating command to the electric motor in accordance with a training data set prepared based on a combination of the judgment data acquired by a judgment data acquiring part and the status variables.                                                                                                                                                                                                                                                                                                                                         980\n",
       "Information throttle based on compliance with electronic communication rules. Throttles electronic devices based on compliance with rules for electronic communications such as emails, texts, or postings on social media sites. Rules may for example prohibit certain topics, language, or behaviors such as online bullying. If system detects a violation of the electronic communications rules, it may block access or reduce performance on one or more electronic devices as a consequence. In some cases, devices or selected apps or services may continue to function, but at a reduced level. Conversely the system may provide rewards for conforming with the rules. Throttling of devices may also depend on other factors, such as homework completion, test results, grades, and environmental conditions. Machine learning techniques may be applied to determine when electronic communications may violate the rules. For example, probabilistic topic models may be applied to determine the topics of electronic communications, and to assess whether these topics violate the rules.                                                                                                                 981\n",
       "Intelligently splitting text in messages posted on social media website to be more readable and understandable for user. A method, system and computer program product for improving readability and understandability in messages posted on a social media website. The messages posted on a social media website, such as the user's social networking feed, are scanned. The scanned messages are analyzed for topics, meaning and/or tenses using natural language processing. The text in the scanned messages are split into message segments based on topic, meaning, tenses, punctuation, custom identifiers, hashtags and/or @ symbols. These message segments are then grouped based on relatedness of the topics, meaning and/or tenses. The message segments are ordered in each group of message segments, such as based on timestamps. The ordered message segments are then displayed to the user. By displaying these message segments in separate groupings in a logical order, the user will be able to view the messages posted on the user's social media website in a more readable and understandable manner.                                                                                           982\n",
       "Diverse radio frequency signature, video, and image sensing for detection and localization. Systems and methods can support coprocessing radio signals and video to identify and locate a radio transmitter. Positions and orientations for cameras and RF sensors may be maintained. An RF signature associated with the radio transmitter may be received from the RF sensors to determine an RF persona. A first physical location for the radio transmitter may be estimated according to a physical radio propagation model operating on RF signals. A video stream from one or more of the cameras may be received. An individual may be identified in the video stream using computer vision techniques. A second physical location for the radio transmitter may be estimated from the video stream. Relationships may be established between the first physical location and the second physical location and between the RF persona and the identified individual. The relationships may be presented to an operator interface.                                                                                                                                                                                     983\n",
       "Systems, devices, and methods for gesture identification. Systems, devices, and methods adapt established concepts from natural language processing for use in gesture identification algorithms. A gesture identification system includes sensors, a processor, and a non-transitory processor-readable memory that stores data and/or instructions for performing gesture identification. A gesture identification system may include a wearable gesture identification device. The gesture identification process involves segmenting signals from the sensors into data windows, assigning a respective “window class” to each data window, and identifying a user-performed gesture based on the corresponding sequence of window classes. Each window class exclusively characterizes at least one data window property and is analogous to a “letter” of an alphabet. Under this model, each gesture is analogous to a “word” made up of a particular combination of window classes.                                                                                                                                                                                                                                   984\n",
       "Inferring type classifications from natural language text. A device may obtain text to be processed to infer type classifications associated with terms in the text. The type classifications may indicate types of values that the terms are intended to represent. The device may infer type classifications corresponding to terms in the text by performing a type classification technique. The type classification technique may include a name-based analysis, a context-based analysis a synonym-based analysis, or a valued-based analysis. These analyses may compare information, associated with the terms in the text, to type indicators that indicate the type classifications. The device may provide information that identifies a type relationship between a particular type classification and a particular term based on inferring the one or more type classifications.                                                                                                                                                                                                                                                                                                                                 985\n",
       "Natural language relatedness tool using mined semantic analysis. Mined semantic analysis techniques (MSA) include generating a first subset of concepts, from a NL corpus, that are latently associated with an NL candidate term based on (i) a second subset of concepts from the corpus that are explicitly or implicitly associated with the candidate term and (ii) a set of concept association rules. The concept association rules are mined from a transaction dictionary constructed from the corpus and defining discovered latent associations between corpus concepts. A concept space of the candidate term includes at least portions of both the first and second subset of concepts, and includes indications of relationships between latently-associated concepts and the explicitly/implicitly-associated concepts from which the latently-associated concepts were derived. Measures of relatedness between candidate terms are deterministically determined based on their respective concept spaces. Example corpora include digital corpora such as encyclopedias, journals, intellectual property datasets, health-care related datasets/records, financial-sector related datasets/records, etc.    987\n",
       "Gender and name translation from a first to a second language. A system and method are provided for inferring person gender and translating people's names into a second language. The present invention translates an individual's name into a required second language to reduce waiting time in registration areas. It also is able to infer gender once the registration clerk enters the individual's first name in a native language. It also prevents duplication of a person's record generated because of the confusion that happens around how a native name is translated into a second language by standardizing such translation. The embodiments of the present invention utilize machine learning and statistical approaches to infer gender and translate an individual's name into a second language.                                                                                                                                                                                                                                                                                                                                                                                                        988\n",
       "Internet based method and system for ranking individuals using a popularity profile. Technologies are described herein for determining popularity of an individual. A popularity profiling server is used to receive information of the individual, analyze a quantitative data portion of the information, and analyze a qualitative data portion of the information using sentiment analysis. The sentiment analysis uses natural language parsing to separate the qualitative data into parts of speech. The popularity profiling server calculates an initial set of scores for the popularity of the individual based on the analysis of the quantitative data portion and the qualitative data portion of the information, update a ranking for the popularity of the individual in a database, and determine, after waiting a predetermined amount of time, if new data has been updated to the information on the information server. If new information has been updated, an updated set of scores are calculated.                                                                                                                                                                                                   989\n",
       "System and method for automatic, unsupervised contextualized content summarization of single and multiple documents. A method, system and computer-usable medium are disclosed for generating a context-sensitive summarization of a corpus of content. Natural Language Processing (NLP) operations are performed on text within an input corpus to extract phrases, which are then used to generate a grammatical analysis. In turn, the grammatical analysis is used to determine the thematic relevance of individual sentences in the input corpus. Sentences within the input corpus are then ranked according to their respective thematic relevance. This ranking is used to construct a contextualized content graph, which in turn is used to generate a content summarization for the input corpus.                                                                                                                                                                                                                                                                                                                                                                                                                990\n",
       "Natural language solution generating devices and methods. Natural language solution generating devices and methods are provided herein. Exemplary devices may execute logic via one or more processors, which are programmed to receive a complex query in natural language format, the complex query including a real-world problem that requires interrogation of a plurality of information sources in order to ascertain a response to the problem, evaluate the complex query to determine query segments, which are each included with at least one domain, wherein a domain corresponds to an information source, query the information sources to obtain responses for the query segments, and generate a natural language solution using the responses.                                                                                                                                                                                                                                                                                                                                                                                                                                                              991\n",
       "Methods and systems for real-time user extraction using deep learning networks. Methods and systems for real-time user extraction using deep learning networks. In one embodiment, user extraction comprises obtaining a given frame of color pixel data, checking whether a reset flag is cleared or set, and generating a trimap for the given frame. If the reset flag is cleared, generating the trimap comprises: obtaining a user-extraction contour based on a preceding frame; and generating the trimap based on the obtained user-extraction contour. If the reset flag is set, generating the trimap comprises: detecting at least one persona feature in the given frame; generating an alpha mask by aligning an intermediate contour with the detected persona feature(s), wherein the intermediate contour is based on a color-based flood-fill operation performed on a previous frame which was segmented by a machine-learning-segmentation process; and generating the trimap based on the generated alpha mask. The generated trimap is output for extracting a user persona.                                                                                                                             992\n",
       "Optimizing a visual perspective of media. One or more signals are used to identify regions of interest of an image. The signals are applied to the image to generate one or more models that are based on the regions of interest. The models may present different perspectives of the image by emphasizing various features and focal points. The models may be ranked and displayed according to a scoring paradigm that is based on one or more signals. Multi-tiered feedback mechanisms allow for the collection of user intent and/or other forms of explicit input. Feedback associated to the models may be obtained and used to generate additional models that are based on one or more signals and the feedback. The feedback may also be stored and utilized for machine learning purposes.                                                                                                                                                                                                                                                                                                                                                                                                                      993\n",
       "Intelligently splitting text in messages posted on social media website to be more readable and understandable for user. A method, system and computer program product for improving readability and understandability in messages posted on a social media website. The messages posted on a social media website, such as the user's social networking feed, are scanned. The scanned messages are analyzed for topics, meaning and/or tenses using natural language processing. The text in the scanned messages are split into message segments based on topic, meaning, tenses, punctuation, custom identifiers, hashtags and/or @ symbols. These message segments are then grouped based on relatedness of the topics, meaning and/or tenses. The message segments are ordered in each group of message segments, such as based on timestamps. The ordered message segments are then displayed to the user. By displaying these message segments in separate groupings in a logical order, the user will be able to view the messages posted on the user's social media website in a more readable and understandable manner.                                                                                           994\n",
       "Collision avoidance using auditory data. A controller for an autonomous vehicle receives audio signals from one or more microphones. The outputs of the microphones are pre-processed to enhance audio features that originated from vehicles. The outputs may also be processed to remove noise. The audio features are input to a machine learning model that classifies the source of the audio features. For example, features may be classified as originating from a vehicle. A direction to a source of the audio features is determined based on relative delays of the audio features in signals from multiple microphones. Where audio features are classified with an above-threshold confidence as originating from a vehicle, collision avoidance is performed with respect to the direction to the source of the audio features.                                                                                                                                                                                                                                                                                                                                                                                995\n",
       "Process flow diagramming based on natural language processing. Non-limiting examples of the present disclosure describe natural language translation capabilities that enable automated process flow diagram generation from received input. Input may be received through an application for automated generation of a process flow diagram. The received input may be provided to a natural language processing component of a language understanding intelligence service. A data object, received from the natural language processing component, may be accessed. The data object provides data for creation of a process flow diagram based on the received input. In examples, the data object is generated based on natural language processing by the natural language processing component and at least one user defined grammar rule, provided by the application, for converting the received input to one or more process flow steps. The process flow diagram may be presented within the application. Other examples are also described such as reverse engineering an existing process flow diagram.                                                                                                          996\n",
       "Generating search strings and refinements from an image. Approaches include using a machine learning-based approach to generating search strings and refinements based on a specific item represented in an image. For example, a classifier that is trained on descriptions of images can be provided. An image that includes a representation of an item of interest is obtained. The image is analyzed using the classifier algorithm to determine a first term representing a visual characteristic of the image. Then, the image is analyzed again to determine a second term representing another visual characteristic of the image based at least in part on the first term. Additional terms can be determined to generate a description of the image, including characteristics of the item of interest. Based on the determined characteristics of the item of interest, a search query and one or more refinements can be generated.                                                                                                                                                                                                                                                                              997\n",
       "Natural language management of online social network connections. Natural language management of online social network connections may comprise receiving natural language data associated with a user's social network interactions. The natural language data associated with a user's social network interactions is analyzed and features used in the user's social network interactions are determined based on the analysis. The contexts in which the features are used in the user's social network interactions may be also determined. Point values to the features used in the user's social network interactions are assigned. A fingerprint of the user is created at least based on the features and the point values. The created fingerprint may be compared with information associated with online communities, and based on the comparison, one or more of the online communities may be recommended as user's potential social network connections.                                                                                                                                                                                                                                                       998\n",
       "Readability awareness in natural language processing systems. Electronic natural language processing in a natural language processing (NLP) system, such as a Question-Answering (QA) system. A receives electronic text input, in question form, and determines a readability level indicator in the question. The readability level indicator includes at least a grammatical error, a slang term, and a misspelling type. The computer determines a readability level for the electronic text input based on the readability level indicator, and retrieves candidate answers based on the readability level.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              999\n",
       "Length: 972, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf vec requires list, not just string\n",
    "unseen_data = 'computer vision natural language processor'\n",
    "unseen_data=[unseen_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_tfidf = tfidf.transform(unseen_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #2 - Apply K means clustering to distance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmresult = km.fit(tfidf_matrix).predict(unseen_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmresult_p = km.predict(unseen_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15], dtype=int32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmresult_p"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
