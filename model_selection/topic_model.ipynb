{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from test_model import (get_patent_fields_list, get_ml_patents, \n",
    "                        create_title_abstract_col,trim_data, \n",
    "                        structure_dataframe, partition_dataframe, \n",
    "                        build_pipeline, process_docs, pat_inv_map, get_topics)\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.corpora import Dictionary, mmcorpus\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models import AuthorTopicModel\n",
    "from gensim.test.utils import common_dictionary, datapath, temporary_file\n",
    "from smart_open import smart_open\n",
    "\n",
    "import spacy\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, punkt, RegexpTokenizer, wordpunct_tokenize\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
    "\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "import calendar\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Lee) - resolve deprecation warnings\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acquire data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# acquire dataset of ML patents by call to PatentsView API \n",
    "raw_data_1000 = get_ml_patents(pats_per_page=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_data_1000[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adhoc requests calls\n",
    "endpoint_url = 'http://www.patentsview.org/api/patents/query'\n",
    "query={\"_or\":[{\"_text_phrase\":{\"patent_title\":\"natural language\"}},\n",
    "              {\"_text_phrase\":{\"patent_abstract\":\"natural language\"}},\n",
    "              {\"_text_phrase\":{\"patent_abstract\":\"machine learning\"}},\n",
    "              {\"_text_phrase\":{\"patent_title\":\"machine learning\"}},\n",
    "              {\"_text_phrase\":{\"patent_abstract\":\"computer vision\"}},\n",
    "              {\"_text_phrase\":{\"patent_abstract\":\"computer vision\"}}]}\n",
    "pat_fields = get_patent_fields_list()\n",
    "fields = pat_fields\n",
    "pats_per_page = 1000\n",
    "options = {\"per_page\": pats_per_page}\n",
    "sort = [{\"patent_date\": \"desc\"}]\n",
    "\n",
    "params = {'q': json.dumps(query),\n",
    "          'f': json.dumps(fields),\n",
    "          'o': json.dumps(options),\n",
    "          's': json.dumps(sort)}\n",
    "\n",
    "# request and results\n",
    "response = requests.get(endpoint_url, params=params)\n",
    "status = response.status_code\n",
    "results = response.json()\n",
    "# print(results)\n",
    "count = results.get(\"count\")\n",
    "data_resp = results['patents']\n",
    "total_pats = results.get(\"total_patent_count\")\n",
    "# return data_resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify fields (key:val pairs) to retain from full api response\n",
    "retained_keys = ['patent_number', 'patent_date', 'patent_title', 'patent_abstract', 'inventors']\n",
    "data = trim_data(data=raw_data, keys=retained_keys)\n",
    "\n",
    "# create new key:value pair by combining values from patent_title and patent_abstract keys\n",
    "data = create_title_abstract_col(data=data)\n",
    "\n",
    "# create dataframe, organize columns and sort by patent_date\n",
    "df = structure_dataframe(data=data)\n",
    "\n",
    "# partition data\n",
    "data_train, data_test = partition_dataframe(df, .8)\n",
    "\n",
    "# convert dataframe to list\n",
    "text_data = df.patent_title_abstract.tolist()\n",
    "text_train = data_train.patent_title_abstract.tolist()\n",
    "text_test = data_test.patent_title_abstract.tolist()\n",
    "\n",
    "# TODO (Lee) - this explores direct structuring from api response without df\n",
    "text_list = []\n",
    "for i in data:\n",
    "    text_list.append(i['patent_title_abstract'])\n",
    "# text_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'ner']\n"
     ]
    }
   ],
   "source": [
    "# uncomment to download stop words\n",
    "# !python -m spacy download en\n",
    "stop_words = stopwords.words('/Users/lee/Documents/techniche/techniche/data/stopwords/english')\n",
    "\n",
    "# construct pipeline\n",
    "nlp = build_pipeline()\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# pre-process documents TODO (Lee) - via text_list directly from api response\n",
    "processed_docs_1 = process_docs(text_list)\n",
    "\n",
    "# pre-process documents TODO (Lee) - via df to list\n",
    "processed_docs_2 = process_docs(text_train)\n",
    "\n",
    "### Build corpus and dictionary\n",
    "\n",
    "# build dictionary\n",
    "id_to_word = Dictionary(processed_docs_2)\n",
    "\n",
    "# apply term-doc frequency (list of (token_id, token_count) tuples) to docs in corpus \n",
    "corpus_2 = [id_to_word.doc2bow(doc) for doc in processed_docs_2]\n",
    "\n",
    "# view formatted corpus\n",
    "formatted_corpus = [[(id_to_word[id], freq) for id, freq in text] for text in corpus]\n",
    "#formatted_corpus\n",
    "#id_to_word.token2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model #1: Genism LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Lee) - resolve deprecation warnings\n",
    "# construct LDA model\n",
    "model_lda = LdaModel(corpus=corpus_2,\n",
    "                     id2word=id_to_word,\n",
    "                     num_topics=5, \n",
    "                     random_state=100,\n",
    "                     update_every=1,\n",
    "                     chunksize=100,\n",
    "                     passes=10,\n",
    "                     alpha='auto',\n",
    "                     per_word_topics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #1 - Explore and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore topics visually\n",
    "pyLDAvis.enable_notebook()\n",
    "viz_topics_1 = pyLDAvis.gensim.prepare(model_lda, corpus_2, id_to_word)\n",
    "# viz_topics_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.019*\"system\" + 0.013*\"documentation\" + 0.013*\"user\" + 0.013*\"embodiments\" '\n",
      "  '+ 0.010*\"model\" + 0.010*\"learning\" + 0.010*\"machine\" + 0.009*\"based\" + '\n",
      "  '0.009*\"auto\" + 0.009*\"text\"'),\n",
      " (1,\n",
      "  '0.027*\"machine\" + 0.026*\"data\" + 0.025*\"learning\" + 0.017*\"plurality\" + '\n",
      "  '0.015*\"classifier\" + 0.014*\"method\" + 0.011*\"systems\" + 0.011*\"user\" + '\n",
      "  '0.011*\"information\" + 0.010*\"search\"'),\n",
      " (2,\n",
      "  '0.032*\"content\" + 0.017*\"language\" + 0.016*\"based\" + 0.013*\"one\" + '\n",
      "  '0.013*\"method\" + 0.013*\"natural\" + 0.011*\"entity\" + 0.011*\"first\" + '\n",
      "  '0.011*\"machine\" + 0.011*\"provided\"'),\n",
      " (3,\n",
      "  '0.019*\"recognition\" + 0.015*\"state\" + 0.015*\"crowd\" + 0.013*\"method\" + '\n",
      "  '0.013*\"output\" + 0.013*\"electronic\" + 0.013*\"documents\" + 0.011*\"image\" + '\n",
      "  '0.011*\"patient\" + 0.009*\"one\"'),\n",
      " (4,\n",
      "  '0.030*\"one\" + 0.023*\"data\" + 0.019*\"system\" + 0.016*\"machine\" + '\n",
      "  '0.016*\"learning\" + 0.013*\"using\" + 0.013*\"response\" + 0.013*\"distributed\" + '\n",
      "  '0.012*\"value\" + 0.012*\"model\"')]\n"
     ]
    }
   ],
   "source": [
    "# keywords in n topics in corpus\n",
    "pprint(model_lda.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('0.030*\"one\" + 0.023*\"data\" + 0.019*\"system\" + 0.016*\"machine\" + '\n",
      " '0.016*\"learning\" + 0.013*\"using\" + 0.013*\"response\" + 0.013*\"distributed\" + '\n",
      " '0.012*\"value\" + 0.012*\"model\"')\n"
     ]
    }
   ],
   "source": [
    "# most important keywords, and the respective weight, that form topic with index 0\n",
    "pprint(model_lda.print_topic(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #1 - Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an unsupervised task, there are no labels with which to evaluate the \"expected\" prediction.\n",
    "\n",
    "- Per Radhurek: \"Each topic modeling method (LSI, LDA...) has ways to measure \"internal quality\" (perplexity, reconstruction error...), but these are an artifact of the particular approach taken (bayesian training, matrix factorization...). \n",
    "- No way to compare such scores across different types of topic models. \n",
    "- Extrinsic evaluation\n",
    "    - evaluate quality of superordinate task that topic models trained for (e.g., when task is retrieval of semantically similar documents, evaluate by manually tagging a set of similar documents and then see how well a given semantic model maps those similar documents together).\n",
    "    - Automatic tagging (word intrusion method) - Such manual tagging can be resource intensive, so people hae been looking for clever ways to automate it. In Reading tea leaves: How humans interpret topic models, Wallach&al suggest a \"word intrusion\" method that works well for models where the topics are meant to be \"human interpretable\", such as LDA. For each trained topic, they take its first ten words, then substitute one of them with another, randomly chosen word (intruder!) and see whether a human can reliably tell which one it was. If so, the trained topic is topically coherent (good); if not, the topic has no discernible theme (bad):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #1 - Evaluate - Coherence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate topic coherence for topic models.\n",
    "Implements 'CoherenceModel' coherence pipeline (segmentation, probability estimation, confirmation measure, aggregation) from Roeder et al., 2015. \"Exploring the space of topic coherence measures\", WSDM '15 Proceedings of the Eighth ACM International Conference on Web Search and Data Mining (WSDM) 2015, 399-408."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41013435823858185\n"
     ]
    }
   ],
   "source": [
    "# calculate coherence metric for test_set (n = 20 docs/100 docs)\n",
    "coherence = CoherenceModel(model=model_lda, texts=processed_docs_3, dictionary=id_to_word, coherence='c_v')\n",
    "coherence_1_3 = coherence.get_coherence()\n",
    "print(coherence_1_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4591438924438723\n"
     ]
    }
   ],
   "source": [
    "# calculate coherence metric for train set ((n = 80 docs/100 docs))\n",
    "coherence = CoherenceModel(model=model_lda, texts=processed_docs_2, dictionary=id_to_word, coherence='c_v')\n",
    "coherence_1_2 = coherence.get_coherence()\n",
    "print(coherence_1_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate coherence metric for each of the n topicss\n",
    "coherence_1 = coherence.get_coherence_per_topic()\n",
    "# print(coherence_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #1 - Evaluate - Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-process test set documents TODO (Lee) - via df to list\n",
    "processed_docs_3 = process_docs(text_test)\n",
    "\n",
    "# build dictionary\n",
    "id_to_word = Dictionary(processed_docs_3)\n",
    "\n",
    "# apply term-doc frequency (list of (token_id, token_count) tuples) to docs in corpus \n",
    "corpus_3 = [id_to_word.doc2bow(doc) for doc in processed_docs_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-8.987212168216839\n"
     ]
    }
   ],
   "source": [
    "# calculate perplexity metric \n",
    "\n",
    "# metric calculates and returns per-word likelihood bound using a chunk of documents as evaluation corpus\n",
    "# output calculated statistics, including the perplexity=2^(-bound), to log at INFO level\n",
    "# Returns the variational bound score calculated for each word\n",
    "\n",
    "perplexity_train = model_lda.log_perplexity(corpus_2)\n",
    "print(perplexity_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-8.987212097864088\n"
     ]
    }
   ],
   "source": [
    "perplexity_test = model_lda.log_perplexity(corpus_3)\n",
    "print(perplexity_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #1 - Evaluate - holdout set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Lee) - evaluate on 1k documents **not** used in LDA training\n",
    "doc_stream = (tokens for _, tokens in iter_wiki('./data/simplewiki-20140623-pages-articles.xml.bz2'))  # generator\n",
    "test_docs = list(itertools.islice(doc_stream, 8000, 9000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #1 - Evaluate - Doc split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Lee) - split each document into two parts, and check that 1) topics of the first half are similar to \n",
    "topics of the second 2) halves of different documents are mostly dissimilar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Lee)\n",
    "def intra_inter(model, test_docs, num_pairs=10000):\n",
    "    # split each test document into two halves and compute topics for each half\n",
    "    part1 = [model[id2word.doc2bow(tokens[: len(tokens) / 2])] for tokens in test_docs]\n",
    "    part2 = [model[id2word.doc2bow(tokens[len(tokens) / 2 :])] for tokens in test_docs]\n",
    "    \n",
    "    # print computed similarities (uses cossim)\n",
    "    print(\"average cosine similarity between corresponding parts (higher is better):\")\n",
    "    print(np.mean([gensim.matutils.cossim(p1, p2) for p1, p2 in zip(part1, part2)]))\n",
    "\n",
    "    random_pairs = np.random.randint(0, len(test_docs), size=(num_pairs, 2))\n",
    "    print(\"average cosine similarity between 10,000 random parts (lower is better):\")    \n",
    "    print(np.mean([gensim.matutils.cossim(part1[i[0]], part2[i[1]]) for i in random_pairs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Lee)\n",
    "print(\"LDA results:\")\n",
    "intra_inter(lda_model, test_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #1 - Evaluate - Log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #1 - Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #1 - Predict - Pickle model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle model\n",
    "pickle.dump(model_lda, open('/Users/lee/Documents/techniche/techniche/data/model_lda_1.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lda = pickle.load(open('/Users/lee/Documents/techniche/techniche/data/model_lda_1.pkl','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #1 - inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracts topics for given document from Gensim\n",
    "# TODO (Lee) - call from test_model.py\n",
    "def get_topics(doc, k=5, model_lda=model_lda):\n",
    "    topic_id = sorted(model_lda[doc][0], key=lambda x: -x[1])\n",
    "    top_k_topics = [x[0] for x in topic_id[:k]]\n",
    "    return [(i, model_lda.print_topic(i)) for i in top_k_topics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `get_document_topics()` returns topic probability distribution for given document\n",
    "# topic_dist_675_a = model_lda.get_document_topics(corpus[50])\n",
    "# pprint(sorted(topic_dist_50_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topicid = 3\n",
    "# model_lda.get_topic_terms(topicid, topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_train[doc_id]\n",
    "# doc_id = 675\n",
    "# topic_dist_675_b = sorted(get_topics(corpus[doc_id], k=10)), text_train[doc_id]\n",
    "# pprint(topic_dist_675_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = 'virtual dictionary lexicon enablement voice'.split()\n",
    "text_input_1 = 'smart assistant transformer model translation'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_word.doc2bow(text_input_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_topics(id_to_word.doc2bow(text_input_1), k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input_2 = \"\"\"At the Siri International team within Apple we bring the Siri intelligent assistant to our customers worldwide in over 40 languages and dialects. Join us, and tackle some of the most challenging problems in natural language processing and large scale applied machine learning. You will build cutting edge natural language understanding technologies and deploy them on a global scale. Your work will advance and shape the future vision of our multi-lingual, multi-cultural Siri assistant, and Search applications used by millions across the world Key Qualifications\n",
    "Extensive track record of scientific research in NLP and Machine Learning, or similar experience in developing language technologies for shipping products.\n",
    "Strong coding and software engineering skills in a mainstream programming language, such as Python, Java, C/C++.\n",
    "Familiarity with NLP/ML tools and packages like Caffe, pyTorch, TensorFlow, Weka, scikit-learn, nltk, etc.\n",
    "Practical experience building production quality applications related to natural language processing and machine learning.\n",
    "In-depth knowledge of machine learning algorithms and ability to apply them in data driven natural language processing systems.\n",
    "Ability to quickly prototype ideas / solutions, perform critical analysis, and use creative approaches for solving complex problems.\n",
    "Attention to detail and excellent communication skills.\n",
    "Description\n",
    "We are looking for a highly motivated technologist with a strong background in Natural Language Processing and Machine Learning research. The ideal candidate will have a strong track record of taking research ideas to real-world applications. In this position you will apply your problem solving skills to challenges and opportunities within Siri International, which involves development of large-scale language technologies for many natural languages worldwide. The primary responsibility of this role is to conduct research and develop innovative machine learning, artificial intelligence and NLP solutions for multi-lingual conversational agents. You will have the opportunity to investigate cutting edge research methods that will improve customer experience of our products and enable our engineers to scale these technologies across a variety of natural languages. You will also provide technical leadership and experiment-driven insights for engineering teams on their machine learning modeling and data decisions. You will play a central role in defining the future technical directions of Siri International through quick prototyping, critical analysis and development of core multi-lingual NLP technologies.\n",
    "Education & Experience\n",
    "* PhD in Machine Learning, Statistics, Computer Science, Mathematics or related field with specialization in natural language processing and/or machine learning, OR * Masters degree in a related field with a strong academic/industrial track record. * Hands-on research experience in an academic or industrial setting.\"\"\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_topics(id_to_word.doc2bow(text_input_2), k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print keywords in n topics\n",
    "sorted(model_lda.show_topics(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print keywords in n topics\n",
    "sorted(model_lda.print_topics(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print keywords in n topics\n",
    "sorted(model_lda.print_topics(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print keywords in n topics\n",
    "sorted(model_lda.print_topics(), key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_topic() returns n most important/relevant words, and their weights, that comprise given topic\n",
    "pprint(model_lda.show_topic(1, topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(model_lda.show_topics(num_topics=5, num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Gensim example - Alternate predict workflow - Create a new corpus, made of previously unseen documents.\n",
    "# other_texts = [\n",
    "#      ['computer', 'time', 'graph'],\n",
    "#      ['survey', 'response', 'eps'],\n",
    "#      ['human', 'system', 'computer']\n",
    "#  ]\n",
    "# other_corpus = [common_dictionary.doc2bow(text) for text in other_texts]\n",
    "# unseen_doc = other_corpus[0]\n",
    "# vector = lda[unseen_doc]  # get topic probability distribution for a document\n",
    "# Update the model by incrementally training on the new corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #2 - Mallet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to download Mallet topic model\n",
    "# !wget http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip\n",
    "# update path\n",
    "path_mallet = '/Users/lee/Documents/techniche/techniche/data/mallet-2.0.8/bin/mallet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = gensim.models.wrappers.LdaMallet(path_mallet, corpus=corpus, num_topics=25, id2word=id_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topics\n",
    "# pprint(model_2.show_topics(formatted=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Lee) - calculate coherence metric\n",
    "coherence_2 = CoherenceModel(model=model_2, texts=data, dictionary=id_to_word, coherence='c_v')\n",
    "coherence_2 = coherence_2.get_coherence()\n",
    "# print(coherence_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Lee)\n",
    "# def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "#     \"\"\"\n",
    "#     Compute c_v coherence for various number of topics\n",
    "\n",
    "#     Parameters:\n",
    "#     ----------\n",
    "#     dictionary : Gensim dictionary\n",
    "#     corpus : Gensim corpus\n",
    "#     texts : List of input texts\n",
    "#     limit : Max num of topics\n",
    "\n",
    "#     Returns:\n",
    "#     -------\n",
    "#     model_list : List of LDA topic models\n",
    "#     coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "#     \"\"\"\n",
    "#     coherence_values = []\n",
    "#     model_list = []\n",
    "#     for num_topics in range(start, limit, step):\n",
    "#         model = gensim.models.wrappers.LdaMallet(path_mallet, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "#         model_list.append(model)\n",
    "#         coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "#         coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "#     return model_list, coherence_values\n",
    "\n",
    "# model_list, coherence_values = compute_coherence_values(dictionary=id_to_word, corpus=corpus, texts=data, start=2, limit=40, step=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #3 - Author topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct inventor-to-doc mapping as df from nested inventors column in json api response\n",
    "# df_inventors = json_normalize(data, record_path=['inventors'], meta=['patent_number', 'patent_date'])\n",
    "# df_inventors = df_inventors[['inventor_id', 'patent_number', 'patent_date']]\n",
    "# df_inventors.sort_values(by=['patent_date'])\n",
    "# df_inventors.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick visual index to patent number mapping\n",
    "# for i in data:\n",
    "#     print(data.index(i), i['patent_number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO (Lee) review fix to pat_inv_map, in which \"patent\" in mapping is idx of pat, not pat_number from api\n",
    "pat2inv = pat_inv_map(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct author-topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct author-topic model\n",
    "model_at = AuthorTopicModel(corpus=corpus,\n",
    "                         doc2author=pat2inv,\n",
    "                         id2word=id_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct vectors for authors\n",
    "author_vecs = [model_at.get_author_topics(author) for author in model_at.id2author.values()]\n",
    "author_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve topic distribution for author using use model[name] syntax\n",
    "# each topic has a probability of being expressed given the particular author, \n",
    "# but only the ones above a certain threshold are displayed\n",
    "\n",
    "model_at['7788103-1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def show_author(name):\n",
    "#     print('\\n%s' % name)\n",
    "#     print('Docs:', model.author2doc[name])\n",
    "#     print('Topics:')\n",
    "#     pprint([(topic_labels[topic[0]], topic[1]) for topic in model[name]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build mapping from inventor to patent\n",
    "inv2pat = gensim.models.atmodel.construct_author2doc(pat2inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #3 - Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction functions that take input of new text string, and predict topic distribution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
