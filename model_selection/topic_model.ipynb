{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from test_model import (get_patent_fields_list, get_ml_patents, \n",
    "                        create_title_abstract_col,trim_data, \n",
    "                        structure_dataframe, partition_dataframe, \n",
    "                        build_pipeline, process_docs, pat_inv_map, get_topics)\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.corpora import Dictionary, mmcorpus\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models import AuthorTopicModel\n",
    "from gensim.test.utils import common_dictionary, datapath, temporary_file\n",
    "from smart_open import smart_open\n",
    "\n",
    "import spacy\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, punkt, RegexpTokenizer, wordpunct_tokenize\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
    "\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "import calendar\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Lee) - resolve deprecation warnings\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acquire data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# acquire dataset of ML patents by call to PatentsView API \n",
    "raw_data_1000 = get_ml_patents(pats_per_page=1000)\n",
    "raw_data_2000 = get_ml_patents(pats_per_page=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Acquire data - Structure data - 1000 pats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify fields (key:val pairs) to retain from full api response\n",
    "retained_keys = ['patent_number', 'patent_date', 'patent_title', 'patent_abstract', 'inventors']\n",
    "data_1000 = trim_data(data=raw_data_1000, keys=retained_keys)\n",
    "\n",
    "# create new key:value pair by combining values from patent_title and patent_abstract keys\n",
    "data_1000 = create_title_abstract_col(data=data_1000)\n",
    "\n",
    "# create dataframe, organize columns and sort by patent_date\n",
    "df_1000 = structure_dataframe(data=data_1000)\n",
    "\n",
    "# partition data\n",
    "data_train_1000, data_test_1000 = partition_dataframe(df_1000, .8)\n",
    "\n",
    "# convert dataframe to list\n",
    "text_data_1000 = df_1000.patent_title_abstract.tolist()\n",
    "text_train_1000 = data_train_1000.patent_title_abstract.tolist()\n",
    "text_test_1000 = data_test_1000.patent_title_abstract.tolist()\n",
    "\n",
    "# TODO (Lee) - this explores direct structuring from api response without df\n",
    "text_list_1000 = []\n",
    "for i in data_1000:\n",
    "    text_list_1000.append(i['patent_title_abstract'])\n",
    "# text_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Acquire data - Structure data - 2000 pats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify fields (key:val pairs) to retain from full api response\n",
    "retained_keys = ['patent_number', 'patent_date', 'patent_title', 'patent_abstract', 'inventors']\n",
    "data_2000 = trim_data(data=raw_data_2000, keys=retained_keys)\n",
    "\n",
    "# create new key:value pair by combining values from patent_title and patent_abstract keys\n",
    "data_2000 = create_title_abstract_col(data=data_2000)\n",
    "\n",
    "# create dataframe, organize columns and sort by patent_date\n",
    "df_2000 = structure_dataframe(data=data_2000)\n",
    "\n",
    "# partition data\n",
    "data_train_2000, data_test_2000 = partition_dataframe(df_2000, .8)\n",
    "\n",
    "# convert dataframe to list\n",
    "text_data_2000 = df_2000.patent_title_abstract.tolist()\n",
    "text_train_2000 = data_train_2000.patent_title_abstract.tolist()\n",
    "text_test_2000 = data_test_2000.patent_title_abstract.tolist()\n",
    "\n",
    "# TODO (Lee) - this explores direct structuring from api response without df\n",
    "text_list_2000 = []\n",
    "for i in data_2000:\n",
    "    text_list_2000.append(i['patent_title_abstract'])\n",
    "# text_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'ner']\n"
     ]
    }
   ],
   "source": [
    "# uncomment to download stop words\n",
    "# !python -m spacy download en\n",
    "stop_words = stopwords.words('/Users/lee/Documents/techniche/techniche/data/stopwords/english')\n",
    "\n",
    "# construct pipeline\n",
    "nlp = build_pipeline()\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# TODO (Lee) - pre-process documents TODO (Lee) - via text_list directly from api response\n",
    "# processed_docs_1 = process_docs(text_list)\n",
    "\n",
    "# pre-process documents TODO (Lee) - via df to list\n",
    "processed_docs_1000train = process_docs(text_train_1000)\n",
    "\n",
    "### Build corpus and dictionary\n",
    "\n",
    "# build dictionary\n",
    "id_to_word_1000train = Dictionary(processed_docs_1000train)\n",
    "\n",
    "# apply term-doc frequency (list of (token_id, token_count) tuples) to docs in corpus \n",
    "corpus_1000train = [id_to_word_1000train.doc2bow(doc) for doc in processed_docs_1000train]\n",
    "\n",
    "# view formatted corpus\n",
    "# uncomment below to view\n",
    "# formatted_corpus_1000 = [[(id_to_word[id], freq) for id, freq in text] for text in corpus_1000train]\n",
    "# formatted_corpus_1000\n",
    "# id_to_word_1000train.token2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model #1: Genism LDA model\n",
    "Model #1: implementation: Gensim LDAmodel; k_topics=5; n_docs=1000, partition = 80/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct model #1\n",
    "# TODO (Lee) - resolve deprecation warnings\n",
    "model_1 = LdaModel(corpus=corpus_1000train,\n",
    "                   id2word=id_to_word_1000train,\n",
    "                   num_topics=5, \n",
    "                   random_state=100,\n",
    "                   update_every=1,\n",
    "                   chunksize=100,\n",
    "                   passes=10,\n",
    "                   alpha='auto',\n",
    "                   per_word_topics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #1 - Explore and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore topics visually\n",
    "pyLDAvis.enable_notebook()\n",
    "viz_topics_model_1 = pyLDAvis.gensim.prepare(model_1, corpus_1000train, id_to_word_1000train)\n",
    "# viz_topics_model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keywords in n topics in corpus\n",
    "# uncomment below to view\n",
    "# pprint(model_1.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most important keywords, and the respective weight, that form topic with index 0\n",
    "# uncomment below to view\n",
    "# pprint(model_1.print_topic(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #1 - Evaluate\n",
    "As unsupervised learning task, no labels with which to evaluate the \"expected\" prediction. There is an open research agenda on various evaluation approaches (intrinsic vs extrinsic; machine vs human-interpretable, etc., task-specific)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #1 - Evaluate - Pre-process test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-process documents TODO (Lee) - via df to list\n",
    "processed_docs_1000test = process_docs(text_test_1000)\n",
    "\n",
    "# build dictionary\n",
    "id_to_word_1000test = Dictionary(processed_docs_1000test)\n",
    "\n",
    "# apply term-doc frequency (list of (token_id, token_count) tuples) to docs in corpus \n",
    "corpus_1000test = [id_to_word_1000test.doc2bow(doc) for doc in processed_docs_1000test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #1 - Evaluate - Coherence\n",
    "Calculate topic coherence for topic models.\n",
    "Implements 'CoherenceModel' coherence pipeline (segmentation, probability estimation, confirmation measure, aggregation) from Roeder et al., 2015. \"Exploring the space of topic coherence measures\", WSDM '15 Proceedings of the Eighth ACM International Conference on Web Search and Data Mining (WSDM) 2015, 399-408."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3557055926610265\n"
     ]
    }
   ],
   "source": [
    "# calculate coherence metric for train set ((n = 800 docs/1000 docs total in dataset))\n",
    "coherence_model_1train = CoherenceModel(model=model_1, \n",
    "                                        texts=processed_docs_1000train,\n",
    "                                        dictionary=id_to_word_1000train,\n",
    "                                        coherence='c_v')\n",
    "coherence_model_1train_get = coherence_model_1train.get_coherence()\n",
    "print(coherence_model_1train_get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6020573135746659\n"
     ]
    }
   ],
   "source": [
    "# calculate coherence metric for test_set (n = 200 docs/100 docs total in dataset)\n",
    "coherence_model_1test = CoherenceModel(model=model_1, \n",
    "                                       texts=processed_docs_1000test, \n",
    "                                       dictionary=id_to_word_1000test, \n",
    "                                       coherence='c_v')\n",
    "coherence_model_1test_get = coherence_model_1test.get_coherence()\n",
    "print(coherence_model_1test_get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate coherence metric for each of the n topics in the test set\n",
    "coherence_model_1_per_topic = coherence_model_1test.get_coherence_per_topic()\n",
    "# print(coherence_model_1_per_topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #1 - Evaluate - Perplexity\n",
    "Calculate perplexity metric. Metric calculates and returns per-word likelihood bound using a chunk of documents as evaluation corpus. Output calculated statistics, including the perplexity=2^(-bound), to log at INFO level. Returns the variational bound score calculated for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-7.091984311693296\n"
     ]
    }
   ],
   "source": [
    "# calculate perplexity metric for model_1 train set\n",
    "perplexity_model_1train = model_1.log_perplexity(corpus_1000train)\n",
    "print(perplexity_model_1train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-9.66161358304598\n"
     ]
    }
   ],
   "source": [
    "# calculate perplexity metric for model_1 test set\n",
    "perplexity_model_1test = model_1.log_perplexity(corpus_1000test)\n",
    "print(perplexity_model_1test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #1 - Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #1 - Predict - Pickle model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle model\n",
    "pickle.dump(model_1, open('/Users/lee/Documents/techniche/techniche/data/model_lda_1.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = pickle.load(open('/Users/lee/Documents/techniche/techniche/data/model_lda_1.pkl','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #1 - inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define example text_input #1\n",
    "text_input_1 = 'smart assistant transformer model translation'.split()\n",
    "\n",
    "# define example text_input #1\n",
    "text_input_2 = \"\"\"At the Siri International team within Apple we bring the\n",
    "Siri intelligent assistant to our customers worldwide in over 40 languages\n",
    "and dialects. Join us, and tackle some of the most challenging problems in\n",
    "natural language processing and large scale applied machine learning. You \n",
    "will build cutting edge natural language understanding technologies and \n",
    "deploy them on a global scale. Your work will advance and shape the future\n",
    "vision of our multi-lingual, multi-cultural Siri assistant, and Search \n",
    "applications used by millions across the world Key Qualifications Extensive\n",
    "track record of scientific research in NLP and Machine Learning, or similar\n",
    "experience in developing language technologies for shipping products.\n",
    "Strong coding and software engineering skills in a mainstream programming \n",
    "language, such as Python, Java, C/C++. Familiarity with NLP/ML tools and \n",
    "packages like Caffe, pyTorch, TensorFlow, Weka, scikit-learn, nltk, etc.\n",
    "Practical experience building production quality applications related to \n",
    "natural language processing and machine learning. In-depth knowledge of \n",
    "machine learning algorithms and ability to apply them in data driven natural\n",
    "language processing systems. Ability to quickly prototype ideas / solutions,\n",
    "perform critical analysis, and use creative approaches for solving complex \n",
    "problems. Attention to detail and excellent communication skills. Description\n",
    "We are looking for a highly motivated technologist with a strong background \n",
    "in Natural Language Processing and Machine Learning research. The ideal \n",
    "candidate will have a strong track record of taking research ideas to \n",
    "real-world applications. In this position you will apply your problem solving\n",
    "skills to challenges and opportunities within Siri International, which \n",
    "involves development of large-scale language technologies for many natural\n",
    "languages worldwide. The primary responsibility of this role is to conduct\n",
    "research and develop innovative machine learning, artificial intelligence \n",
    "and NLP solutions for multi-lingual conversational agents. You will have \n",
    "the opportunity to investigate cutting edge research methods that will \n",
    "improve customer experience of our products and enable our engineers to \n",
    "scale these technologies across a variety of natural languages. You will \n",
    "also provide technical leadership and experiment-driven insights for \n",
    "engineering teams on their machine learning modeling and data decisions. \n",
    "You will play a central role in defining the future technical directions \n",
    "of Siri International through quick prototyping, critical analysis and \n",
    "development of core multi-lingual NLP technologies. Education & Experience\n",
    "PhD in Machine Learning, Statistics, Computer Science, Mathematics or \n",
    "related field with specialization in natural language processing and/or \n",
    "machine learning, OR * Masters degree in a related field with a strong \n",
    "academic/industrial track record. * Hands-on research experience in an \n",
    "academic or industrial setting.\"\"\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(53, 1), (166, 1), (1455, 1), (2332, 1), (3146, 1)]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_word_1000train.doc2bow(text_input_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_input_1 = get_topics(id_to_word_1000train.doc2bow(text_input_1), model_1, k=10)\n",
    "# uncomment below to view\n",
    "# predict_input_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_input_2 = get_topics(id_to_word_1000train.doc2bow(text_input_2), model_1, k=10)\n",
    "# uncomment below to view\n",
    "# predict_input_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model #2: Genism LDAMallet model\n",
    "Model #2: implementation: Gensim LDAMallet wrapper around LDA Mallet model; \n",
    "          k_topics=5; \n",
    "          n_docs=1000; \n",
    "          partition = 80/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to download Mallet topic model\n",
    "# !wget http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip\n",
    "# update path\n",
    "path_mallet = '/Users/lee/Documents/techniche/techniche/data/mallet-2.0.8/bin/mallet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct model #2\n",
    "# TODO (Lee) - resolve deprecation warnings\n",
    "model_2 = gensim.models.wrappers.LdaMallet(path_mallet, \n",
    "                                           corpus=corpus_1000train, \n",
    "                                           num_topics=5, \n",
    "                                           id2word=id_to_word_1000train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #2 - Explore and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Lee) - visualization with LDA Mallet\n",
    "# explore topics visually\n",
    "# pyLDAvis.enable_notebook()\n",
    "# viz_topics_model_2 = pyLDAvis.prepare(model_2, corpus_1000train, id_to_word_1000train)\n",
    "# viz_topics_model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keywords in n topics in corpus\n",
    "# uncomment below to view\n",
    "# pprint(model_2.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most important keywords, and the respective weight, that form topic with index 0\n",
    "# uncomment below to view\n",
    "# pprint(model_2.print_topic(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #2- Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #2 - Evaluate - Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35959575939439825\n"
     ]
    }
   ],
   "source": [
    "# calculate coherence metric for train set ((n = 800 docs/1000 docs total in dataset))\n",
    "coherence_model_2train = CoherenceModel(model=model_2, \n",
    "                                        texts=processed_docs_1000train,\n",
    "                                        dictionary=id_to_word_1000train,\n",
    "                                        coherence='c_v')\n",
    "coherence_model_2train_get = coherence_model_2train.get_coherence()\n",
    "print(coherence_model_2train_get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5969635120006694\n"
     ]
    }
   ],
   "source": [
    "# calculate coherence metric for test_set (n = 200 docs/100 docs total in dataset)\n",
    "coherence_model_2test = CoherenceModel(model=model_2, \n",
    "                                       texts=processed_docs_1000test, \n",
    "                                       dictionary=id_to_word_1000test, \n",
    "                                       coherence='c_v')\n",
    "coherence_model_2test_get = coherence_model_2test.get_coherence()\n",
    "print(coherence_model_2test_get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate coherence metric for each of the n topics in the test set\n",
    "coherence_model_2_per_topic = coherence_model_2test.get_coherence_per_topic()\n",
    "# print(coherence_model_2_per_topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model #3: Gensim LDA model\n",
    "Model #1: implementation: Gensim LDAmodel; k_topics=10; n_docs=1000, partition = 80/20\n",
    "This model increases the k_topics from 5 to 10, relative to model #1 above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct model #3\n",
    "# TODO (Lee) - resolve deprecation warnings\n",
    "model_3 = LdaModel(corpus=corpus_1000train,\n",
    "                   id2word=id_to_word_1000train,\n",
    "                   num_topics=10, \n",
    "                   random_state=100,\n",
    "                   update_every=1,\n",
    "                   chunksize=100,\n",
    "                   passes=10,\n",
    "                   alpha='auto',\n",
    "                   per_word_topics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #3 - Explore and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore topics visually\n",
    "pyLDAvis.enable_notebook()\n",
    "viz_topics_model_3 = pyLDAvis.gensim.prepare(model_3, corpus_1000train, id_to_word_1000train)\n",
    "# viz_topics_model_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keywords in n topics in corpus\n",
    "# uncomment below to view\n",
    "# pprint(model_3.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most important keywords, and the respective weight, that form topic with index 0\n",
    "# uncomment below to view\n",
    "# pprint(model_3.print_topic(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #3 - Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #3 - Evaluate - Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3628245805478861\n"
     ]
    }
   ],
   "source": [
    "# calculate coherence metric for train set ((n = 800 docs/1000 docs total in dataset))\n",
    "coherence_model_3train = CoherenceModel(model=model_3, \n",
    "                                        texts=processed_docs_1000train,\n",
    "                                        dictionary=id_to_word_1000train,\n",
    "                                        coherence='c_v')\n",
    "coherence_model_3train_get = coherence_model_3train.get_coherence()\n",
    "print(coherence_model_3train_get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6081902191345155\n"
     ]
    }
   ],
   "source": [
    "# calculate coherence metric for test_set (n = 200 docs/100 docs total in dataset)\n",
    "coherence_model_3test = CoherenceModel(model=model_3, \n",
    "                                       texts=processed_docs_1000test, \n",
    "                                       dictionary=id_to_word_1000test, \n",
    "                                       coherence='c_v')\n",
    "coherence_model_3test_get = coherence_model_3test.get_coherence()\n",
    "print(coherence_model_3test_get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate coherence metric for each of the n topics in the test set\n",
    "coherence_model_3_per_topic = coherence_model_3test.get_coherence_per_topic()\n",
    "# print(coherence_model_1_per_topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #3 - Evaluate - Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-7.0676967060500635\n"
     ]
    }
   ],
   "source": [
    "# calculate perplexity metric for model_1 train set\n",
    "perplexity_model_3train = model_3.log_perplexity(corpus_1000train)\n",
    "print(perplexity_model_3train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-9.945866263106378\n"
     ]
    }
   ],
   "source": [
    "# calculate perplexity metric for model_1 test set\n",
    "perplexity_model_3test = model_3.log_perplexity(corpus_1000test)\n",
    "print(perplexity_model_3test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #3 - Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #3 - Predict - Pickle model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle model #3\n",
    "pickle.dump(model_3, open('/Users/lee/Documents/techniche/techniche/data/model_lda_1.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = pickle.load(open('/Users/lee/Documents/techniche/techniche/data/model_lda_1.pkl','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #3 - inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_input_1 = get_topics(id_to_word_1000train.doc2bow(text_input_1), model_3, k=10)\n",
    "# uncomment below to view\n",
    "# predict_input_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_input_2 = get_topics(id_to_word_1000train.doc2bow(text_input_2), model_3, k=10)\n",
    "# uncomment below to view\n",
    "# predict_input_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model #4: Gensim LDA model\n",
    "Model #4: implementation: Gensim LDAmodel; k_topics=15; n_docs=1000, partition = 80/20\n",
    "This model increases the k_topics to 15, relative to model #1 and model #3 above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct model #4\n",
    "# TODO (Lee) - resolve deprecation warnings\n",
    "model_4 = LdaModel(corpus=corpus_1000train,\n",
    "                   id2word=id_to_word_1000train,\n",
    "                   num_topics=15, \n",
    "                   random_state=100,\n",
    "                   update_every=1,\n",
    "                   chunksize=100,\n",
    "                   passes=10,\n",
    "                   alpha='auto',\n",
    "                   per_word_topics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #4 - Explore and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore topics visually\n",
    "pyLDAvis.enable_notebook()\n",
    "viz_topics_model_4 = pyLDAvis.gensim.prepare(model_4, corpus_1000train, id_to_word_1000train)\n",
    "# viz_topics_model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keywords in n topics in corpus\n",
    "# uncomment below to view\n",
    "# pprint(model_4.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most important keywords, and the respective weight, that form topic with index 0\n",
    "# uncomment below to view\n",
    "# pprint(model_4.print_topic(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #4 - Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #4 - Evaluate - Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37342906623443217\n"
     ]
    }
   ],
   "source": [
    "# calculate coherence metric for train set ((n = 800 docs/1000 docs total in dataset))\n",
    "coherence_model_4train = CoherenceModel(model=model_4, \n",
    "                                        texts=processed_docs_1000train,\n",
    "                                        dictionary=id_to_word_1000train,\n",
    "                                        coherence='c_v')\n",
    "coherence_model_4train_get = coherence_model_4train.get_coherence()\n",
    "print(coherence_model_4train_get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "3125",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-151-f18f89904c74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m                                        \u001b[0mdictionary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mid_to_word_1000test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                        coherence='c_v')\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcoherence_model_4test_get\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoherence_model_4test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_coherence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoherence_model_4test_get\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim/models/coherencemodel.py\u001b[0m in \u001b[0;36mget_coherence\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_coherence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0;34m\"\"\"Return coherence value based on pipeline parameters.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mconfirmed_measures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_coherence_per_topic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregate_measures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfirmed_measures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim/models/coherencemodel.py\u001b[0m in \u001b[0;36mget_coherence_per_topic\u001b[0;34m(self, segmented_topics, with_std, with_support)\u001b[0m\n\u001b[1;32m    411\u001b[0m             \u001b[0msegmented_topics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeasure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accumulator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimate_probabilities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegmented_topics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwith_std\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwith_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_support\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwith_support\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim/models/coherencemodel.py\u001b[0m in \u001b[0;36mestimate_probabilities\u001b[0;34m(self, segmented_topics)\u001b[0m\n\u001b[1;32m    401\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeyed_vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accumulator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeasure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accumulator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim/topic_coherence/probability_estimation.py\u001b[0m in \u001b[0;36mp_boolean_sliding_window\u001b[0;34m(texts, segmented_topics, dictionary, window_size, processes)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0maccumulator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordOccurrenceAccumulator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0maccumulator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallelWordOccurrenceAccumulator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocesses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"using %s to estimate probabilities from sliding windows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccumulator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0maccumulator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim/topic_coherence/text_analysis.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, processes, *args, **kwargs)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocesses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParallelWordOccurrenceAccumulator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprocesses\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m             raise ValueError(\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim/topic_coherence/text_analysis.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, relevant_ids, dictionary)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \"\"\"\n\u001b[0;32m--> 280\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWindowedTextsAnalyzer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelevant_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_none_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vocab_size\u001b[0m  \u001b[0;31m# see _iter_texts for use of none token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim/topic_coherence/text_analysis.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, relevant_ids, dictionary)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \"\"\"\n\u001b[1;32m    184\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUsesDictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelevant_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelevant_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ids_to_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelevant_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken2id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken2id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim/topic_coherence/text_analysis.py\u001b[0m in \u001b[0;36m_ids_to_words\u001b[0;34m(ids, dictionary)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mtop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2token\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mtop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 3125"
     ]
    }
   ],
   "source": [
    "# calculate coherence metric for test_set (n = 200 docs/100 docs total in dataset)\n",
    "coherence_model_4test = CoherenceModel(model=model_4, \n",
    "                                       texts=processed_docs_1000test, \n",
    "                                       dictionary=id_to_word_1000test, \n",
    "                                       coherence='c_v')\n",
    "coherence_model_4test_get = coherence_model_4test.get_coherence()\n",
    "print(coherence_model_4test_get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "4130",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-78fc0f067656>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# calculate coherence metric for each of the n topics in the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcoherence_model_4_per_topic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoherence_model_3test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_coherence_per_topic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# print(coherence_model_1_per_topic)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim/models/coherencemodel.py\u001b[0m in \u001b[0;36mget_coherence_per_topic\u001b[0;34m(self, segmented_topics, with_std, with_support)\u001b[0m\n\u001b[1;32m    411\u001b[0m             \u001b[0msegmented_topics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeasure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accumulator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimate_probabilities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegmented_topics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwith_std\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwith_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_support\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwith_support\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim/models/coherencemodel.py\u001b[0m in \u001b[0;36mestimate_probabilities\u001b[0;34m(self, segmented_topics)\u001b[0m\n\u001b[1;32m    401\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeyed_vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accumulator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeasure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accumulator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim/topic_coherence/probability_estimation.py\u001b[0m in \u001b[0;36mp_boolean_sliding_window\u001b[0;34m(texts, segmented_topics, dictionary, window_size, processes)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0maccumulator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordOccurrenceAccumulator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0maccumulator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallelWordOccurrenceAccumulator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocesses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"using %s to estimate probabilities from sliding windows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccumulator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0maccumulator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim/topic_coherence/text_analysis.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, processes, *args, **kwargs)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocesses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParallelWordOccurrenceAccumulator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprocesses\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m             raise ValueError(\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim/topic_coherence/text_analysis.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, relevant_ids, dictionary)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \"\"\"\n\u001b[0;32m--> 280\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWindowedTextsAnalyzer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelevant_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_none_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vocab_size\u001b[0m  \u001b[0;31m# see _iter_texts for use of none token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim/topic_coherence/text_analysis.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, relevant_ids, dictionary)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \"\"\"\n\u001b[1;32m    184\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUsesDictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelevant_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelevant_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ids_to_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelevant_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken2id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken2id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim/topic_coherence/text_analysis.py\u001b[0m in \u001b[0;36m_ids_to_words\u001b[0;34m(ids, dictionary)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mtop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2token\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mtop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 4130"
     ]
    }
   ],
   "source": [
    "# calculate coherence metric for each of the n topics in the test set\n",
    "coherence_model_4_per_topic = coherence_model_3test.get_coherence_per_topic()\n",
    "# print(coherence_model_1_per_topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #4 - Evaluate - Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-7.103390272359293\n"
     ]
    }
   ],
   "source": [
    "# calculate perplexity metric for model_1 train set\n",
    "perplexity_model_3train = model_3.log_perplexity(corpus_1000train)\n",
    "print(perplexity_model_3train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-10.324662145126718\n"
     ]
    }
   ],
   "source": [
    "# calculate perplexity metric for model_1 test set\n",
    "perplexity_model_3test = model_3.log_perplexity(corpus_1000test)\n",
    "print(perplexity_model_3test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #4 - Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #3 - Predict - Pickle model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle model\n",
    "pickle.dump(model_4, open('/Users/lee/Documents/techniche/techniche/data/model_lda_1.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4 = pickle.load(open('/Users/lee/Documents/techniche/techniche/data/model_lda_1.pkl','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #4 - inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_input_1 = get_topics(id_to_word_1000train.doc2bow(text_input_1), model_4, k=10)\n",
    "# uncomment below to view\n",
    "# predict_input_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_input_2 = get_topics(id_to_word_1000train.doc2bow(text_input_2), model_4, k=10)\n",
    "# uncomment below to view\n",
    "# predict_input_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model #5: Author-topic model\n",
    "Model #4: implementation: Gensim AuthorTopicModel; k_topics=15; n_docs=1000, partition = 80/20\n",
    "This model increases the k_topics to 15, relative to model #1 and model #3 above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct inventor-to-doc mapping as df from nested inventors column in json api response\n",
    "# df_inventors = json_normalize(data, record_path=['inventors'], meta=['patent_number', 'patent_date'])\n",
    "# df_inventors = df_inventors[['inventor_id', 'patent_number', 'patent_date']]\n",
    "# df_inventors.sort_values(by=['patent_date'])\n",
    "# df_inventors.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick visual index to patent number mapping\n",
    "# for i in data:\n",
    "#     print(data.index(i), i['patent_number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-7b9b3128563e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# TODO (Lee) review fix to pat_inv_map, in which \"patent\" in mapping is idx of pat, not pat_number from api\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpat2inv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpat_inv_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO (Lee) review fix to pat_inv_map, in which \"patent\" in mapping is idx of pat, not pat_number from api\n",
    "pat2inv = pat_inv_map(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct author-topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct author-topic model\n",
    "model_at = AuthorTopicModel(corpus=corpus,\n",
    "                         doc2author=pat2inv,\n",
    "                         id2word=id_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct vectors for authors\n",
    "author_vecs = [model_at.get_author_topics(author) for author in model_at.id2author.values()]\n",
    "author_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve topic distribution for author using use model[name] syntax\n",
    "# each topic has a probability of being expressed given the particular author, \n",
    "# but only the ones above a certain threshold are displayed\n",
    "\n",
    "model_at['7788103-1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def show_author(name):\n",
    "#     print('\\n%s' % name)\n",
    "#     print('Docs:', model.author2doc[name])\n",
    "#     print('Topics:')\n",
    "#     pprint([(topic_labels[topic[0]], topic[1]) for topic in model[name]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build mapping from inventor to patent\n",
    "inv2pat = gensim.models.atmodel.construct_author2doc(pat2inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #X - Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction functions that take input of new text string, and predict topic distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Appendix - Model #1 - Evaluate - holdout set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Lee) - evaluate on 1k documents **not** used in LDA training\n",
    "doc_stream = (tokens for _, tokens in iter_wiki('./data/simplewiki-20140623-pages-articles.xml.bz2'))  # generator\n",
    "test_docs = list(itertools.islice(doc_stream, 8000, 9000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Appendix - Model #1 - Evaluate - Doc split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Lee) - split each document into two parts, and check that 1) topics of the first half are similar to \n",
    "topics of the second 2) halves of different documents are mostly dissimilar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Lee)\n",
    "def intra_inter(model, test_docs, num_pairs=10000):\n",
    "    # split each test document into two halves and compute topics for each half\n",
    "    part1 = [model[id2word.doc2bow(tokens[: len(tokens) / 2])] for tokens in test_docs]\n",
    "    part2 = [model[id2word.doc2bow(tokens[len(tokens) / 2 :])] for tokens in test_docs]\n",
    "    \n",
    "    # print computed similarities (uses cossim)\n",
    "    print(\"average cosine similarity between corresponding parts (higher is better):\")\n",
    "    print(np.mean([gensim.matutils.cossim(p1, p2) for p1, p2 in zip(part1, part2)]))\n",
    "\n",
    "    random_pairs = np.random.randint(0, len(test_docs), size=(num_pairs, 2))\n",
    "    print(\"average cosine similarity between 10,000 random parts (lower is better):\")    \n",
    "    print(np.mean([gensim.matutils.cossim(part1[i[0]], part2[i[1]]) for i in random_pairs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Lee)\n",
    "print(\"LDA results:\")\n",
    "intra_inter(lda_model, test_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Appendix - Model #1 - Evaluate - Log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Appendix - Model #1 - Evaluate - Alternate unimplemented workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Lee)\n",
    "# def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "#     \"\"\"\n",
    "#     Compute c_v coherence for various number of topics\n",
    "\n",
    "#     Parameters:\n",
    "#     ----------\n",
    "#     dictionary : Gensim dictionary\n",
    "#     corpus : Gensim corpus\n",
    "#     texts : List of input texts\n",
    "#     limit : Max num of topics\n",
    "\n",
    "#     Returns:\n",
    "#     -------\n",
    "#     model_list : List of LDA topic models\n",
    "#     coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "#     \"\"\"\n",
    "#     coherence_values = []\n",
    "#     model_list = []\n",
    "#     for num_topics in range(start, limit, step):\n",
    "#         model = gensim.models.wrappers.LdaMallet(path_mallet, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "#         model_list.append(model)\n",
    "#         coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "#         coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "#     return model_list, coherence_values\n",
    "\n",
    "# model_list, coherence_values = compute_coherence_values(dictionary=id_to_word, corpus=corpus, texts=data, start=2, limit=40, step=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Appendix - Model 1 - Inference - Alternate workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `get_document_topics()` returns topic probability distribution for given document\n",
    "# topic_dist_675_a = model_lda.get_document_topics(corpus[50])\n",
    "# pprint(sorted(topic_dist_50_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topicid = 3\n",
    "# model_lda.get_topic_terms(topicid, topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_train[doc_id]\n",
    "# doc_id = 675\n",
    "# topic_dist_675_b = sorted(get_topics(corpus[doc_id], k=10)), text_train[doc_id]\n",
    "# pprint(topic_dist_675_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Gensim example - Alternate predict workflow - Create a new corpus, made of previously unseen documents.\n",
    "# other_texts = [\n",
    "#      ['computer', 'time', 'graph'],\n",
    "#      ['survey', 'response', 'eps'],\n",
    "#      ['human', 'system', 'computer']\n",
    "#  ]\n",
    "# other_corpus = [common_dictionary.doc2bow(text) for text in other_texts]\n",
    "# unseen_doc = other_corpus[0]\n",
    "# vector = lda[unseen_doc]  # get topic probability distribution for a document\n",
    "# Update the model by incrementally training on the new corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print keywords in n topics\n",
    "sorted(model_lda.show_topics(), key=lambda x: x[1])\n",
    "\n",
    "# print keywords in n topics\n",
    "sorted(model_lda.print_topics(), key=lambda x: x[1])\n",
    "\n",
    "# print keywords in n topics\n",
    "sorted(model_l.print_topics(), key=lambda x: x[1])\n",
    "\n",
    "# print keywords in n topics\n",
    "sorted(model_1.print_topics(), key=lambda x: x[0])\n",
    "\n",
    "# show_topic() returns n most important/relevant words, and their weights, that comprise given topic\n",
    "pprint(model_1.show_topic(1, topn=10))\n",
    "\n",
    "pprint(model_1.show_topics(num_topics=5, num_words=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Appendix - Model #2 - Evaluate - Perplexity\n",
    "No implementation of log_perplexity method for LDAMallet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
