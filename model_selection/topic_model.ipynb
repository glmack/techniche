{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from test_model import (get_patent_fields_list, get_ml_patents, \n",
    "                        create_title_abstract_col,trim_data, \n",
    "                        structure_dataframe, partition_dataframe, \n",
    "                        build_pipeline, process_docs, pat_inv_map, get_topics)\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.corpora import Dictionary, mmcorpus\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models import AuthorTopicModel\n",
    "from gensim.test.utils import common_dictionary, datapath, temporary_file\n",
    "from smart_open import smart_open\n",
    "\n",
    "import spacy\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, punkt, RegexpTokenizer, wordpunct_tokenize\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
    "\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "import calendar\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress gensim deprecation warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acquire data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "# supress jupyter notebook output of json response\n",
    "\n",
    "# acquire ML patent dataset from Patentsview API query\n",
    "raw_data_1000 = get_ml_patents(pats_per_page=1000)\n",
    "raw_data_2000 = get_ml_patents(pats_per_page=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update path below with user's local path to save pickled raw data\n",
    "path_raw_data_1000 = '/Users/lee/Documents/techniche/techniche/data/raw_data_1000'\n",
    "path_raw_data_2000 = '/Users/lee/Documents/techniche/techniche/data/raw_data_2000'\n",
    "\n",
    "# pickle raw_data_1000 and raw_data_2000\n",
    "with open(path_raw_data_1000, 'wb') as f:\n",
    "    pickle.dump(raw_data_1000, f)   \n",
    "with open(path_raw_data_2000, 'wb') as f:\n",
    "    pickle.dump(raw_data_2000, f)\n",
    "\n",
    "# un-comment to de-serialize pickled list of dictionaries\n",
    "# with open(path_raw_data_1000, 'rb') as f:\n",
    "#     raw_data_1000 = pickle.load(f) \n",
    "# with open(path_raw_data_2000, 'rb') as f:\n",
    "#     raw_data_2000 = pickle.load(f)\n",
    "\n",
    "# un-comment to view first dictionary in raw_data_1000\n",
    "# raw_data_1000[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Acquire data - Structure data - 1000 patent documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define keys as criteria to subset dataset\n",
    "retained_keys = ['patent_number', 'patent_date', 'patent_title',\n",
    "                 'patent_abstract', 'inventors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset JSON dict dataset of full api response by keys\n",
    "data_1000 = trim_data(data=raw_data_1000, keys=retained_keys)\n",
    "\n",
    "# create item in dict by concatenating patent_title and patent_abstract\n",
    "data_1000 = create_title_abstract_col(data=data_1000)\n",
    "\n",
    "# convert dataframe from subsetted dict, organize columns, sort by patent_date\n",
    "df_1000 = structure_dataframe(data=data_1000)\n",
    "\n",
    "# partition df_1000 into train and test dataframes\n",
    "data_train_1000, data_test_1000 = partition_dataframe(df_1000, .8)\n",
    "\n",
    "# convert dataframes (full, train, test) to list format required by model\n",
    "text_data_1000 = df_1000.patent_title_abstract.tolist()\n",
    "text_train_1000 = data_train_1000.patent_title_abstract.tolist()\n",
    "text_test_1000 = data_test_1000.patent_title_abstract.tolist()\n",
    "\n",
    "# convert text target in JSON response to list w/o dataframe step\n",
    "text_list_1000 = []\n",
    "for i in data_1000:\n",
    "    text_list_1000.append(i['patent_title_abstract'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Acquire data - Structure data - 2000 patent documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset dataset of full api response by keys\n",
    "data_2000 = trim_data(data=raw_data_2000, keys=retained_keys)\n",
    "\n",
    "# create item by concatenating patent_title and patent_abstract\n",
    "data_2000 = create_title_abstract_col(data=data_2000)\n",
    "\n",
    "# create dataframe, organize columns and sort by patent_date\n",
    "df_2000 = structure_dataframe(data=data_2000)\n",
    "\n",
    "# partition dataframe\n",
    "data_train_2000, data_test_2000 = partition_dataframe(df_2000, .8)\n",
    "\n",
    "# convert dataframe to list format required by model\n",
    "text_data_2000 = df_2000.patent_title_abstract.tolist()\n",
    "text_train_2000 = data_train_2000.patent_title_abstract.tolist()\n",
    "text_test_2000 = data_test_2000.patent_title_abstract.tolist()\n",
    "\n",
    "# convert text target in JSON response to list w/o dataframe step\n",
    "text_list_2000 = []\n",
    "for i in data_2000:\n",
    "    text_list_2000.append(i['patent_title_abstract'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to download standard stop words from Spacy\n",
    "# !python -m spacy download en\n",
    "\n",
    "# update path with location to save stopwords\n",
    "path_stopwords = '/Users/lee/Documents/techniche/techniche/data/stopwords/english'\n",
    "stop_words = stopwords.words(path_stopwords)\n",
    "\n",
    "# create text pre-processing pipeline to tokenize, clean and lower text\n",
    "nlp = build_pipeline()\n",
    "\n",
    "# pre-process documents via json-to-df-to-list workflow above\n",
    "processed_docs_1000train = process_docs(text_train_1000)\n",
    "\n",
    "# optional pre-process documents via json-to-list workflow above\n",
    "# processed_docs_1 = process_docs(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Build corpus and dictionary\n",
    "\n",
    "# build dictionary\n",
    "id_to_word_1000train = Dictionary(processed_docs_1000train)\n",
    "\n",
    "# update path with location to save pickled dictionary\n",
    "path_pickle_id_to_word = '/Users/lee/Documents/techniche/techniche/data/id_to_word_1000train.pkl'\n",
    "\n",
    "# pickle dictionary\n",
    "pickle.dump(id_to_word_1000train, open(path_pickle_id_to_word,'wb'))\n",
    "\n",
    "# apply term-doc freq (list of (token_id, token_count) tuples) to docs\n",
    "corpus_1000train = [id_to_word_1000train.doc2bow(doc) for doc in processed_docs_1000train]\n",
    "\n",
    "# uncomment below to create/view formatted corpus\n",
    "# formatted_corpus_1000 = [[(id_to_word[id], freq) for id, freq in text] for text in corpus_1000train]\n",
    "# formatted_corpus_1000\n",
    "# id_to_word_1000train.token2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model #1: Genism LDA model\n",
    "Model #1: implementation: Gensim LDAmodel; k_topics=5; n_docs=1000, partition = 80/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct model #1\n",
    "model_1 = LdaModel(corpus=corpus_1000train,\n",
    "                   id2word=id_to_word_1000train,\n",
    "                   num_topics=5, \n",
    "                   random_state=100,\n",
    "                   update_every=1,\n",
    "                   chunksize=100,\n",
    "                   passes=10,\n",
    "                   alpha='auto',\n",
    "                   per_word_topics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #1 - Explore and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el5181126134848486950750417\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el5181126134848486950750417_data = {\"mdsDat\": {\"x\": [-0.11770723309172906, -0.16839328590097832, -0.11776959290892179, 0.151145325880672, 0.2527247860209575], \"y\": [0.13636072289965517, -0.08663527563829541, -0.09420263384855573, 0.20548296494481097, -0.1610057783576151], \"topics\": [1, 2, 3, 4, 5], \"cluster\": [1, 1, 1, 1, 1], \"Freq\": [40.598026275634766, 25.10063362121582, 18.438392639160156, 8.992602348327637, 6.8703508377075195]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\"], \"Freq\": [799.0, 797.0, 824.0, 709.0, 550.0, 325.0, 232.0, 1158.0, 706.0, 293.0, 181.0, 139.0, 363.0, 281.0, 92.0, 302.0, 222.0, 457.0, 101.0, 135.0, 157.0, 130.0, 563.0, 762.0, 125.0, 124.0, 130.0, 74.0, 124.0, 99.0, 292.4393310546875, 130.64767456054688, 546.3734130859375, 106.41109466552734, 100.79271697998047, 80.62344360351562, 68.93416595458984, 67.43342590332031, 61.58974075317383, 63.35391616821289, 58.22960662841797, 54.96826171875, 65.28838348388672, 159.43296813964844, 46.66471481323242, 46.140438079833984, 45.43117904663086, 45.17485427856445, 42.86412048339844, 42.10296630859375, 42.51827621459961, 40.1005859375, 42.35511016845703, 40.259464263916016, 36.011531829833984, 34.12744903564453, 34.79706573486328, 33.30453109741211, 64.05870056152344, 32.14226150512695, 85.32707214355469, 664.9569702148438, 153.10055541992188, 650.31591796875, 95.9791030883789, 735.2859497070312, 277.56842041015625, 410.2611999511719, 279.56884765625, 68.76155090332031, 140.78086853027344, 695.1178588867188, 307.3055114746094, 298.1615905761719, 143.736083984375, 145.9380645751953, 112.71930694580078, 541.5913696289062, 232.93563842773438, 240.66299438476562, 259.6750183105469, 117.81233978271484, 119.87728881835938, 121.36483001708984, 312.11083984375, 414.1456298828125, 226.31944274902344, 379.35552978515625, 443.6905212402344, 263.7581481933594, 187.88433837890625, 208.76795959472656, 129.94729614257812, 131.4072723388672, 325.0648193359375, 181.0117645263672, 134.90499877929688, 129.85145568847656, 124.57611083984375, 124.16593933105469, 89.25699615478516, 66.2757568359375, 61.61550521850586, 61.10020446777344, 55.39395523071289, 122.3685302734375, 54.114315032958984, 54.30664825439453, 53.597591400146484, 44.07364273071289, 48.49138641357422, 41.73349380493164, 40.685157775878906, 41.67316818237305, 41.421382904052734, 40.18552780151367, 42.671058654785156, 34.19185256958008, 33.93014144897461, 30.797624588012695, 30.821435928344727, 29.706783294677734, 28.211376190185547, 26.483261108398438, 67.77545166015625, 54.65810012817383, 234.2135467529297, 90.94464874267578, 39.51184844970703, 115.30364990234375, 418.2864074707031, 405.9560852050781, 54.90632629394531, 95.3998794555664, 374.8255920410156, 307.9337158203125, 65.40999603271484, 171.03135681152344, 182.0361785888672, 95.00880432128906, 165.4403839111328, 136.68711853027344, 200.7393035888672, 182.27218627929688, 87.17066192626953, 69.63888549804688, 114.67430877685547, 88.17643737792969, 108.60488891601562, 78.13509368896484, 69.46829986572266, 86.41603088378906, 68.9332046508789, 54.14048767089844, 53.28093338012695, 50.44268798828125, 46.7485237121582, 46.617088317871094, 45.84192657470703, 37.783782958984375, 32.95903396606445, 32.493934631347656, 31.522979736328125, 31.875469207763672, 30.71367835998535, 32.568565368652344, 29.369239807128906, 28.580913543701172, 27.85899543762207, 27.357189178466797, 26.97983169555664, 27.176477432250977, 26.596071243286133, 25.81880760192871, 25.137388229370117, 23.041229248046875, 23.157161712646484, 22.746089935302734, 22.152420043945312, 21.638212203979492, 21.605648040771484, 65.92578125, 22.0595760345459, 71.8322982788086, 44.10574722290039, 75.63623809814453, 38.31934356689453, 44.205196380615234, 80.09925842285156, 100.87207794189453, 44.78899002075195, 74.64449310302734, 50.35666275024414, 135.5560760498047, 358.3294372558594, 353.92852783203125, 60.29142379760742, 339.4925842285156, 64.26445770263672, 68.6981430053711, 62.924190521240234, 99.0795669555664, 60.672271728515625, 64.93340301513672, 103.04541778564453, 87.77204132080078, 136.4261016845703, 82.59764862060547, 108.07887268066406, 62.51441955566406, 70.2145767211914, 62.280155181884766, 50.73605728149414, 91.5395278930664, 61.23988723754883, 50.199951171875, 45.82963943481445, 37.38621520996094, 33.927886962890625, 27.476856231689453, 27.488155364990234, 24.789764404296875, 22.413387298583984, 21.214828491210938, 22.897127151489258, 17.443084716796875, 17.643754959106445, 16.554271697998047, 16.60014533996582, 27.850337982177734, 16.08636474609375, 17.371572494506836, 15.47469425201416, 15.626408576965332, 15.16201400756836, 15.289974212646484, 14.959409713745117, 15.45319652557373, 15.06574535369873, 14.22761344909668, 13.9162015914917, 15.085051536560059, 13.35142707824707, 19.57135581970215, 62.96120071411133, 34.037532806396484, 123.86177062988281, 25.11783790588379, 76.70743560791016, 27.98218536376953, 22.94846534729004, 19.875473022460938, 44.06968307495117, 51.83449172973633, 59.6546745300293, 95.88968658447266, 39.706783294677734, 71.1257553100586, 80.90494537353516, 36.83649826049805, 28.350370407104492, 41.24287796020508, 30.577682495117188, 31.997404098510742, 28.13714599609375, 23.97128677368164, 51.352073669433594, 36.25886154174805, 35.64845275878906, 24.75147819519043, 24.666751861572266, 23.5001163482666, 22.152915954589844, 21.836854934692383, 20.199838638305664, 20.19391441345215, 16.863065719604492, 16.125286102294922, 16.528400421142578, 15.224686622619629, 14.398009300231934, 14.26760482788086, 13.88443660736084, 13.771919250488281, 12.906484603881836, 13.049590110778809, 11.837665557861328, 11.51495361328125, 11.841645240783691, 11.086384773254395, 10.531983375549316, 11.655560493469238, 10.39559268951416, 10.069411277770996, 9.996126174926758, 9.886890411376953, 42.29402160644531, 26.4981632232666, 12.659483909606934, 20.578710556030273, 28.632646560668945, 16.342754364013672, 18.448211669921875, 33.6206169128418, 50.77724075317383, 30.387237548828125, 19.517107009887695, 23.25482940673828, 17.075157165527344, 21.578794479370117, 23.479209899902344, 22.796241760253906, 32.779666900634766, 44.05073547363281, 35.630615234375, 23.899574279785156, 21.505218505859375, 20.497953414916992, 19.578872680664062, 19.436676025390625], \"Term\": [\"machine\", \"learning\", \"language\", \"natural\", \"first\", \"image\", \"query\", \"data\", \"user\", \"text\", \"security\", \"search\", \"information\", \"network\", \"question\", \"second\", \"training\", \"processing\", \"message\", \"platform\", \"document\", \"images\", \"method\", \"may\", \"vehicle\", \"sensor\", \"semantic\", \"terms\", \"object\", \"form\", \"text\", \"service\", \"first\", \"electronic\", \"receiving\", \"request\", \"test\", \"mobile\", \"node\", \"graph\", \"attribute\", \"malicious\", \"representation\", \"content\", \"nodes\", \"notification\", \"instructions\", \"metadata\", \"reminder\", \"structured\", \"map\", \"textual\", \"independent\", \"probability\", \"member\", \"case\", \"list\", \"file\", \"components\", \"services\", \"comprising\", \"natural\", \"response\", \"user\", \"context\", \"language\", \"second\", \"processing\", \"input\", \"respective\", \"received\", \"one\", \"information\", \"set\", \"generating\", \"interface\", \"identifying\", \"may\", \"associated\", \"least\", \"plurality\", \"identified\", \"corresponding\", \"generate\", \"device\", \"based\", \"computer\", \"system\", \"data\", \"method\", \"includes\", \"using\", \"computing\", \"include\", \"image\", \"security\", \"platform\", \"images\", \"vehicle\", \"sensor\", \"vision\", \"anomalies\", \"threats\", \"camera\", \"signals\", \"object\", \"traffic\", \"performance\", \"risk\", \"detected\", \"sensors\", \"virtual\", \"anomaly\", \"monitoring\", \"online\", \"algorithms\", \"previously\", \"behavior\", \"captured\", \"depth\", \"employs\", \"autonomous\", \"dimensional\", \"samples\", \"quality\", \"neural\", \"network\", \"detection\", \"known\", \"features\", \"machine\", \"learning\", \"detect\", \"feature\", \"data\", \"system\", \"algorithm\", \"model\", \"using\", \"techniques\", \"device\", \"computer\", \"based\", \"may\", \"time\", \"multi\", \"method\", \"systems\", \"one\", \"include\", \"training\", \"command\", \"code\", \"motor\", \"current\", \"flow\", \"condition\", \"tool\", \"individual\", \"token\", \"correction\", \"resources\", \"events\", \"tokens\", \"predictive\", \"automation\", \"examples\", \"optimization\", \"adaptive\", \"observation\", \"translated\", \"cad\", \"person\", \"electric\", \"linguistic\", \"hvac\", \"patient\", \"accordance\", \"learns\", \"machining\", \"adjustment\", \"unit\", \"axis\", \"domain\", \"new\", \"digital\", \"cloud\", \"controller\", \"state\", \"apparatus\", \"prediction\", \"server\", \"characteristics\", \"training\", \"learning\", \"machine\", \"specific\", \"data\", \"recognition\", \"value\", \"engine\", \"methods\", \"models\", \"process\", \"model\", \"systems\", \"system\", \"includes\", \"based\", \"including\", \"method\", \"plurality\", \"one\", \"question\", \"concept\", \"term\", \"answer\", \"answers\", \"concepts\", \"trust\", \"structures\", \"ml\", \"questions\", \"account\", \"dictionary\", \"affinity\", \"url\", \"statement\", \"transaction\", \"ontology\", \"analogical\", \"defining\", \"ambiguous\", \"free\", \"clauses\", \"simulation\", \"answering\", \"procedure\", \"ecg\", \"index\", \"statements\", \"possible\", \"noc\", \"driving\", \"terms\", \"knowledge\", \"query\", \"rule\", \"search\", \"approach\", \"sequences\", \"searching\", \"form\", \"document\", \"analysis\", \"based\", \"semantic\", \"method\", \"language\", \"determining\", \"candidate\", \"natural\", \"least\", \"system\", \"includes\", \"set\", \"parameter\", \"messages\", \"messaging\", \"entries\", \"customer\", \"x\", \"log\", \"measurements\", \"chest\", \"boiler\", \"ray\", \"bot\", \"numerical\", \"agricultural\", \"hierarchy\", \"correlation\", \"definition\", \"preset\", \"particle\", \"attention\", \"panoramic\", \"literary\", \"matrix\", \"bgp\", \"print\", \"kernel\", \"color\", \"wind\", \"mood\", \"transfer\", \"character\", \"topic\", \"interact\", \"distribution\", \"element\", \"characters\", \"post\", \"elements\", \"message\", \"interest\", \"well\", \"layer\", \"smart\", \"automated\", \"activity\", \"sequence\", \"time\", \"method\", \"information\", \"values\", \"present\", \"event\", \"real\", \"invention\"], \"Total\": [799.0, 797.0, 824.0, 709.0, 550.0, 325.0, 232.0, 1158.0, 706.0, 293.0, 181.0, 139.0, 363.0, 281.0, 92.0, 302.0, 222.0, 457.0, 101.0, 135.0, 157.0, 130.0, 563.0, 762.0, 125.0, 124.0, 130.0, 74.0, 124.0, 99.0, 293.5288391113281, 131.38412475585938, 550.0564575195312, 107.1497573852539, 101.54978942871094, 81.36869812011719, 69.66895294189453, 68.19493103027344, 62.327552795410156, 64.13207244873047, 58.966827392578125, 55.70661544799805, 66.1705322265625, 161.9296112060547, 47.40932846069336, 46.880916595458984, 46.17522430419922, 45.92291259765625, 43.59425735473633, 42.83750534057617, 43.282005310058594, 40.84000778198242, 43.15128707885742, 41.03499984741211, 36.742496490478516, 34.858306884765625, 35.55635452270508, 34.03284454345703, 65.46492004394531, 32.88688659667969, 87.68814849853516, 709.5269775390625, 159.699951171875, 706.70166015625, 100.4612045288086, 824.5437622070312, 302.29339599609375, 457.78289794921875, 308.8324890136719, 71.41717529296875, 156.0315704345703, 873.64208984375, 363.9981384277344, 359.7330017089844, 163.79713439941406, 167.3179168701172, 125.16168975830078, 762.1278076171875, 304.3358459472656, 320.93023681640625, 361.2594909667969, 137.24246215820312, 141.22702026367188, 146.47311401367188, 515.90771484375, 828.2012939453125, 366.0271301269531, 863.5733642578125, 1158.6654052734375, 563.823486328125, 345.0052795410156, 468.6756286621094, 181.25973510742188, 228.99368286132812, 325.8015441894531, 181.74757385253906, 135.6504364013672, 130.589599609375, 125.31151580810547, 124.90557861328125, 89.99501037597656, 67.00628662109375, 62.34727478027344, 61.83808898925781, 56.132667541503906, 124.02511596679688, 54.849117279052734, 55.0461540222168, 54.3383674621582, 44.81611633300781, 49.32845687866211, 42.47819137573242, 41.417301177978516, 42.4268913269043, 42.171592712402344, 40.929039001464844, 43.46243667602539, 34.931129455566406, 34.68250274658203, 31.532028198242188, 31.563329696655273, 30.440814971923828, 28.957290649414062, 27.231853485107422, 71.16634368896484, 58.380470275878906, 281.4481201171875, 103.33683013916016, 41.83222198486328, 142.63424682617188, 799.0222778320312, 797.7738037109375, 63.6910400390625, 144.4223175048828, 1158.6654052734375, 863.5733642578125, 84.80134582519531, 383.9939270019531, 468.6756286621094, 169.0078125, 515.90771484375, 366.0271301269531, 828.2012939453125, 762.1278076171875, 210.69703674316406, 111.65511322021484, 563.823486328125, 275.70916748046875, 873.64208984375, 228.99368286132812, 222.33802795410156, 87.15118408203125, 69.68708038330078, 54.871463775634766, 54.025360107421875, 51.177452087402344, 47.48745346069336, 47.369407653808594, 46.582279205322266, 38.54190444946289, 33.693172454833984, 33.25407409667969, 32.26600646972656, 32.63029861450195, 31.463958740234375, 33.36933898925781, 30.129377365112305, 29.322134017944336, 28.60346221923828, 28.09176254272461, 27.71930503845215, 27.925325393676758, 27.33936309814453, 26.546974182128906, 25.876903533935547, 23.770952224731445, 23.904708862304688, 23.494714736938477, 22.893308639526367, 22.366004943847656, 22.33774757385254, 68.46139526367188, 22.81377601623535, 81.65403747558594, 48.33498764038086, 87.11306762695312, 41.78941345214844, 49.31193923950195, 103.94029998779297, 137.844482421875, 51.71977996826172, 97.3321533203125, 61.07037353515625, 222.33802795410156, 797.7738037109375, 799.0222778320312, 86.67859649658203, 1158.6654052734375, 109.59678649902344, 124.35704803466797, 107.76536560058594, 269.16876220703125, 109.67346954345703, 131.00619506835938, 383.9939270019531, 275.70916748046875, 863.5733642578125, 345.0052795410156, 828.2012939453125, 151.18203735351562, 563.823486328125, 361.2594909667969, 873.64208984375, 92.27351379394531, 61.97679138183594, 50.938655853271484, 46.56694030761719, 38.12216567993164, 34.67097473144531, 28.245878219604492, 28.267620086669922, 25.5317440032959, 23.14888572692871, 21.991411209106445, 23.751386642456055, 18.181800842285156, 18.394319534301758, 17.290103912353516, 17.353599548339844, 29.122175216674805, 16.826818466186523, 18.178171157836914, 16.209062576293945, 16.389183044433594, 15.905643463134766, 16.04045867919922, 15.696087837219238, 16.21710968017578, 15.81707763671875, 14.979145050048828, 14.653603553771973, 15.91192626953125, 14.088303565979004, 20.772375106811523, 74.0710678100586, 43.4261360168457, 232.2441864013672, 30.69820213317871, 139.38795471191406, 39.61558151245117, 30.80927848815918, 24.897205352783203, 99.55472564697266, 157.6478271484375, 236.58839416503906, 828.2012939453125, 130.4436798095703, 563.823486328125, 824.5437622070312, 200.3219757080078, 104.31450653076172, 709.5269775390625, 320.93023681640625, 863.5733642578125, 345.0052795410156, 359.7330017089844, 52.11452102661133, 37.01755142211914, 36.40948486328125, 25.50949478149414, 25.432966232299805, 24.24972915649414, 22.920297622680664, 22.598840713500977, 20.955780029296875, 20.95613670349121, 17.612911224365234, 16.880887985229492, 17.33806610107422, 15.975845336914062, 15.179403305053711, 15.04473876953125, 14.651710510253906, 14.545130729675293, 13.668522834777832, 13.839431762695312, 12.593472480773926, 12.261054992675781, 12.623191833496094, 11.846664428710938, 11.276784896850586, 12.483137130737305, 11.15025806427002, 10.813650131225586, 10.744829177856445, 10.642942428588867, 45.682735443115234, 29.25579071044922, 13.74230670928955, 23.093828201293945, 36.84718704223633, 19.1778564453125, 22.494468688964844, 49.31182861328125, 101.46206665039062, 51.13528823852539, 30.070375442504883, 43.384063720703125, 23.960527420043945, 46.50044250488281, 60.11137390136719, 56.35719299316406, 210.69703674316406, 563.823486328125, 363.9981384277344, 98.31205749511719, 71.3044662475586, 78.3468246459961, 69.44538879394531, 65.68450927734375], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.8977000117301941, 0.895799994468689, 0.8946999907493591, 0.8945000171661377, 0.8939999938011169, 0.8921999931335449, 0.8907999992370605, 0.8902000188827515, 0.8895000219345093, 0.88919997215271, 0.8888999819755554, 0.8881000280380249, 0.8880000114440918, 0.8859000205993652, 0.8855999708175659, 0.8855000138282776, 0.885200023651123, 0.8849999904632568, 0.8845999836921692, 0.8841999769210815, 0.8835999965667725, 0.8831999897956848, 0.8827999830245972, 0.8823999762535095, 0.8813999891281128, 0.880299985408783, 0.8798999786376953, 0.879800021648407, 0.8797000050544739, 0.8784999847412109, 0.8741999864578247, 0.8366000056266785, 0.8592000007629395, 0.8183000087738037, 0.8557999730110168, 0.786899983882904, 0.816100001335144, 0.7918000221252441, 0.8019000291824341, 0.8636000156402588, 0.7986000180244446, 0.6729000210762024, 0.7321000099182129, 0.713699996471405, 0.770799994468689, 0.7646999955177307, 0.7967000007629395, 0.5598000288009644, 0.6341000199317932, 0.6136000156402588, 0.5713000297546387, 0.7487999796867371, 0.7376000285148621, 0.7134000062942505, 0.39890000224113464, 0.20839999616146088, 0.4207000136375427, 0.07880000025033951, -0.058400001376867294, 0.14169999957084656, 0.2937000095844269, 0.09279999881982803, 0.5685999989509583, 0.34610000252723694, 1.3799999952316284, 1.3782000541687012, 1.3767999410629272, 1.3766000270843506, 1.3763999938964844, 1.3762999773025513, 1.3739999532699585, 1.3712999820709229, 1.3704999685287476, 1.370300054550171, 1.36899995803833, 1.3688000440597534, 1.3688000440597534, 1.3688000440597534, 1.3686000108718872, 1.3655999898910522, 1.3652000427246094, 1.3645999431610107, 1.364400029182434, 1.364400029182434, 1.364300012588501, 1.3638999462127686, 1.3638999462127686, 1.3609000444412231, 1.3602999448776245, 1.3587000370025635, 1.3585000038146973, 1.3579000234603882, 1.3561999797821045, 1.3544000387191772, 1.3335000276565552, 1.3164000511169434, 1.1986000537872314, 1.2545000314712524, 1.3251999616622925, 1.169600009918213, 0.7350999712944031, 0.7067000269889832, 1.2338999509811401, 0.9675999879837036, 0.25369998812675476, 0.35109999775886536, 1.1225999593734741, 0.5734999775886536, 0.436599999666214, 0.8062999844551086, 0.24500000476837158, 0.39730000495910645, -0.03500000014901161, -0.04830000177025795, 0.49970000982284546, 0.9101999998092651, -0.21040000021457672, 0.24230000376701355, -0.7027000188827515, 0.3070000112056732, 0.21889999508857727, 1.6822999715805054, 1.679900050163269, 1.677299976348877, 1.676900029182434, 1.676300048828125, 1.6750999689102173, 1.6747000217437744, 1.6747000217437744, 1.6708999872207642, 1.6686999797821045, 1.6676000356674194, 1.6674000024795532, 1.6672999858856201, 1.666599988937378, 1.6663999557495117, 1.6651999950408936, 1.6650999784469604, 1.6643999814987183, 1.664199948310852, 1.663699984550476, 1.663599967956543, 1.6632000207901, 1.6628999710083008, 1.6617000102996826, 1.659600019454956, 1.659000039100647, 1.658400058746338, 1.6577999591827393, 1.6576999425888062, 1.6574000120162964, 1.652999997138977, 1.657099962234497, 1.562600016593933, 1.5992000102996826, 1.5494999885559082, 1.6039999723434448, 1.5814000368118286, 1.4301999807357788, 1.378499984741211, 1.5469000339508057, 1.4253000020980835, 1.4977999925613403, 1.1958999633789062, 0.8903999924659729, 0.8763999938964844, 1.3277000188827515, 0.46320000290870667, 1.1569000482559204, 1.0973000526428223, 1.1526999473571777, 0.6912999749183655, 1.0987000465393066, 0.9889000058174133, 0.37529999017715454, 0.5461000204086304, -0.15459999442100525, 0.2612000107765198, -0.3456999957561493, 0.8076000213623047, -0.39239999651908875, -0.06719999760389328, -1.1553000211715698, 2.4007999897003174, 2.3968000411987305, 2.394200086593628, 2.3928000926971436, 2.3893001079559326, 2.3870999813079834, 2.381200075149536, 2.3808000087738037, 2.379300117492676, 2.376499891281128, 2.37280011177063, 2.3721001148223877, 2.367300033569336, 2.3671000003814697, 2.365299940109253, 2.3643999099731445, 2.3640999794006348, 2.363800048828125, 2.3633999824523926, 2.3624000549316406, 2.3610999584198, 2.3608999252319336, 2.3608999252319336, 2.3606998920440674, 2.3605000972747803, 2.360100030899048, 2.357300043106079, 2.357100009918213, 2.3554000854492188, 2.3550000190734863, 2.3492000102996826, 2.246299982070923, 2.1651999950408936, 1.7800999879837036, 2.2081000804901123, 1.8114999532699585, 2.0611000061035156, 2.1142001152038574, 2.183500051498413, 1.5937999486923218, 1.2964999675750732, 1.031000018119812, 0.25270000100135803, 1.2193000316619873, 0.3384999930858612, 0.08720000088214874, 0.7153000235557556, 1.1059999465942383, -0.43639999628067017, 0.05779999867081642, -0.8866999745368958, -0.09769999980926514, -0.299699991941452, 2.6631999015808105, 2.6572000980377197, 2.6568000316619873, 2.6477999687194824, 2.64739990234375, 2.6466000080108643, 2.643899917602539, 2.643699884414673, 2.641200065612793, 2.640899896621704, 2.634399890899658, 2.632200002670288, 2.6301000118255615, 2.6298000812530518, 2.6250998973846436, 2.6249001026153564, 2.6242001056671143, 2.623300075531006, 2.6205999851226807, 2.6191999912261963, 2.6161000728607178, 2.6152000427246094, 2.614000082015991, 2.611599922180176, 2.609600067138672, 2.6094000339508057, 2.6078999042510986, 2.606600046157837, 2.6057000160217285, 2.604300022125244, 2.6008999347686768, 2.5789999961853027, 2.595900058746338, 2.5625998973846436, 2.4256999492645264, 2.5179998874664307, 2.4797000885009766, 2.2948999404907227, 1.985700011253357, 2.1575000286102295, 2.245699882507324, 2.0543999671936035, 2.339200019836426, 1.9101999998092651, 1.7379000186920166, 1.7727999687194824, 0.817300021648407, 0.12860000133514404, 0.3540000021457672, 1.263700008392334, 1.4793000221252441, 1.3371000289916992, 1.4119000434875488, 1.4602999687194824], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -4.518799781799316, -5.3246002197265625, -3.8938000202178955, -5.529799938201904, -5.584000110626221, -5.807300090789795, -5.963900089263916, -5.985899925231934, -6.076600074768066, -6.048299789428711, -6.132699966430664, -6.190299987792969, -6.0183000564575195, -5.125500202178955, -6.354100227355957, -6.3653998374938965, -6.380899906158447, -6.386499881744385, -6.439000129699707, -6.456999778747559, -6.4471001625061035, -6.50570011138916, -6.451000213623047, -6.501699924468994, -6.6132001876831055, -6.666999816894531, -6.647500038146973, -6.691400051116943, -6.037300109863281, -6.726900100708008, -5.7505998611450195, -3.6974000930786133, -5.165999889373779, -3.719599962234497, -5.632900238037109, -3.5968000888824463, -4.571000099182129, -4.180300235748291, -4.563799858093262, -5.966400146484375, -5.249899864196777, -3.6530001163482666, -4.469200134277344, -4.4994001388549805, -5.229100227355957, -5.213900089263916, -5.4721999168396, -3.902600049972534, -4.746300220489502, -4.713699817657471, -4.637599945068359, -5.427999973297119, -5.410600185394287, -5.3983001708984375, -4.453700065612793, -4.170899868011475, -4.775100231170654, -4.258600234985352, -4.1020002365112305, -4.622000217437744, -4.961299896240234, -4.855899810791016, -5.329999923706055, -5.31879997253418, -3.9321999549865723, -4.5177001953125, -4.811699867248535, -4.849899768829346, -4.891300201416016, -4.894599914550781, -5.224699974060059, -5.52239990234375, -5.595300197601318, -5.603700160980225, -5.7017998695373535, -4.909200191497803, -5.725200176239014, -5.72160005569458, -5.7347002029418945, -5.9303998947143555, -5.83489990234375, -5.984899997711182, -6.01039981842041, -5.986400127410889, -5.992499828338623, -6.02269983291626, -5.962699890136719, -6.184299945831299, -6.19189977645874, -6.28879976272583, -6.288000106811523, -6.324900150299072, -6.376500129699707, -6.439700126647949, -5.500100135803223, -5.715199947357178, -4.260000228881836, -5.205999851226807, -6.039700031280518, -4.968699932098389, -3.6800999641418457, -3.7100000381469727, -5.710599899291992, -5.158199787139893, -3.789799928665161, -3.9863998889923096, -5.535600185394287, -4.574399948120117, -4.51200008392334, -5.162300109863281, -4.607600212097168, -4.798600196838379, -4.4141998291015625, -4.510799884796143, -5.2484002113342285, -5.472899913787842, -4.9741997718811035, -5.2368998527526855, -5.028500080108643, -5.357800006866455, -5.475399971008301, -4.948599815368652, -5.174699783325195, -5.416200160980225, -5.432199954986572, -5.486999988555908, -5.563000202178955, -5.565800189971924, -5.582600116729736, -5.775899887084961, -5.912499904632568, -5.926700115203857, -5.957099914550781, -5.946000099182129, -5.983099937438965, -5.9243998527526855, -6.0278000831604, -6.055099964141846, -6.080599784851074, -6.098800182342529, -6.11269998550415, -6.105400085449219, -6.126999855041504, -6.156700134277344, -6.1834001541137695, -6.270500183105469, -6.265500068664551, -6.283400058746338, -6.309800148010254, -6.3333001136779785, -6.334799766540527, -5.219299793243408, -6.314000129699707, -5.133500099182129, -5.621200084686279, -5.081900119781494, -5.7617998123168945, -5.61899995803833, -5.024499893188477, -4.793900012969971, -5.605800151824951, -5.095099925994873, -5.488699913024902, -4.4984002113342285, -3.5262999534606934, -3.5387001037597656, -5.308599948883057, -3.5803000926971436, -5.244800090789795, -5.178100109100342, -5.265900135040283, -4.8119001388549805, -5.302299976348877, -5.234399795532227, -4.772600173950195, -4.93310022354126, -4.492000102996826, -4.993800163269043, -4.724899768829346, -5.27239990234375, -5.156199932098389, -5.276199817657471, -5.481200218200684, -4.172999858856201, -4.574999809265137, -4.77370023727417, -4.864799976348877, -5.06850004196167, -5.165500164031982, -5.376399993896484, -5.375999927520752, -5.479300022125244, -5.580100059509277, -5.6350998878479, -5.558800220489502, -5.8308000564575195, -5.819399833679199, -5.8831000328063965, -5.88040018081665, -5.3628997802734375, -5.911799907684326, -5.83489990234375, -5.9506001472473145, -5.940800189971924, -5.9710001945495605, -5.962600231170654, -5.984399795532227, -5.951900005340576, -5.97730016708374, -6.034599781036377, -6.056700229644775, -5.976099967956543, -6.098100185394287, -5.715700149536133, -4.5472002029418945, -5.162300109863281, -3.8705999851226807, -5.46619987487793, -4.349800109863281, -5.3582000732421875, -5.55649995803833, -5.700300216674805, -4.9039998054504395, -4.741700172424316, -4.601200103759766, -4.1265997886657715, -5.008200168609619, -4.425300121307373, -4.296500205993652, -5.0833001136779785, -5.345099925994873, -4.970300197601318, -5.269499778747559, -5.224100112915039, -5.352700233459473, -5.512899875640869, -4.481900215148926, -4.829899787902832, -4.84689998626709, -5.211699962615967, -5.215099811553955, -5.263599872589111, -5.3225998878479, -5.336999893188477, -5.414899826049805, -5.415200233459473, -5.595399856567383, -5.640200138092041, -5.615499973297119, -5.697700023651123, -5.753499984741211, -5.762599945068359, -5.78980016708374, -5.797900199890137, -5.862800121307373, -5.851799964904785, -5.9492998123168945, -5.976900100708008, -5.948999881744385, -6.014900207519531, -6.066199779510498, -5.964799880981445, -6.07919979095459, -6.111100196838379, -6.1184000968933105, -6.12939977645874, -4.675899982452393, -5.143499851226807, -5.882199764251709, -5.396299839019775, -5.065999984741211, -5.626800060272217, -5.5055999755859375, -4.905399799346924, -4.493100166320801, -5.0065999031066895, -5.4492998123168945, -5.274099826812744, -5.583000183105469, -5.348899841308594, -5.264500141143799, -5.294000148773193, -4.930799961090088, -4.635200023651123, -4.847400188446045, -5.246699810028076, -5.35230016708374, -5.400300025939941, -5.446100234985352, -5.453400135040283]}, \"token.table\": {\"Topic\": [3, 4, 1, 5, 3, 3, 4, 5, 2, 3, 2, 4, 4, 1, 2, 3, 4, 5, 2, 2, 4, 4, 4, 1, 2, 3, 1, 3, 4, 1, 2, 3, 4, 5, 1, 1, 3, 4, 5, 3, 2, 3, 1, 2, 3, 4, 5, 2, 5, 5, 5, 3, 2, 1, 3, 4, 2, 1, 3, 5, 1, 3, 4, 2, 5, 5, 4, 2, 3, 4, 3, 5, 3, 1, 3, 1, 3, 1, 2, 5, 1, 2, 4, 4, 3, 1, 3, 1, 3, 2, 3, 3, 5, 1, 2, 5, 3, 5, 1, 2, 3, 4, 5, 2, 2, 3, 4, 2, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 2, 3, 2, 2, 5, 1, 3, 4, 3, 4, 3, 4, 4, 3, 1, 1, 5, 1, 4, 5, 2, 1, 2, 3, 5, 2, 3, 5, 3, 3, 1, 2, 1, 2, 3, 1, 1, 4, 3, 1, 4, 4, 1, 2, 4, 1, 2, 4, 5, 1, 5, 3, 1, 2, 3, 5, 1, 3, 4, 2, 2, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 1, 4, 3, 1, 2, 3, 4, 5, 1, 2, 3, 1, 5, 1, 4, 5, 1, 3, 1, 2, 3, 4, 5, 5, 3, 4, 2, 4, 1, 3, 4, 2, 5, 1, 2, 3, 5, 3, 1, 2, 3, 4, 3, 1, 5, 5, 1, 2, 3, 5, 3, 1, 1, 5, 1, 2, 3, 5, 1, 1, 3, 5, 5, 5, 1, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 1, 2, 3, 5, 1, 2, 3, 2, 5, 3, 1, 2, 1, 3, 4, 1, 2, 3, 2, 3, 2, 3, 4, 1, 1, 1, 5, 1, 2, 3, 1, 2, 3, 4, 2, 3, 4, 3, 5, 5, 5, 3, 2, 3, 2, 1, 2, 3, 4, 4, 3, 5, 2, 3, 4, 3, 1, 2, 3, 4, 5, 5, 2, 5, 1, 4, 1, 2, 3, 1, 2, 3, 4, 5, 1, 2, 1, 4, 4, 4, 5, 2, 5, 1, 2, 5, 1, 1, 2, 3, 1, 1, 1, 3, 1, 2, 3, 1, 2, 3, 2, 1, 4, 2, 1, 4, 4, 5, 1, 2, 4, 2, 1, 4, 2, 2, 1, 3, 4, 5, 3, 4, 1, 3, 5, 1, 1, 1, 2, 3, 4, 2, 4, 1, 5, 1, 3, 4, 2, 3, 4, 4, 1, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 1, 4, 1, 1, 5, 1, 2, 1, 2, 3, 4, 5, 3, 3, 3, 1, 5, 2, 1, 2, 3, 4, 5, 3, 4, 3, 5, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 3, 4, 5, 1, 2, 4, 5, 2, 2, 2, 3, 4, 5, 5, 5], \"Freq\": [0.9789435863494873, 0.9549182653427124, 0.5988883376121521, 0.3826231062412262, 0.9789024591445923, 0.9848799705505371, 0.9350008964538574, 0.9389174580574036, 0.7664972543716431, 0.2240530550479889, 0.9773012399673462, 0.9254082441329956, 0.9508630633354187, 0.4649425148963928, 0.21556425094604492, 0.03804074972867966, 0.2536050081253052, 0.02958725020289421, 0.9849821925163269, 0.9899244904518127, 0.9878252744674683, 0.955652117729187, 0.9705639481544495, 0.10881828516721725, 0.1596001535654068, 0.7327097654342651, 0.15145555138587952, 0.15145555138587952, 0.7067925930023193, 0.7656015753746033, 0.04600181058049202, 0.1347195953130722, 0.055859342217445374, 0.9393449425697327, 0.9836038947105408, 0.1505361944437027, 0.19354654848575592, 0.19354654848575592, 0.4731137752532959, 0.9889317750930786, 0.9855189323425293, 0.9643296003341675, 0.499878466129303, 0.24269461631774902, 0.1304030865430832, 0.11591385304927826, 0.01086692325770855, 0.9733438491821289, 0.9285314083099365, 0.954374372959137, 0.9478174448013306, 0.966864287853241, 0.9864470362663269, 0.699806809425354, 0.01917278952896595, 0.2684190571308136, 0.9803214073181152, 0.9753772616386414, 0.06567032635211945, 0.9193845391273499, 0.13099642097949982, 0.8187276124954224, 0.04912365600466728, 0.10428693890571594, 0.8342955112457275, 0.9543906450271606, 0.9430615305900574, 0.023929504677653313, 0.9093211889266968, 0.047859009355306625, 0.9901404976844788, 0.8968402147293091, 0.9867909550666809, 0.9776228070259094, 0.015275356359779835, 0.9693441987037659, 0.022808099165558815, 0.6174405813217163, 0.3742891848087311, 0.005464076064527035, 0.7172028422355652, 0.28136420249938965, 0.984239399433136, 0.9806473851203918, 0.9897351264953613, 0.9819081425666809, 0.01235104538500309, 0.9555927515029907, 0.03981636464595795, 0.10139532387256622, 0.8922788500785828, 0.979426920413971, 0.9305578470230103, 0.8496957421302795, 0.09205037355422974, 0.05664638429880142, 0.9810207486152649, 0.9829761981964111, 0.3831994831562042, 0.32364821434020996, 0.29257798194885254, 0.935187578201294, 0.9555198550224304, 0.9831273555755615, 0.8635437488555908, 0.03140158951282501, 0.09420477598905563, 0.9817896485328674, 0.8806153535842896, 0.11612509936094284, 0.5391320586204529, 0.22963032126426697, 0.04492767155170441, 0.18470264971256256, 0.6047593355178833, 0.3198246359825134, 0.0736565813422203, 0.9683645367622375, 0.12627267837524414, 0.872429370880127, 0.9669412970542908, 0.08660322427749634, 0.9093338847160339, 0.5899224877357483, 0.08246228098869324, 0.32984912395477295, 0.8817690014839172, 0.11022112518548965, 0.048140861093997955, 0.9628171920776367, 0.9483420848846436, 0.9793959856033325, 0.9892696142196655, 0.21711291372776031, 0.787034273147583, 0.18251198530197144, 0.12167465686798096, 0.6894897222518921, 0.9821524024009705, 0.3433385193347931, 0.07423534989356995, 0.5846034288406372, 0.9800272583961487, 0.33185774087905884, 0.39567652344703674, 0.2552751898765564, 0.9917558431625366, 0.9625157713890076, 0.33235859870910645, 0.6577930450439453, 0.09815314412117004, 0.806257963180542, 0.0911422073841095, 0.9696515202522278, 0.99262535572052, 0.005453985650092363, 0.9769927859306335, 0.5524599552154541, 0.4419679641723633, 0.9762536883354187, 0.8260901570320129, 0.16385260224342346, 0.006827191449701786, 0.8791362643241882, 0.030525565147399902, 0.07936646789312363, 0.012210225686430931, 0.9823477864265442, 0.9223023653030396, 0.9675674438476562, 0.8597922325134277, 0.036431875079870224, 0.09472286701202393, 0.0072863744571805, 0.9028321504592896, 0.015979330986738205, 0.07989665120840073, 0.997539758682251, 0.9954850673675537, 0.5720681548118591, 0.340620756149292, 0.04803625866770744, 0.03493546321988106, 0.5449191927909851, 0.133331298828125, 0.24057602882385254, 0.08115817606449127, 0.48947614431381226, 0.09921813756227493, 0.41671615839004517, 0.9733197689056396, 0.9346327781677246, 0.987500011920929, 0.8434109091758728, 0.01923086680471897, 0.005494533572345972, 0.03296720236539841, 0.09890160709619522, 0.906640350818634, 0.0712360292673111, 0.02266600914299488, 0.9745485782623291, 0.9459838271141052, 0.27378353476524353, 0.13689176738262177, 0.5866789817810059, 0.8725903630256653, 0.125509575009346, 0.28926149010658264, 0.12179432064294815, 0.04567286744713783, 0.2435886412858963, 0.28926149010658264, 0.961296796798706, 0.20724846422672272, 0.7829386591911316, 0.9562007188796997, 0.047810036689043045, 0.8914020657539368, 0.009702335111796856, 0.09823614358901978, 0.4609987735748291, 0.5301485657691956, 0.03885813057422638, 0.5089161992073059, 0.44874876737594604, 0.0037604644894599915, 0.9609794616699219, 0.7509419918060303, 0.0186956524848938, 0.1339855045080185, 0.0965941995382309, 0.966112494468689, 0.9843528866767883, 0.9787086248397827, 0.9598479270935059, 0.032539770007133484, 0.5231393575668335, 0.44304147362709045, 0.0012515295529738069, 0.983635663986206, 0.9873154163360596, 0.9934844970703125, 0.9506312012672424, 0.7111667990684509, 0.23880508542060852, 0.049860402941703796, 0.9735012650489807, 0.9797918796539307, 0.019711799919605255, 0.4730831980705261, 0.5026509165763855, 0.9725116491317749, 0.9887533187866211, 0.979902982711792, 0.4682316482067108, 0.20396454632282257, 0.12415233254432678, 0.1259259283542633, 0.07803861051797867, 0.4012352526187897, 0.23033876717090607, 0.36779898405075073, 0.9791732430458069, 0.9824777245521545, 0.24219132959842682, 0.4453195333480835, 0.26823341846466064, 0.04427153244614601, 0.2461853325366974, 0.2005954533815384, 0.5561965107917786, 0.9899381995201111, 0.9306802153587341, 0.9841181039810181, 0.36720216274261475, 0.6269305348396301, 0.9372441172599792, 0.004228169098496437, 0.05778497830033302, 0.11725074052810669, 0.8314143419265747, 0.04974273592233658, 0.9420958757400513, 0.05138704925775528, 0.08275578916072845, 0.9103136658668518, 0.922751247882843, 0.9947446584701538, 0.9913660883903503, 0.9812094569206238, 0.9805015325546265, 0.008062883280217648, 0.9836717247962952, 0.9611358642578125, 0.7955203056335449, 0.1247650533914566, 0.05837630853056908, 0.02174803614616394, 0.9722184538841248, 0.0343380942940712, 0.9614666700363159, 0.9890139698982239, 0.9528746008872986, 0.9786139726638794, 0.9510903358459473, 0.9621535539627075, 0.9809949398040771, 0.987587034702301, 0.9952050447463989, 0.719704270362854, 0.10795564204454422, 0.17162178456783295, 0.002768093254417181, 0.9426891207695007, 0.13336612284183502, 0.8001967072486877, 0.09667481482028961, 0.8700733184814453, 0.019334962591528893, 0.9852542877197266, 0.25243860483169556, 0.1542680412530899, 0.11219493299722672, 0.18231676518917084, 0.3085360825061798, 0.9625214338302612, 0.9893600940704346, 0.9754553437232971, 0.9747776389122009, 0.924949049949646, 0.3434952199459076, 0.15266454219818115, 0.49615973234176636, 0.8956210613250732, 0.013106649741530418, 0.0415043905377388, 0.013106649741530418, 0.039319947361946106, 0.04215475916862488, 0.9555078744888306, 0.465027779340744, 0.5339207649230957, 0.9970358610153198, 0.9503697156906128, 0.9652010202407837, 0.7055904269218445, 0.2879960834980011, 0.9036632776260376, 0.025635838508605957, 0.07049855589866638, 0.9945859909057617, 0.27373066544532776, 0.13686533272266388, 0.5839587450027466, 0.9863684773445129, 0.9823103547096252, 0.99546879529953, 0.9622880816459656, 0.966154158115387, 0.014002233743667603, 0.014002233743667603, 0.9580466151237488, 0.03130871430039406, 0.006261742673814297, 0.9937729835510254, 0.16287599503993988, 0.8143799304962158, 0.9547642469406128, 0.4448017179965973, 0.5524150133132935, 0.8033030033111572, 0.16066059470176697, 0.9196363687515259, 0.013232178054749966, 0.06946893036365509, 0.9958867430686951, 0.6899529099464417, 0.3066457509994507, 0.9927498698234558, 0.9730691313743591, 0.05323189124464989, 0.4790870249271393, 0.05323189124464989, 0.40811118483543396, 0.22720427811145782, 0.7465283274650574, 0.21575604379177094, 0.7705572843551636, 0.020548194646835327, 0.9970763325691223, 0.9730322360992432, 0.8283921480178833, 0.027798395603895187, 0.07505566626787186, 0.06671614944934845, 0.9798216223716736, 0.9351353645324707, 0.25041183829307556, 0.709500253200531, 0.24227434396743774, 0.6922124028205872, 0.057684365659952164, 0.22128087282180786, 0.7696725726127625, 0.9832214117050171, 0.9553964138031006, 0.9804492592811584, 0.9551564455032349, 0.43887412548065186, 0.35665759444236755, 0.15748517215251923, 0.03705533593893051, 0.009263833984732628, 0.35907402634620667, 0.31917691230773926, 0.31917691230773926, 0.003627010388299823, 0.34909629821777344, 0.5621041655540466, 0.08875329047441483, 0.9815728068351746, 0.1485060304403305, 0.8505344986915588, 0.990398108959198, 0.9947915077209473, 0.0034068203531205654, 0.9794317483901978, 0.9944300055503845, 0.19933834671974182, 0.4129151701927185, 0.16611529886722565, 0.06644611805677414, 0.1566229909658432, 0.9859398603439331, 0.9806836247444153, 0.992201566696167, 0.0683625340461731, 0.8887129426002502, 0.9845190644264221, 0.07646015286445618, 0.3103382885456085, 0.6116812229156494, 0.9796238541603088, 0.93958979845047, 0.9740504026412964, 0.9558916687965393, 0.9640469551086426, 0.029213543981313705, 0.9785629510879517, 0.9197657704353333, 0.05943102017045021, 0.0183953158557415, 0.0014150242786854506, 0.0014150242786854506, 0.4459374248981476, 0.38832828402519226, 0.10668358951807022, 0.021336719393730164, 0.038406092673540115, 0.1849513202905655, 0.5548539757728577, 0.17690995335578918, 0.08845497667789459, 0.3661809265613556, 0.3051507771015167, 0.08137353509664536, 0.24412061274051666, 0.9975140690803528, 0.9887426495552063, 0.9889436960220337, 0.1995319277048111, 0.16627660393714905, 0.6651064157485962, 0.9247571229934692, 0.9897017478942871], \"Term\": [\"accordance\", \"account\", \"activity\", \"activity\", \"adaptive\", \"adjustment\", \"affinity\", \"agricultural\", \"algorithm\", \"algorithm\", \"algorithms\", \"ambiguous\", \"analogical\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"anomalies\", \"anomaly\", \"answer\", \"answering\", \"answers\", \"apparatus\", \"apparatus\", \"apparatus\", \"approach\", \"approach\", \"approach\", \"associated\", \"associated\", \"associated\", \"associated\", \"attention\", \"attribute\", \"automated\", \"automated\", \"automated\", \"automated\", \"automation\", \"autonomous\", \"axis\", \"based\", \"based\", \"based\", \"based\", \"based\", \"behavior\", \"bgp\", \"boiler\", \"bot\", \"cad\", \"camera\", \"candidate\", \"candidate\", \"candidate\", \"captured\", \"case\", \"character\", \"character\", \"characteristics\", \"characteristics\", \"characteristics\", \"characters\", \"characters\", \"chest\", \"clauses\", \"cloud\", \"cloud\", \"cloud\", \"code\", \"color\", \"command\", \"components\", \"components\", \"comprising\", \"comprising\", \"computer\", \"computer\", \"computer\", \"computing\", \"computing\", \"concept\", \"concepts\", \"condition\", \"content\", \"content\", \"context\", \"context\", \"controller\", \"controller\", \"correction\", \"correlation\", \"corresponding\", \"corresponding\", \"corresponding\", \"current\", \"customer\", \"data\", \"data\", \"data\", \"defining\", \"definition\", \"depth\", \"detect\", \"detect\", \"detect\", \"detected\", \"detection\", \"detection\", \"determining\", \"determining\", \"determining\", \"determining\", \"device\", \"device\", \"device\", \"dictionary\", \"digital\", \"digital\", \"dimensional\", \"distribution\", \"distribution\", \"document\", \"document\", \"document\", \"domain\", \"domain\", \"driving\", \"driving\", \"ecg\", \"electric\", \"electronic\", \"element\", \"element\", \"elements\", \"elements\", \"elements\", \"employs\", \"engine\", \"engine\", \"engine\", \"entries\", \"event\", \"event\", \"event\", \"events\", \"examples\", \"feature\", \"feature\", \"features\", \"features\", \"features\", \"file\", \"first\", \"first\", \"flow\", \"form\", \"form\", \"free\", \"generate\", \"generate\", \"generate\", \"generating\", \"generating\", \"generating\", \"generating\", \"graph\", \"hierarchy\", \"hvac\", \"identified\", \"identified\", \"identified\", \"identified\", \"identifying\", \"identifying\", \"identifying\", \"image\", \"images\", \"include\", \"include\", \"include\", \"include\", \"includes\", \"includes\", \"includes\", \"includes\", \"including\", \"including\", \"including\", \"independent\", \"index\", \"individual\", \"information\", \"information\", \"information\", \"information\", \"information\", \"input\", \"input\", \"input\", \"instructions\", \"interact\", \"interest\", \"interest\", \"interest\", \"interface\", \"interface\", \"invention\", \"invention\", \"invention\", \"invention\", \"invention\", \"kernel\", \"knowledge\", \"knowledge\", \"known\", \"known\", \"language\", \"language\", \"language\", \"layer\", \"layer\", \"learning\", \"learning\", \"learning\", \"learning\", \"learns\", \"least\", \"least\", \"least\", \"least\", \"linguistic\", \"list\", \"literary\", \"log\", \"machine\", \"machine\", \"machine\", \"machine\", \"machining\", \"malicious\", \"map\", \"matrix\", \"may\", \"may\", \"may\", \"measurements\", \"member\", \"message\", \"message\", \"message\", \"messages\", \"messaging\", \"metadata\", \"method\", \"method\", \"method\", \"method\", \"method\", \"methods\", \"methods\", \"methods\", \"ml\", \"mobile\", \"model\", \"model\", \"model\", \"model\", \"models\", \"models\", \"models\", \"monitoring\", \"mood\", \"motor\", \"multi\", \"multi\", \"natural\", \"natural\", \"natural\", \"network\", \"network\", \"network\", \"neural\", \"neural\", \"new\", \"new\", \"noc\", \"node\", \"nodes\", \"notification\", \"numerical\", \"object\", \"object\", \"observation\", \"one\", \"one\", \"one\", \"one\", \"online\", \"ontology\", \"ontology\", \"optimization\", \"panoramic\", \"parameter\", \"particle\", \"patient\", \"performance\", \"person\", \"platform\", \"plurality\", \"plurality\", \"plurality\", \"plurality\", \"possible\", \"post\", \"post\", \"prediction\", \"prediction\", \"prediction\", \"predictive\", \"present\", \"present\", \"present\", \"present\", \"present\", \"preset\", \"previously\", \"print\", \"probability\", \"procedure\", \"process\", \"process\", \"process\", \"processing\", \"processing\", \"processing\", \"processing\", \"processing\", \"quality\", \"quality\", \"query\", \"query\", \"question\", \"questions\", \"ray\", \"real\", \"real\", \"received\", \"received\", \"received\", \"receiving\", \"recognition\", \"recognition\", \"recognition\", \"reminder\", \"representation\", \"request\", \"resources\", \"respective\", \"respective\", \"respective\", \"response\", \"response\", \"response\", \"risk\", \"rule\", \"rule\", \"samples\", \"search\", \"search\", \"searching\", \"searching\", \"second\", \"second\", \"second\", \"security\", \"semantic\", \"semantic\", \"sensor\", \"sensors\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"sequences\", \"sequences\", \"server\", \"server\", \"server\", \"service\", \"services\", \"set\", \"set\", \"set\", \"set\", \"signals\", \"simulation\", \"smart\", \"smart\", \"specific\", \"specific\", \"specific\", \"state\", \"state\", \"statement\", \"statements\", \"structured\", \"structures\", \"system\", \"system\", \"system\", \"system\", \"system\", \"systems\", \"systems\", \"systems\", \"systems\", \"techniques\", \"techniques\", \"techniques\", \"term\", \"terms\", \"terms\", \"test\", \"text\", \"text\", \"textual\", \"threats\", \"time\", \"time\", \"time\", \"time\", \"time\", \"token\", \"tokens\", \"tool\", \"topic\", \"topic\", \"traffic\", \"training\", \"training\", \"training\", \"transaction\", \"transfer\", \"translated\", \"trust\", \"unit\", \"unit\", \"url\", \"user\", \"user\", \"user\", \"user\", \"user\", \"using\", \"using\", \"using\", \"using\", \"using\", \"value\", \"value\", \"value\", \"value\", \"values\", \"values\", \"values\", \"values\", \"vehicle\", \"virtual\", \"vision\", \"well\", \"well\", \"well\", \"wind\", \"x\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 5, 2, 4, 3]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el5181126134848486950750417\", ldavis_el5181126134848486950750417_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el5181126134848486950750417\", ldavis_el5181126134848486950750417_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el5181126134848486950750417\", ldavis_el5181126134848486950750417_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "0     -0.117707  0.136361       1        1  40.598026\n",
       "4     -0.168393 -0.086635       2        1  25.100634\n",
       "1     -0.117770 -0.094203       3        1  18.438393\n",
       "3      0.151145  0.205483       4        1   8.992602\n",
       "2      0.252725 -0.161006       5        1   6.870351, topic_info=     Category         Freq          Term        Total  loglift  logprob\n",
       "term                                                                   \n",
       "50    Default   799.000000       machine   799.000000  30.0000  30.0000\n",
       "49    Default   797.000000      learning   797.000000  29.0000  29.0000\n",
       "14    Default   824.000000      language   824.000000  28.0000  28.0000\n",
       "18    Default   709.000000       natural   709.000000  27.0000  27.0000\n",
       "108   Default   550.000000         first   550.000000  26.0000  26.0000\n",
       "112   Default   325.000000         image   325.000000  25.0000  25.0000\n",
       "121   Default   232.000000         query   232.000000  24.0000  24.0000\n",
       "42    Default  1158.000000          data  1158.000000  23.0000  23.0000\n",
       "95    Default   706.000000          user   706.000000  22.0000  22.0000\n",
       "30    Default   293.000000          text   293.000000  21.0000  21.0000\n",
       "1046  Default   181.000000      security   181.000000  20.0000  20.0000\n",
       "126   Default   139.000000        search   139.000000  19.0000  19.0000\n",
       "84    Default   363.000000   information   363.000000  18.0000  18.0000\n",
       "87    Default   281.000000       network   281.000000  17.0000  17.0000\n",
       "22    Default    92.000000      question    92.000000  16.0000  16.0000\n",
       "127   Default   302.000000        second   302.000000  15.0000  15.0000\n",
       "722   Default   222.000000      training   222.000000  14.0000  14.0000\n",
       "20    Default   457.000000    processing   457.000000  13.0000  13.0000\n",
       "213   Default   101.000000       message   101.000000  12.0000  12.0000\n",
       "785   Default   135.000000      platform   135.000000  11.0000  11.0000\n",
       "877   Default   157.000000      document   157.000000  10.0000  10.0000\n",
       "113   Default   130.000000        images   130.000000   9.0000   9.0000\n",
       "215   Default   563.000000        method   563.000000   8.0000   8.0000\n",
       "51    Default   762.000000           may   762.000000   7.0000   7.0000\n",
       "70    Default   125.000000       vehicle   125.000000   6.0000   6.0000\n",
       "649   Default   124.000000        sensor   124.000000   5.0000   5.0000\n",
       "317   Default   130.000000      semantic   130.000000   4.0000   4.0000\n",
       "130   Default    74.000000         terms    74.000000   3.0000   3.0000\n",
       "156   Default   124.000000        object   124.000000   2.0000   2.0000\n",
       "9     Default    99.000000          form    99.000000   1.0000   1.0000\n",
       "...       ...          ...           ...          ...      ...      ...\n",
       "2089   Topic5    10.531983         print    11.276785   2.6096  -6.0662\n",
       "2685   Topic5    11.655560        kernel    12.483137   2.6094  -5.9648\n",
       "473    Topic5    10.395593         color    11.150258   2.6079  -6.0792\n",
       "3563   Topic5    10.069411          wind    10.813650   2.6066  -6.1111\n",
       "1898   Topic5     9.996126          mood    10.744829   2.6057  -6.1184\n",
       "3544   Topic5     9.886890      transfer    10.642942   2.6043  -6.1294\n",
       "876    Topic5    42.294022     character    45.682735   2.6009  -4.6759\n",
       "233    Topic5    26.498163         topic    29.255791   2.5790  -5.1435\n",
       "1872   Topic5    12.659484      interact    13.742307   2.5959  -5.8822\n",
       "2182   Topic5    20.578711  distribution    23.093828   2.5626  -5.3963\n",
       "1117   Topic5    28.632647       element    36.847187   2.4257  -5.0660\n",
       "895    Topic5    16.342754    characters    19.177856   2.5180  -5.6268\n",
       "1102   Topic5    18.448212          post    22.494469   2.4797  -5.5056\n",
       "1118   Topic5    33.620617      elements    49.311829   2.2949  -4.9054\n",
       "213    Topic5    50.777241       message   101.462067   1.9857  -4.4931\n",
       "116    Topic5    30.387238      interest    51.135288   2.1575  -5.0066\n",
       "323    Topic5    19.517107          well    30.070375   2.2457  -5.4493\n",
       "1714   Topic5    23.254829         layer    43.384064   2.0544  -5.2741\n",
       "1455   Topic5    17.075157         smart    23.960527   2.3392  -5.5830\n",
       "135    Topic5    21.578794     automated    46.500443   1.9102  -5.3489\n",
       "1301   Topic5    23.479210      activity    60.111374   1.7379  -5.2645\n",
       "387    Topic5    22.796242      sequence    56.357193   1.7728  -5.2940\n",
       "420    Topic5    32.779667          time   210.697037   0.8173  -4.9308\n",
       "215    Topic5    44.050735        method   563.823486   0.1286  -4.6352\n",
       "84     Topic5    35.630615   information   363.998138   0.3540  -4.8474\n",
       "96     Topic5    23.899574        values    98.312057   1.2637  -5.2467\n",
       "157    Topic5    21.505219       present    71.304466   1.4793  -5.3523\n",
       "1480   Topic5    20.497953         event    78.346825   1.3371  -5.4003\n",
       "460    Topic5    19.578873          real    69.445389   1.4119  -5.4461\n",
       "509    Topic5    19.436676     invention    65.684509   1.4603  -5.4534\n",
       "\n",
       "[320 rows x 6 columns], token_table=      Topic      Freq          Term\n",
       "term                               \n",
       "705       3  0.978944    accordance\n",
       "1813      4  0.954918       account\n",
       "1301      1  0.598888      activity\n",
       "1301      5  0.382623      activity\n",
       "836       3  0.978902      adaptive\n",
       "2453      3  0.984880    adjustment\n",
       "2150      4  0.935001      affinity\n",
       "3008      5  0.938917  agricultural\n",
       "98        2  0.766497     algorithm\n",
       "98        3  0.224053     algorithm\n",
       "368       2  0.977301    algorithms\n",
       "1618      4  0.925408     ambiguous\n",
       "768       4  0.950863    analogical\n",
       "72        1  0.464943      analysis\n",
       "72        2  0.215564      analysis\n",
       "72        3  0.038041      analysis\n",
       "72        4  0.253605      analysis\n",
       "72        5  0.029587      analysis\n",
       "1031      2  0.984982     anomalies\n",
       "1032      2  0.989924       anomaly\n",
       "816       4  0.987825        answer\n",
       "0         4  0.955652     answering\n",
       "1         4  0.970564       answers\n",
       "625       1  0.108818     apparatus\n",
       "625       2  0.159600     apparatus\n",
       "625       3  0.732710     apparatus\n",
       "100       1  0.151456      approach\n",
       "100       3  0.151456      approach\n",
       "100       4  0.706793      approach\n",
       "75        1  0.765602    associated\n",
       "...     ...       ...           ...\n",
       "4795      4  0.955892         trust\n",
       "889       3  0.964047          unit\n",
       "889       5  0.029214          unit\n",
       "3125      4  0.978563           url\n",
       "95        1  0.919766          user\n",
       "95        2  0.059431          user\n",
       "95        3  0.018395          user\n",
       "95        4  0.001415          user\n",
       "95        5  0.001415          user\n",
       "69        1  0.445937         using\n",
       "69        2  0.388328         using\n",
       "69        3  0.106684         using\n",
       "69        4  0.021337         using\n",
       "69        5  0.038406         using\n",
       "814       1  0.184951         value\n",
       "814       3  0.554854         value\n",
       "814       4  0.176910         value\n",
       "814       5  0.088455         value\n",
       "96        1  0.366181        values\n",
       "96        2  0.305151        values\n",
       "96        4  0.081374        values\n",
       "96        5  0.244121        values\n",
       "70        2  0.997514       vehicle\n",
       "1245      2  0.988743       virtual\n",
       "272       2  0.988944        vision\n",
       "323       3  0.199532          well\n",
       "323       4  0.166277          well\n",
       "323       5  0.665106          well\n",
       "3563      5  0.924757          wind\n",
       "1889      5  0.989702             x\n",
       "\n",
       "[476 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[1, 5, 2, 4, 3])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# explore topics visually\n",
    "pyLDAvis.enable_notebook()\n",
    "viz_topics_model_1 = pyLDAvis.gensim.prepare(model_1, \n",
    "                                             corpus_1000train, \n",
    "                                             id_to_word_1000train)\n",
    "\n",
    "# uncomment to view visualization\n",
    "viz_topics_model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to view keywords in n topics in corpus\n",
    "# pprint(model_1.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to view important keywords/weight in topic with idx 0\n",
    "# pprint(model_1.print_topic(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #1 - Evaluate\n",
    "Evaluate models using coherence and perplexity metrics. As unsupervised learning task, no labels with which to evaluate the \"expected\" prediction. There is an open research agenda on LDA evaluation approaches (intrinsic vs extrinsic; machine vs human-interpretable, etc., task-specific). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #1 - Evaluate - Pre-process test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-process 1000 patents from df-to-list worfklow above\n",
    "processed_docs_1000test = process_docs(text_test_1000)\n",
    "\n",
    "# build dictionary with dataset of 1000 patents\n",
    "id_to_word_1000test = Dictionary(processed_docs_1000test)\n",
    "\n",
    "# apply term-doc frequency (list of (token_id, token_count) tuples) to 1000 patents\n",
    "corpus_1000test = [id_to_word_1000test.doc2bow(doc) for doc in processed_docs_1000test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #1 - Evaluate - Coherence\n",
    "Calculate topic coherence for topic models with 4-step coherence pipeline (segmentation, probability estimation, confirmation measure, aggregation) from Roeder et al., 2015. \"Exploring the space of topic coherence measures\", WSDM '15 Proceedings of the Eighth ACM International Conference on Web Search and Data Mining (WSDM) 2015, 399-408."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3557055926610265\n"
     ]
    }
   ],
   "source": [
    "# calculate coherence metric for train set\n",
    "coherence_model_1train = CoherenceModel(model=model_1, \n",
    "                                        texts=processed_docs_1000train,\n",
    "                                        dictionary=id_to_word_1000train,\n",
    "                                        coherence='c_v')\n",
    "coherence_model_1train_get = coherence_model_1train.get_coherence()\n",
    "print(coherence_model_1train_get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6020573135746659\n"
     ]
    }
   ],
   "source": [
    "# calculate coherence metric for test_set\n",
    "coherence_model_1test = CoherenceModel(model=model_1, \n",
    "                                       texts=processed_docs_1000test, \n",
    "                                       dictionary=id_to_word_1000test, \n",
    "                                       coherence='c_v')\n",
    "coherence_model_1test_get = coherence_model_1test.get_coherence()\n",
    "print(coherence_model_1test_get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate coherence metric for each of n topics in test set\n",
    "coherence_model_1_per_topic = coherence_model_1test.get_coherence_per_topic()\n",
    "\n",
    "# uncomment to print coherence_model_1_per_topic\n",
    "# print(coherence_model_1_per_topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #1 - Evaluate - Perplexity\n",
    "Calculate perplexity metric. Metric calculates and returns per-word likelihood bound using a chunk of documents as evaluation corpus. Output calculated statistics, including the perplexity=2^(-bound), to log at INFO level. Returns the variational bound score calculated for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-7.091984309948656\n"
     ]
    }
   ],
   "source": [
    "# calculate perplexity metric for model_1 train set (1000 pats dataset)\n",
    "perplexity_model_1train = model_1.log_perplexity(corpus_1000train)\n",
    "print(perplexity_model_1train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-9.661613601356644\n"
     ]
    }
   ],
   "source": [
    "# calculate perplexity metric for model_1 test set (1000 pats dataset)\n",
    "perplexity_model_1test = model_1.log_perplexity(corpus_1000test)\n",
    "print(perplexity_model_1test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #1 - Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #1 - Predict - Pickle model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update path with location to save pickled model\n",
    "path_pickle_model_1 = '/Users/lee/Documents/techniche/techniche/data/model_lda_1.pkl'\n",
    "\n",
    "# save/pickle model #1 for subsequent use\n",
    "pickle.dump(model_1, open(path_pickle_model_1,'wb'))\n",
    "\n",
    "# uncommment below to load pickled model #1\n",
    "# model_1 = pickle.load(open(path_pickle_model_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #1 - inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test model #1 on 2 new text w/ two strings\n",
    "\n",
    "# define example text_input #1, expressing keyword-type search\n",
    "text_input_1 = 'smart assistant transformer model translation'.split()\n",
    "\n",
    "# define example text_input #1, expressing technical details in job post\n",
    "text_input_2 = \"\"\"At the Siri International team within Apple we bring the\n",
    "Siri intelligent assistant to our customers worldwide in over 40 languages\n",
    "and dialects. Join us, and tackle some of the most challenging problems in\n",
    "natural language processing and large scale applied machine learning. You \n",
    "will build cutting edge natural language understanding technologies and \n",
    "deploy them on a global scale. Your work will advance and shape the future\n",
    "vision of our multi-lingual, multi-cultural Siri assistant, and Search \n",
    "applications used by millions across the world Key Qualifications Extensive\n",
    "track record of scientific research in NLP and Machine Learning, or similar\n",
    "experience in developing language technologies for shipping products.\n",
    "Strong coding and software engineering skills in a mainstream programming \n",
    "language, such as Python, Java, C/C++. Familiarity with NLP/ML tools and \n",
    "packages like Caffe, pyTorch, TensorFlow, Weka, scikit-learn, nltk, etc.\n",
    "Practical experience building production quality applications related to \n",
    "natural language processing and machine learning. In-depth knowledge of \n",
    "machine learning algorithms and ability to apply them in data driven natural\n",
    "language processing systems. Ability to quickly prototype ideas / solutions,\n",
    "perform critical analysis, and use creative approaches for solving complex \n",
    "problems. Attention to detail and excellent communication skills. Description\n",
    "We are looking for a highly motivated technologist with a strong background \n",
    "in Natural Language Processing and Machine Learning research. The ideal \n",
    "candidate will have a strong track record of taking research ideas to \n",
    "real-world applications. In this position you will apply your problem solving\n",
    "skills to challenges and opportunities within Siri International, which \n",
    "involves development of large-scale language technologies for many natural\n",
    "languages worldwide. The primary responsibility of this role is to conduct\n",
    "research and develop innovative machine learning, artificial intelligence \n",
    "and NLP solutions for multi-lingual conversational agents. You will have \n",
    "the opportunity to investigate cutting edge research methods that will \n",
    "improve customer experience of our products and enable our engineers to \n",
    "scale these technologies across a variety of natural languages. You will \n",
    "also provide technical leadership and experiment-driven insights for \n",
    "engineering teams on their machine learning modeling and data decisions. \n",
    "You will play a central role in defining the future technical directions \n",
    "of Siri International through quick prototyping, critical analysis and \n",
    "development of core multi-lingual NLP technologies. Education & Experience\n",
    "PhD in Machine Learning, Statistics, Computer Science, Mathematics or \n",
    "related field with specialization in natural language processing and/or \n",
    "machine learning, OR * Masters degree in a related field with a strong \n",
    "academic/industrial track record. * Hands-on research experience in an \n",
    "academic or industrial setting.\"\"\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# suppress cell output in notebook\n",
    "\n",
    "# pass text through pre-process pipeline\n",
    "id_to_word_1000train.doc2bow(text_input_1)\n",
    "id_to_word_1000train.doc2bow(text_input_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_input_1 = get_topics(id_to_word_1000train.doc2bow(text_input_1), model_1, k=10)\n",
    "# uncomment below to view predict_input_1\n",
    "# predict_input_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_input_2 = get_topics(id_to_word_1000train.doc2bow(text_input_2), model_1, k=10)\n",
    "# uncomment below to view predict_input_2\n",
    "# predict_input_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model #2: Genism LDAMallet model\n",
    "Model #2: implementation: Gensim LDAMallet wrapper around LDA Mallet model; \n",
    "          k_topics=5; \n",
    "          n_docs=1000; \n",
    "          partition = 80/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to download Mallet topic model\n",
    "# !wget http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip\n",
    "\n",
    "# update path with location of saved Mallet topic model\n",
    "path_mallet = '/Users/lee/Documents/techniche/techniche/data/mallet-2.0.8/bin/mallet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct model #2\n",
    "model_2 = gensim.models.wrappers.LdaMallet(path_mallet, \n",
    "                                           corpus=corpus_1000train, \n",
    "                                           num_topics=5, \n",
    "                                           id2word=id_to_word_1000train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #2- Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #2 - Evaluate - Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3515133637766028\n"
     ]
    }
   ],
   "source": [
    "# calculate coherence metric for train set (1000 pats dataset)\n",
    "coherence_model_2train = CoherenceModel(model=model_2, \n",
    "                                        texts=processed_docs_1000train,\n",
    "                                        dictionary=id_to_word_1000train,\n",
    "                                        coherence='c_v')\n",
    "coherence_model_2train_get = coherence_model_2train.get_coherence()\n",
    "print(coherence_model_2train_get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6133152288653513\n"
     ]
    }
   ],
   "source": [
    "# calculate coherence metric for test_set (1000 pats dataset)\n",
    "coherence_model_2test = CoherenceModel(model=model_2, \n",
    "                                       texts=processed_docs_1000test, \n",
    "                                       dictionary=id_to_word_1000test, \n",
    "                                       coherence='c_v')\n",
    "coherence_model_2test_get = coherence_model_2test.get_coherence()\n",
    "print(coherence_model_2test_get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate coherence metric for each of n topics in test set\n",
    "coherence_model_2_per_topic = coherence_model_2test.get_coherence_per_topic()\n",
    "# print(coherence_model_2_per_topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model #3: Gensim LDA model\n",
    "Model #3: implementation: Gensim LDAmodel; k_topics=10; n_docs=1000, partition = 80/20\n",
    "This model increases the k_topics from 5 to 10, relative to model #1 above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct model #3\n",
    "model_3 = LdaModel(corpus=corpus_1000train,\n",
    "                   id2word=id_to_word_1000train,\n",
    "                   num_topics=10, \n",
    "                   random_state=100,\n",
    "                   update_every=1,\n",
    "                   chunksize=100,\n",
    "                   passes=10,\n",
    "                   alpha='auto',\n",
    "                   per_word_topics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #3 - Explore and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore topics visually\n",
    "pyLDAvis.enable_notebook()\n",
    "viz_topics_model_3 = pyLDAvis.gensim.prepare(model_3, \n",
    "                                             corpus_1000train, \n",
    "                                             id_to_word_1000train)\n",
    "# viz_topics_model_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keywords in n topics in corpus\n",
    "# uncomment below to view\n",
    "# pprint(model_3.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view most important keywords/weight of topic with idx 0\n",
    "# uncomment below to view\n",
    "# pprint(model_3.print_topic(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #3 - Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #3 - Evaluate - Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3628245805478861\n"
     ]
    }
   ],
   "source": [
    "# calculate coherence metric for train set (1000 pats dataset)\n",
    "coherence_model_3train = CoherenceModel(model=model_3, \n",
    "                                        texts=processed_docs_1000train,\n",
    "                                        dictionary=id_to_word_1000train,\n",
    "                                        coherence='c_v')\n",
    "coherence_model_3train_get = coherence_model_3train.get_coherence()\n",
    "print(coherence_model_3train_get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6081902191345155\n"
     ]
    }
   ],
   "source": [
    "# calculate coherence metric for test_set (1000 pats dataset)\n",
    "coherence_model_3test = CoherenceModel(model=model_3, \n",
    "                                       texts=processed_docs_1000test, \n",
    "                                       dictionary=id_to_word_1000test, \n",
    "                                       coherence='c_v')\n",
    "coherence_model_3test_get = coherence_model_3test.get_coherence()\n",
    "print(coherence_model_3test_get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate coherence metric for each of n topics in test set\n",
    "coherence_model_3_per_topic = coherence_model_3test.get_coherence_per_topic()\n",
    "# print(coherence_model_1_per_topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #3 - Evaluate - Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-7.0676967060500635\n"
     ]
    }
   ],
   "source": [
    "# calculate perplexity metric for model_3 train set\n",
    "perplexity_model_3train = model_3.log_perplexity(corpus_1000train)\n",
    "print(perplexity_model_3train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-9.945866263106378\n"
     ]
    }
   ],
   "source": [
    "# calculate perplexity metric for model_3 test set\n",
    "perplexity_model_3test = model_3.log_perplexity(corpus_1000test)\n",
    "print(perplexity_model_3test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #3 - Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #3 - Predict - Pickle model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # update path with location to save pickled model #3\n",
    "path_pickle_model_3 = '/Users/lee/Documents/techniche/techniche/data/model_3.pkl'\n",
    "\n",
    "# pickle model #3\n",
    "pickle.dump(model_3, open(path_pickle_model_3,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = pickle.load(open(path_pickle_model_3,'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #3 - inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_input_1_model_3 = get_topics(id_to_word_1000train.doc2bow(text_input_1), \n",
    "                                     model_3, \n",
    "                                     k=10)\n",
    "# uncomment below to view predict_input_1_model_3\n",
    "# predict_input_1_model_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_input_2_model_3 = get_topics(id_to_word_1000train.doc2bow(text_input_2),\n",
    "                                     model_3, \n",
    "                                     k=10)\n",
    "# uncomment below to view predict_input_2_model_3\n",
    "# predict_input_2_model_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model #4: Gensim LDA model\n",
    "Model #4: implementation: Gensim LDAmodel; k_topics=15; n_docs=1000, partition = 80/20\n",
    "This model increases the k_topics to 15, relative to model #1 and model #3 above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct model #4\n",
    "model_4 = LdaModel(corpus=corpus_1000train,\n",
    "                   id2word=id_to_word_1000train,\n",
    "                   num_topics=15, \n",
    "                   random_state=100,\n",
    "                   update_every=1,\n",
    "                   chunksize=100,\n",
    "                   passes=10,\n",
    "                   alpha='auto',\n",
    "                   per_word_topics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #4 - Explore and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore topics visually\n",
    "pyLDAvis.enable_notebook()\n",
    "viz_topics_model_4 = pyLDAvis.gensim.prepare(model_4, \n",
    "                                             corpus_1000train, \n",
    "                                             id_to_word_1000train)\n",
    "# viz_topics_model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment below to view keywords in n topics in corpus\n",
    "# pprint(model_4.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment below to view keywords/weights for topic with idx 0)\n",
    "# pprint(model_4.print_topic(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #4 - Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #4 - Evaluate - Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37342906623443217\n"
     ]
    }
   ],
   "source": [
    "# calculate coherence metric for train set (1000 pats dataset)\n",
    "coherence_model_4train = CoherenceModel(model=model_4,\n",
    "                                        texts=processed_docs_1000train,\n",
    "                                        dictionary=id_to_word_1000train,\n",
    "                                        coherence='c_v')\n",
    "coherence_model_4train_get = coherence_model_4train.get_coherence()\n",
    "print(coherence_model_4train_get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO (Lee)\n",
    "# calculate coherence metric for test_set (n = 200 docs/1000 docs total in dataset)\n",
    "# coherence_model_4test = CoherenceModel(model=model_4,\n",
    "#                                        texts=processed_docs_1000test,\n",
    "#                                        dictionary=id_to_word_1000test,\n",
    "#                                        coherence='c_v')\n",
    "# coherence_model_4test_get = coherence_model_4test.get_coherence()\n",
    "# print(coherence_model_4test_get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO (Lee)\n",
    "# calculate coherence metric for each of the n topics in the test set\n",
    "# coherence_model_4_per_topic = coherence_model_4test.get_coherence_per_topic()\n",
    "# print(coherence_model_1_per_topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #4 - Evaluate - Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate perplexity metric for model_1 train set\n",
    "perplexity_model_3train = model_3.log_perplexity(corpus_1000train)\n",
    "print(perplexity_model_3train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate perplexity metric for model_1 test set\n",
    "perplexity_model_3test = model_3.log_perplexity(corpus_1000test)\n",
    "print(perplexity_model_3test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #4 - Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #4 - Predict - Pickle model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle model\n",
    "# # update path with location to save pickled model\n",
    "path_pickle_model_4 = '/Users/lee/Documents/techniche/techniche/data/model_4.pkl'\n",
    "pickle.dump(model_4, open(path_pickle_model_4,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4 = pickle.load(open(path_pickle_model_4,'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #4 - inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_input_1_model_4 = get_topics(id_to_word_1000train.doc2bow(text_input_1), model_4, k=10)\n",
    "# uncomment below to view predict_input_1_model_4\n",
    "# predict_input_1_model_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_input_2_model_4 = get_topics(id_to_word_1000train.doc2bow(text_input_2), model_4, k=10)\n",
    "# uncomment below to view predict_input_2_model_4\n",
    "# predict_input_2_model_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model #5: Author-topic model\n",
    "Model #4: implementation: Gensim AuthorTopicModel; k_topics=15; n_docs=1000, partition = 80/20\n",
    "This model increases the k_topics to 15, relative to model #1 and model #3 above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to view quick visual index to patent number mapping\n",
    "# for i in raw_data_1000:\n",
    "#     print(raw_data_1000.index(i), i['patent_number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO (Lee) review pat_inv_map workflow\n",
    "# partitions data_1000 to size of training set (80/20 split so grabs first 800 rows)\n",
    "data_800 = data_1000[:800]\n",
    "\n",
    "# create inventor-to-doc mapping from original list of dicts in json api response\n",
    "pat2inv = pat_inv_map(data_800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct author-topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct author-topic model\n",
    "model_at = AuthorTopicModel(corpus=corpus_1000train,\n",
    "                            doc2author=pat2inv,\n",
    "                            id2word=id_to_word_1000train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct vectors for authors\n",
    "author_vecs = [model_at.get_author_topics(author) for author in model_at.id2author.values()]\n",
    "author_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve topic distribution for author using use model[name] syntax\n",
    "# each topic has a probability of being expressed given the particular author, \n",
    "# but only the ones above a certain threshold are displayed\n",
    "\n",
    "model_at['7788103-1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def show_author(name):\n",
    "#     print('\\n%s' % name)\n",
    "#     print('Docs:', model.author2doc[name])\n",
    "#     print('Topics:')\n",
    "#     pprint([(topic_labels[topic[0]], topic[1]) for topic in model[name]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build mapping from inventor to patent\n",
    "inv2pat = gensim.models.atmodel.construct_author2doc(pat2inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #X - Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction functions that take input of new text string, and predict topic distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model #6: Gensim LDA model\n",
    "Model #6: implementation: Gensim LDAmodel; k_topics=5; n_docs=1000, partition = 80/20\n",
    "This model decreases the k_topics to 5 for human interpretability in webapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct model #6\n",
    "# TODO (Lee) - resolve deprecation warnings\n",
    "model_6 = LdaModel(corpus=corpus_1000train,\n",
    "                   id2word=id_to_word_1000train,\n",
    "                   num_topics=5, \n",
    "                   random_state=100,\n",
    "                   update_every=1,\n",
    "                   chunksize=100,\n",
    "                   passes=10,\n",
    "                   alpha='auto',\n",
    "                   per_word_topics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #6 - Evaluate - Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate coherence metric for train set ((n = 800 docs/1000 docs total in dataset))\n",
    "coherence_model_6train = CoherenceModel(model=model_6,\n",
    "                                        texts=processed_docs_1000train,\n",
    "                                        dictionary=id_to_word_1000train,\n",
    "                                        coherence='c_v')\n",
    "coherence_model_6train_get = coherence_model_6train.get_coherence()\n",
    "print(coherence_model_6train_get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #6 - Evaluate - Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate perplexity metric for model_6 train set\n",
    "perplexity_model_6train = model_6.log_perplexity(corpus_1000train)\n",
    "print(perplexity_model_6train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate perplexity metric for model_6 test set\n",
    "perplexity_model_6test = model_6.log_perplexity(corpus_1000test)\n",
    "print(perplexity_model_6test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #6 - Predict - Pickle model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle model\n",
    "# # update path with location to save pickled model\n",
    "path_pickle_model_6 = '/Users/lee/Documents/techniche/techniche/data/model_6.pkl'\n",
    "pickle.dump(model_6, open(path_pickle_model_6,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_6 = pickle.load(open(path_pickle_model_6,'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #6 - inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_input_1_model_6 = get_topics(id_to_word_1000train.doc2bow(text_input_1), model_6, k=3)\n",
    "# uncomment below to view predict_input_1_model_6\n",
    "predict_input_1_model_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_input_2_model_6 = get_topics(id_to_word_1000train.doc2bow(text_input_2), model_6, k=10)\n",
    "# uncomment below to view predict_input_2_model_6\n",
    "# predict_input_2_model_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
