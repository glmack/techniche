{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from test_model import (get_patent_fields_list, get_ml_patents, \n",
    "                        create_title_abstract_col,trim_data, \n",
    "                        structure_dataframe, partition_dataframe, \n",
    "                        build_pipeline, process_docs, pat_inv_map, get_topics)\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.corpora import Dictionary, mmcorpus\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models import AuthorTopicModel\n",
    "from gensim.test.utils import common_dictionary, datapath, temporary_file\n",
    "from smart_open import smart_open\n",
    "\n",
    "import spacy\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, punkt, RegexpTokenizer, wordpunct_tokenize\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
    "\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "import calendar\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Lee) - resolve deprecation warnings\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acquire data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# above '%%capture' supresses output of large output of json response\n",
    "\n",
    "# acquire dataset of ML patents in JSON by call to PatentsView API \n",
    "raw_data_1000 = get_ml_patents(pats_per_page=1000)\n",
    "raw_data_2000 = get_ml_patents(pats_per_page=2000)\n",
    "\n",
    "# uncomment to view first dictionary in raw_data_1000\n",
    "# raw_data_1000[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Acquire data - Structure data - 1000 patent documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define desired keys/columns as criteria to subset dataset\n",
    "retained_keys = ['patent_number', 'patent_date', 'patent_title',\n",
    "                 'patent_abstract', 'inventors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset JSON dict dataset of full api response by desired keys/columns\n",
    "data_1000 = trim_data(data=raw_data_1000, keys=retained_keys)\n",
    "\n",
    "# TODO (Lee) review naming conv duplication - create item in dict by concatenating patent_title and patent_abstract\n",
    "data_1000 = create_title_abstract_col(data=data_1000)\n",
    "\n",
    "# convert dataframe from subsetted dict, organize columns and sort by patent_date\n",
    "df_1000 = structure_dataframe(data=data_1000)\n",
    "\n",
    "# partition df_1000 into train and test dataframes\n",
    "data_train_1000, data_test_1000 = partition_dataframe(df_1000, .8)\n",
    "\n",
    "# convert dataframes (full, train, test) to list format required by model\n",
    "text_data_1000 = df_1000.patent_title_abstract.tolist()\n",
    "text_train_1000 = data_train_1000.patent_title_abstract.tolist()\n",
    "text_test_1000 = data_test_1000.patent_title_abstract.tolist()\n",
    "\n",
    "# TODO (Lee) - convert text target in JSON response to list w/o dataframe step\n",
    "text_list_1000 = []\n",
    "for i in data_1000:\n",
    "    text_list_1000.append(i['patent_title_abstract'])\n",
    "# text_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Acquire data - Structure data - 2000 patent documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset dataset of full api response by desired keys/columns\n",
    "data_2000 = trim_data(data=raw_data_2000, keys=retained_keys)\n",
    "\n",
    "# create dict item by concatenating patent_title and patent_abstract\n",
    "data_2000 = create_title_abstract_col(data=data_2000)\n",
    "\n",
    "# create dataframe, organize columns and sort by patent_date\n",
    "df_2000 = structure_dataframe(data=data_2000)\n",
    "\n",
    "# partition dataframe\n",
    "data_train_2000, data_test_2000 = partition_dataframe(df_2000, .8)\n",
    "\n",
    "# convert dataframe to list format required by model\n",
    "text_data_2000 = df_2000.patent_title_abstract.tolist()\n",
    "text_train_2000 = data_train_2000.patent_title_abstract.tolist()\n",
    "text_test_2000 = data_test_2000.patent_title_abstract.tolist()\n",
    "\n",
    "# TODO (Lee) - convert json response to list, w/o dataframe step\n",
    "text_list_2000 = []\n",
    "for i in data_2000:\n",
    "    text_list_2000.append(i['patent_title_abstract'])\n",
    "# text_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'ner']\n"
     ]
    }
   ],
   "source": [
    "# uncomment to download standard stop words from Spacy\n",
    "# !python -m spacy download en\n",
    "\n",
    "# update path with location to save stopwords\n",
    "path_stopwords = '/Users/lee/Documents/techniche/techniche/data/stopwords/english'\n",
    "stop_words = stopwords.words(path_stopwords)\n",
    "\n",
    "# create text pre-processing pipeline to tokenize, clean and lower text\n",
    "nlp = build_pipeline()\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# TODO (Lee) # pre-process documents via json-to-list workflow above\n",
    "# processed_docs_1 = process_docs(text_list)\n",
    "\n",
    "# pre-process documents via json-to-df-to-list workflow above\n",
    "processed_docs_1000train = process_docs(text_train_1000)\n",
    "\n",
    "### Build corpus and dictionary\n",
    "\n",
    "# build dictionary\n",
    "id_to_word_1000train = Dictionary(processed_docs_1000train)\n",
    "\n",
    "# save/pickle dictionary for subsequent use\n",
    "# update path with location to save pickled model\n",
    "path_pickle_id_to_word = '/Users/lee/Documents/techniche/techniche/data/id_to_word_1000train.pkl'\n",
    "pickle.dump(id_to_word_1000train, open(path_pickle_id_to_word,'wb'))\n",
    "\n",
    "# apply term-doc freq (list of (token_id, token_count) tuples) to docs\n",
    "corpus_1000train = [id_to_word_1000train.doc2bow(doc) for doc in processed_docs_1000train]\n",
    "\n",
    "# uncomment below to create/view formatted corpus\n",
    "# formatted_corpus_1000 = [[(id_to_word[id], freq) for id, freq in text] for text in corpus_1000train]\n",
    "# formatted_corpus_1000\n",
    "# id_to_word_1000train.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_word_1000train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model #1: Genism LDA model\n",
    "Model #1: implementation: Gensim LDAmodel; k_topics=5; n_docs=1000, partition = 80/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct model #1\n",
    "# TODO (Lee) - resolve deprecation warnings\n",
    "model_1 = LdaModel(corpus=corpus_1000train,\n",
    "                   id2word=id_to_word_1000train,\n",
    "                   num_topics=5, \n",
    "                   random_state=100,\n",
    "                   update_every=1,\n",
    "                   chunksize=100,\n",
    "                   passes=10,\n",
    "                   alpha='auto',\n",
    "                   per_word_topics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #1 - Explore and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore topics visually\n",
    "pyLDAvis.enable_notebook()\n",
    "viz_topics_model_1 = pyLDAvis.gensim.prepare(model_1, corpus_1000train, id_to_word_1000train)\n",
    "# viz_topics_model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment below to view keywords in n topics in corpus\n",
    "# pprint(model_1.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment below to view most important keywords and weight of given topic w/ idx 0\n",
    "# pprint(model_1.print_topic(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #1 - Evaluate\n",
    "We evaluate models using coherence and perplexity metrics. As unsupervised learning task, no labels with which to evaluate the \"expected\" prediction. There is an open research agenda on LDA evaluation approaches (intrinsic vs extrinsic; machine vs human-interpretable, etc., task-specific). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #1 - Evaluate - Pre-process test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Lee) pre-process 1000 patents from df-to-list worfklow above\n",
    "processed_docs_1000test = process_docs(text_test_1000)\n",
    "\n",
    "# build dictionary with dataset of 1000 patents\n",
    "id_to_word_1000test = Dictionary(processed_docs_1000test)\n",
    "\n",
    "# apply term-doc frequency (list of (token_id, token_count) tuples) to 1000 patents\n",
    "corpus_1000test = [id_to_word_1000test.doc2bow(doc) for doc in processed_docs_1000test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #1 - Evaluate - Coherence\n",
    "Calculate topic coherence for topic models with 4-step coherence pipeline (segmentation, probability estimation, confirmation measure, aggregation) from Roeder et al., 2015. \"Exploring the space of topic coherence measures\", WSDM '15 Proceedings of the Eighth ACM International Conference on Web Search and Data Mining (WSDM) 2015, 399-408."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate coherence metric for train set ((n = 800 docs/1000 docs total in dataset))\n",
    "coherence_model_1train = CoherenceModel(model=model_1, \n",
    "                                        texts=processed_docs_1000train,\n",
    "                                        dictionary=id_to_word_1000train,\n",
    "                                        coherence='c_v')\n",
    "coherence_model_1train_get = coherence_model_1train.get_coherence()\n",
    "print(coherence_model_1train_get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate coherence metric for test_set (n = 200 docs/1000 docs total in dataset)\n",
    "coherence_model_1test = CoherenceModel(model=model_1, \n",
    "                                       texts=processed_docs_1000test, \n",
    "                                       dictionary=id_to_word_1000test, \n",
    "                                       coherence='c_v')\n",
    "coherence_model_1test_get = coherence_model_1test.get_coherence()\n",
    "print(coherence_model_1test_get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate coherence metric for each of the n topics in the test set\n",
    "coherence_model_1_per_topic = coherence_model_1test.get_coherence_per_topic()\n",
    "\n",
    "# uncomment below to print coherence_model_1_per_topic\n",
    "# print(coherence_model_1_per_topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #1 - Evaluate - Perplexity\n",
    "Calculate perplexity metric. Metric calculates and returns per-word likelihood bound using a chunk of documents as evaluation corpus. Output calculated statistics, including the perplexity=2^(-bound), to log at INFO level. Returns the variational bound score calculated for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate perplexity metric for model_1 train set (1000 patents)\n",
    "perplexity_model_1train = model_1.log_perplexity(corpus_1000train)\n",
    "print(perplexity_model_1train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate perplexity metric for model_1 test set (1000 patents)\n",
    "perplexity_model_1test = model_1.log_perplexity(corpus_1000test)\n",
    "print(perplexity_model_1test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #1 - Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #1 - Predict - Pickle model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save/pickle model #1 for subsequent use\n",
    "# update path with location to save pickled model\n",
    "path_pickle_model_1 = '/Users/lee/Documents/techniche/techniche/data/model_lda_1.pkl'\n",
    "pickle.dump(model_1, open(path_pickle_model_1,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncommment below to load pickled model #1\n",
    "# model_1 = pickle.load(open(path_pickle_model_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #1 - inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test model #1 on 2 new text w/ two strings\n",
    "\n",
    "# define example text_input #1, expressing keyword-type search\n",
    "text_input_1 = 'smart assistant transformer model translation'.split()\n",
    "\n",
    "# define example text_input #1, expressing technical details in job post\n",
    "text_input_2 = \"\"\"At the Siri International team within Apple we bring the\n",
    "Siri intelligent assistant to our customers worldwide in over 40 languages\n",
    "and dialects. Join us, and tackle some of the most challenging problems in\n",
    "natural language processing and large scale applied machine learning. You \n",
    "will build cutting edge natural language understanding technologies and \n",
    "deploy them on a global scale. Your work will advance and shape the future\n",
    "vision of our multi-lingual, multi-cultural Siri assistant, and Search \n",
    "applications used by millions across the world Key Qualifications Extensive\n",
    "track record of scientific research in NLP and Machine Learning, or similar\n",
    "experience in developing language technologies for shipping products.\n",
    "Strong coding and software engineering skills in a mainstream programming \n",
    "language, such as Python, Java, C/C++. Familiarity with NLP/ML tools and \n",
    "packages like Caffe, pyTorch, TensorFlow, Weka, scikit-learn, nltk, etc.\n",
    "Practical experience building production quality applications related to \n",
    "natural language processing and machine learning. In-depth knowledge of \n",
    "machine learning algorithms and ability to apply them in data driven natural\n",
    "language processing systems. Ability to quickly prototype ideas / solutions,\n",
    "perform critical analysis, and use creative approaches for solving complex \n",
    "problems. Attention to detail and excellent communication skills. Description\n",
    "We are looking for a highly motivated technologist with a strong background \n",
    "in Natural Language Processing and Machine Learning research. The ideal \n",
    "candidate will have a strong track record of taking research ideas to \n",
    "real-world applications. In this position you will apply your problem solving\n",
    "skills to challenges and opportunities within Siri International, which \n",
    "involves development of large-scale language technologies for many natural\n",
    "languages worldwide. The primary responsibility of this role is to conduct\n",
    "research and develop innovative machine learning, artificial intelligence \n",
    "and NLP solutions for multi-lingual conversational agents. You will have \n",
    "the opportunity to investigate cutting edge research methods that will \n",
    "improve customer experience of our products and enable our engineers to \n",
    "scale these technologies across a variety of natural languages. You will \n",
    "also provide technical leadership and experiment-driven insights for \n",
    "engineering teams on their machine learning modeling and data decisions. \n",
    "You will play a central role in defining the future technical directions \n",
    "of Siri International through quick prototyping, critical analysis and \n",
    "development of core multi-lingual NLP technologies. Education & Experience\n",
    "PhD in Machine Learning, Statistics, Computer Science, Mathematics or \n",
    "related field with specialization in natural language processing and/or \n",
    "machine learning, OR * Masters degree in a related field with a strong \n",
    "academic/industrial track record. * Hands-on research experience in an \n",
    "academic or industrial setting.\"\"\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# pass text through pre-process pipeline\n",
    "id_to_word_1000train.doc2bow(text_input_1)\n",
    "id_to_word_1000train.doc2bow(text_input_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_input_1 = get_topics(id_to_word_1000train.doc2bow(text_input_1), model_1, k=10)\n",
    "# uncomment below to view predict_input_1\n",
    "# predict_input_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_input_2 = get_topics(id_to_word_1000train.doc2bow(text_input_2), model_1, k=10)\n",
    "# uncomment below to view predict_input_2\n",
    "# predict_input_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model #2: Genism LDAMallet model\n",
    "Model #2: implementation: Gensim LDAMallet wrapper around LDA Mallet model; \n",
    "          k_topics=5; \n",
    "          n_docs=1000; \n",
    "          partition = 80/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to download Mallet topic model\n",
    "# !wget http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip\n",
    "# update path\n",
    "path_mallet = '/Users/lee/Documents/techniche/techniche/data/mallet-2.0.8/bin/mallet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct model #2\n",
    "# TODO (Lee) - resolve deprecation warnings\n",
    "model_2 = gensim.models.wrappers.LdaMallet(path_mallet, \n",
    "                                           corpus=corpus_1000train, \n",
    "                                           num_topics=5, \n",
    "                                           id2word=id_to_word_1000train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #2 - Explore and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Lee) - visualization with LDA Mallet\n",
    "# explore topics visually\n",
    "# pyLDAvis.enable_notebook()\n",
    "# viz_topics_model_2 = pyLDAvis.prepare(model_2, corpus_1000train, id_to_word_1000train)\n",
    "# viz_topics_model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment below to view keywords in n topics in corpus\n",
    "# pprint(model_2.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment below to view most important keywords and weight for given topic (idx 0)\n",
    "# uncomment below to view\n",
    "# pprint(model_2.print_topic(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #2- Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #2 - Evaluate - Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate coherence metric for train set ((n = 800 docs/1000 docs total in dataset))\n",
    "coherence_model_2train = CoherenceModel(model=model_2, \n",
    "                                        texts=processed_docs_1000train,\n",
    "                                        dictionary=id_to_word_1000train,\n",
    "                                        coherence='c_v')\n",
    "coherence_model_2train_get = coherence_model_2train.get_coherence()\n",
    "print(coherence_model_2train_get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate coherence metric for test_set (n = 200 docs/1000 docs total in dataset)\n",
    "coherence_model_2test = CoherenceModel(model=model_2, \n",
    "                                       texts=processed_docs_1000test, \n",
    "                                       dictionary=id_to_word_1000test, \n",
    "                                       coherence='c_v')\n",
    "coherence_model_2test_get = coherence_model_2test.get_coherence()\n",
    "print(coherence_model_2test_get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate coherence metric for each of the n topics in the test set\n",
    "coherence_model_2_per_topic = coherence_model_2test.get_coherence_per_topic()\n",
    "# print(coherence_model_2_per_topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model #3: Gensim LDA model\n",
    "Model #1: implementation: Gensim LDAmodel; k_topics=10; n_docs=1000, partition = 80/20\n",
    "This model increases the k_topics from 5 to 10, relative to model #1 above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct model #3\n",
    "# TODO (Lee) - resolve deprecation warnings\n",
    "model_3 = LdaModel(corpus=corpus_1000train,\n",
    "                   id2word=id_to_word_1000train,\n",
    "                   num_topics=10, \n",
    "                   random_state=100,\n",
    "                   update_every=1,\n",
    "                   chunksize=100,\n",
    "                   passes=10,\n",
    "                   alpha='auto',\n",
    "                   per_word_topics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #3 - Explore and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore topics visually\n",
    "pyLDAvis.enable_notebook()\n",
    "viz_topics_model_3 = pyLDAvis.gensim.prepare(model_3, corpus_1000train, id_to_word_1000train)\n",
    "# viz_topics_model_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keywords in n topics in corpus\n",
    "# uncomment below to view\n",
    "# pprint(model_3.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most important keywords, and the respective weight, that form topic with index 0\n",
    "# uncomment below to view\n",
    "# pprint(model_3.print_topic(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #3 - Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #3 - Evaluate - Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate coherence metric for train set ((n = 800 docs/1000 docs total in dataset))\n",
    "coherence_model_3train = CoherenceModel(model=model_3, \n",
    "                                        texts=processed_docs_1000train,\n",
    "                                        dictionary=id_to_word_1000train,\n",
    "                                        coherence='c_v')\n",
    "coherence_model_3train_get = coherence_model_3train.get_coherence()\n",
    "print(coherence_model_3train_get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate coherence metric for test_set (n = 200 docs/1000 docs total in dataset)\n",
    "coherence_model_3test = CoherenceModel(model=model_3, \n",
    "                                       texts=processed_docs_1000test, \n",
    "                                       dictionary=id_to_word_1000test, \n",
    "                                       coherence='c_v')\n",
    "coherence_model_3test_get = coherence_model_3test.get_coherence()\n",
    "print(coherence_model_3test_get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate coherence metric for each of the n topics in the test set\n",
    "coherence_model_3_per_topic = coherence_model_3test.get_coherence_per_topic()\n",
    "# print(coherence_model_1_per_topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #3 - Evaluate - Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate perplexity metric for model_1 train set\n",
    "perplexity_model_3train = model_3.log_perplexity(corpus_1000train)\n",
    "print(perplexity_model_3train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate perplexity metric for model_1 test set\n",
    "perplexity_model_3test = model_3.log_perplexity(corpus_1000test)\n",
    "print(perplexity_model_3test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #3 - Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #3 - Predict - Pickle model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle model #3\n",
    "# # update path with location to save pickled model\n",
    "path_pickle_model_3 = '/Users/lee/Documents/techniche/techniche/data/model_3.pkl'\n",
    "pickle.dump(model_3, open(path_pickle_model_3,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = pickle.load(open(path_pickle_model_3,'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #3 - inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_input_1_model_3 = get_topics(id_to_word_1000train.doc2bow(text_input_1), model_3, k=10)\n",
    "# uncomment below to view predict_input_1_model_3\n",
    "# predict_input_1_model_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_input_2_model_3 = get_topics(id_to_word_1000train.doc2bow(text_input_2), model_3, k=10)\n",
    "# uncomment below to view predict_input_2_model_3\n",
    "# predict_input_2_model_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model #4: Gensim LDA model\n",
    "Model #4: implementation: Gensim LDAmodel; k_topics=15; n_docs=1000, partition = 80/20\n",
    "This model increases the k_topics to 15, relative to model #1 and model #3 above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct model #4\n",
    "# TODO (Lee) - resolve deprecation warnings\n",
    "model_4 = LdaModel(corpus=corpus_1000train,\n",
    "                   id2word=id_to_word_1000train,\n",
    "                   num_topics=15, \n",
    "                   random_state=100,\n",
    "                   update_every=1,\n",
    "                   chunksize=100,\n",
    "                   passes=10,\n",
    "                   alpha='auto',\n",
    "                   per_word_topics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #4 - Explore and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore topics visually\n",
    "pyLDAvis.enable_notebook()\n",
    "viz_topics_model_4 = pyLDAvis.gensim.prepare(model_4, corpus_1000train, id_to_word_1000train)\n",
    "# viz_topics_model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment below to view keywords in n topics in corpus\n",
    "# pprint(model_4.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment below to view most important keywords and weight for given topic (idx 0)\n",
    "# pprint(model_4.print_topic(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #4 - Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #4 - Evaluate - Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate coherence metric for train set ((n = 800 docs/1000 docs total in dataset))\n",
    "coherence_model_4train = CoherenceModel(model=model_4,\n",
    "                                        texts=processed_docs_1000train,\n",
    "                                        dictionary=id_to_word_1000train,\n",
    "                                        coherence='c_v')\n",
    "coherence_model_4train_get = coherence_model_4train.get_coherence()\n",
    "print(coherence_model_4train_get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO (Lee)\n",
    "# calculate coherence metric for test_set (n = 200 docs/1000 docs total in dataset)\n",
    "# coherence_model_4test = CoherenceModel(model=model_4,\n",
    "#                                        texts=processed_docs_1000test,\n",
    "#                                        dictionary=id_to_word_1000test,\n",
    "#                                        coherence='c_v')\n",
    "# coherence_model_4test_get = coherence_model_4test.get_coherence()\n",
    "# print(coherence_model_4test_get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO (Lee)\n",
    "# calculate coherence metric for each of the n topics in the test set\n",
    "# coherence_model_4_per_topic = coherence_model_4test.get_coherence_per_topic()\n",
    "# print(coherence_model_1_per_topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #4 - Evaluate - Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate perplexity metric for model_1 train set\n",
    "perplexity_model_3train = model_3.log_perplexity(corpus_1000train)\n",
    "print(perplexity_model_3train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate perplexity metric for model_1 test set\n",
    "perplexity_model_3test = model_3.log_perplexity(corpus_1000test)\n",
    "print(perplexity_model_3test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #4 - Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #4 - Predict - Pickle model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle model\n",
    "# # update path with location to save pickled model\n",
    "path_pickle_model_4 = '/Users/lee/Documents/techniche/techniche/data/model_4.pkl'\n",
    "pickle.dump(model_4, open(path_pickle_model_4,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4 = pickle.load(open(path_pickle_model_4,'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #4 - inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_input_1_model_4 = get_topics(id_to_word_1000train.doc2bow(text_input_1), model_4, k=10)\n",
    "# uncomment below to view predict_input_1_model_4\n",
    "# predict_input_1_model_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_input_2_model_4 = get_topics(id_to_word_1000train.doc2bow(text_input_2), model_4, k=10)\n",
    "# uncomment below to view predict_input_2_model_4\n",
    "# predict_input_2_model_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model #5: Author-topic model\n",
    "Model #4: implementation: Gensim AuthorTopicModel; k_topics=15; n_docs=1000, partition = 80/20\n",
    "This model increases the k_topics to 15, relative to model #1 and model #3 above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to view quick visual index to patent number mapping\n",
    "# for i in raw_data_1000:\n",
    "#     print(raw_data_1000.index(i), i['patent_number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO (Lee) review pat_inv_map workflow\n",
    "# partitions data_1000 to size of training set (80/20 split so grabs first 800 rows)\n",
    "data_800 = data_1000[:800]\n",
    "\n",
    "# create inventor-to-doc mapping from original list of dicts in json api response\n",
    "pat2inv = pat_inv_map(data_800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct author-topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct author-topic model\n",
    "model_at = AuthorTopicModel(corpus=corpus_1000train,\n",
    "                            doc2author=pat2inv,\n",
    "                            id2word=id_to_word_1000train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct vectors for authors\n",
    "author_vecs = [model_at.get_author_topics(author) for author in model_at.id2author.values()]\n",
    "author_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve topic distribution for author using use model[name] syntax\n",
    "# each topic has a probability of being expressed given the particular author, \n",
    "# but only the ones above a certain threshold are displayed\n",
    "\n",
    "model_at['7788103-1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def show_author(name):\n",
    "#     print('\\n%s' % name)\n",
    "#     print('Docs:', model.author2doc[name])\n",
    "#     print('Topics:')\n",
    "#     pprint([(topic_labels[topic[0]], topic[1]) for topic in model[name]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build mapping from inventor to patent\n",
    "inv2pat = gensim.models.atmodel.construct_author2doc(pat2inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #X - Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction functions that take input of new text string, and predict topic distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Appendix - Model #1 - Evaluate - holdout set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Lee) - evaluate on 1k documents **not** used in LDA training\n",
    "doc_stream = (tokens for _, tokens in iter_wiki('./data/simplewiki-20140623-pages-articles.xml.bz2'))  # generator\n",
    "test_docs = list(itertools.islice(doc_stream, 8000, 9000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Appendix - Model #1 - Evaluate - Doc split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Lee) - split each document into two parts, and check that 1) topics of the first half are similar to \n",
    "topics of the second 2) halves of different documents are mostly dissimilar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Lee)\n",
    "def intra_inter(model, test_docs, num_pairs=10000):\n",
    "    # split each test document into two halves and compute topics for each half\n",
    "    part1 = [model[id2word.doc2bow(tokens[: len(tokens) / 2])] for tokens in test_docs]\n",
    "    part2 = [model[id2word.doc2bow(tokens[len(tokens) / 2 :])] for tokens in test_docs]\n",
    "    \n",
    "    # print computed similarities (uses cossim)\n",
    "    print(\"average cosine similarity between corresponding parts (higher is better):\")\n",
    "    print(np.mean([gensim.matutils.cossim(p1, p2) for p1, p2 in zip(part1, part2)]))\n",
    "\n",
    "    random_pairs = np.random.randint(0, len(test_docs), size=(num_pairs, 2))\n",
    "    print(\"average cosine similarity between 10,000 random parts (lower is better):\")    \n",
    "    print(np.mean([gensim.matutils.cossim(part1[i[0]], part2[i[1]]) for i in random_pairs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Lee)\n",
    "print(\"LDA results:\")\n",
    "intra_inter(lda_model, test_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Appendix - Model #1 - Evaluate - Log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Appendix - Model #1 - Evaluate - Alternate unimplemented workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Lee)\n",
    "# def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "#     \"\"\"\n",
    "#     Compute c_v coherence for various number of topics\n",
    "\n",
    "#     Parameters:\n",
    "#     ----------\n",
    "#     dictionary : Gensim dictionary\n",
    "#     corpus : Gensim corpus\n",
    "#     texts : List of input texts\n",
    "#     limit : Max num of topics\n",
    "\n",
    "#     Returns:\n",
    "#     -------\n",
    "#     model_list : List of LDA topic models\n",
    "#     coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "#     \"\"\"\n",
    "#     coherence_values = []\n",
    "#     model_list = []\n",
    "#     for num_topics in range(start, limit, step):\n",
    "#         model = gensim.models.wrappers.LdaMallet(path_mallet, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "#         model_list.append(model)\n",
    "#         coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "#         coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "#     return model_list, coherence_values\n",
    "\n",
    "# model_list, coherence_values = compute_coherence_values(dictionary=id_to_word, corpus=corpus, texts=data, start=2, limit=40, step=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Appendix - Model 1 - Inference - Alternate workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `get_document_topics()` returns topic probability distribution for given document\n",
    "# topic_dist_675_a = model_lda.get_document_topics(corpus[50])\n",
    "# pprint(sorted(topic_dist_50_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topicid = 3\n",
    "# model_lda.get_topic_terms(topicid, topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_train[doc_id]\n",
    "# doc_id = 675\n",
    "# topic_dist_675_b = sorted(get_topics(corpus[doc_id], k=10)), text_train[doc_id]\n",
    "# pprint(topic_dist_675_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Gensim example - Alternate predict workflow - Create a new corpus, made of previously unseen documents.\n",
    "# other_texts = [\n",
    "#      ['computer', 'time', 'graph'],\n",
    "#      ['survey', 'response', 'eps'],\n",
    "#      ['human', 'system', 'computer']\n",
    "#  ]\n",
    "# other_corpus = [common_dictionary.doc2bow(text) for text in other_texts]\n",
    "# unseen_doc = other_corpus[0]\n",
    "# vector = lda[unseen_doc]  # get topic probability distribution for a document\n",
    "# Update the model by incrementally training on the new corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print keywords in n topics\n",
    "sorted(model_lda.show_topics(), key=lambda x: x[1])\n",
    "\n",
    "# print keywords in n topics\n",
    "sorted(model_lda.print_topics(), key=lambda x: x[1])\n",
    "\n",
    "# print keywords in n topics\n",
    "sorted(model_l.print_topics(), key=lambda x: x[1])\n",
    "\n",
    "# print keywords in n topics\n",
    "sorted(model_1.print_topics(), key=lambda x: x[0])\n",
    "\n",
    "# show_topic() returns n most important/relevant words, and their weights, that comprise given topic\n",
    "pprint(model_1.show_topic(1, topn=10))\n",
    "\n",
    "pprint(model_1.show_topics(num_topics=5, num_words=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Appendix - Model #2 - Evaluate - Perplexity\n",
    "No implementation of log_perplexity method for LDAMallet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Appendix - Train Model #5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Appendix - this was alternative partial workflow to create inv:doc mapping in model #5\n",
    "# uncomment to construct inventor-to-doc mapping as df from nested inventors column in json api response\n",
    "df_inventors = json_normalize(raw_data_1000, record_path=['inventors'], meta=['patent_number', 'patent_date'])\n",
    "df_inventors = df_inventors[['inventor_id', 'patent_number', 'patent_date']]\n",
    "df_inventors.sort_values(by=['patent_date'])\n",
    "df_inventors.head(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
