{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from test_model import (get_patent_fields_list, get_ml_patents, \n",
    "                        create_title_abstract_col,trim_data, \n",
    "                        structure_dataframe, partition_dataframe, \n",
    "                        build_pipeline, process_docs)# TODO (Lee) resolve\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.corpora import Dictionary, mmcorpus\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models import AuthorTopicModel\n",
    "from gensim.test.utils import common_dictionary, datapath, temporary_file\n",
    "from smart_open import smart_open\n",
    "\n",
    "import spacy\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, punkt, RegexpTokenizer, wordpunct_tokenize\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
    "\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "import calendar\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acquire data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# acquire dataset of ML patents by making api call from PatentsView API \n",
    "raw_data = get_ml_patents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify fields (key:val pairs) to retain from full set of returned fields from api call\n",
    "retained_keys = ['patent_number', 'patent_date', 'patent_title', 'patent_abstract', 'inventors']\n",
    "data = trim_data(data=raw_data, keys=retained_keys)\n",
    "\n",
    "# create new key:value pair from combined values of patent_title and patent_abstract keys\n",
    "data = create_title_abstract_col(data=data)\n",
    "\n",
    "# create dataframe, organize columns and sort by patent_date\n",
    "df = structure_dataframe(data=data)\n",
    "\n",
    "#### Partition data\n",
    "\n",
    "# partition data\n",
    "data_train, data_test = partition_dataframe(df, .8)\n",
    "\n",
    "# convert dataframe to list\n",
    "text_data = df.patent_title_abstract.tolist()\n",
    "text_train = data_train.patent_title_abstract.tolist()\n",
    "text_train = data_test.patent_title_abstract.tolist()\n",
    "\n",
    "### Pre-process text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = []\n",
    "for i in data:\n",
    "    text_list.append(i['patent_title_abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Statistical, noninvasive measurement of a patient's physiological state. Tools and techniques for the rapid, continuous, invasive and/or noninvasive measurement, estimation, and/or prediction of a patient's physiological state. In an aspect, some tools and techniques can estimate predict the onset of conditions intracranial pressure, an amount of blood volume loss, cardiovascular collapse, and/or dehydration. Some tools can recommend (and, in some cases, administer) a therapeutic treatment for the patient's condition. In another aspect, some techniques employ high speed software technology that enables active, long term learning from extremely large, continually changing datasets. In some cases, this technology utilizes feature extraction, state-of-the-art machine learning and/or statistical methods to autonomously build and apply relevant models in real-time.\",\n",
       " 'Determining a health condition of a structure. The disclosure relates to structural health monitoring (SHM). In particular determining a health condition of a structure, such as a bridge, based on vibration data measured of the bridge. Measured vibration data is calibrated (410-450). Features are then extracted from the calibrated data (610-630) and a support vector machine classifier is then applied (720) to the extracted features to determine (730) the health condition of a part of the structure. Training of the support vector machine classifier by a machine learning process (910) is also described.',\n",
       " 'Generating simulated sensor data for training and validation of detection models. A scenario is defined that including models of vehicles and a typical driving environment. A model of a subject vehicle is added to the scenario and sensor locations are defined on the subject vehicle. Perception of the scenario by sensors at the sensor locations is simulated to obtain simulated sensor outputs. The simulated sensor outputs are annotated to indicate the location of obstacles in the scenario. The annotated sensor outputs may then be used to validate a statistical model or to train a machine learning model. The simulates sensor outputs may be modeled with sufficient detail to include sensor noise or may include artificially added noise to simulate real world conditions.',\n",
       " 'Hybrid parallelization strategies for machine learning programs on top of mapreduce. Parallel execution of machine learning programs is provided. Program code is received. The program code contains at least one parallel for statement having a plurality of iterations. A parallel execution plan is determined for the program code. According to the parallel execution plan, the plurality of iterations is partitioned into a plurality of tasks. Each task comprises at least one iteration. The iterations of each task are independent. Data required by the plurality of tasks is determined. An access pattern by the plurality of tasks of the data is determined. The data is partitioned based on the access pattern.',\n",
       " \"Peripheral device support with a digital assistant for operating system upgrades. A digital assistant supported across computing devices is configured to interact with an operating system (OS) upgrade system so that various user experiences, services, content, or features associated with support for peripheral devices during an OS upgrade of a computing device can be provided by the digital assistant and rendered as a native digital assistant user experience. The digital assistant is configured to surface a notification through a user interface (UI) when an OS upgrade is available for a user's computing device and recommended for installation. The OS upgrade system executes a confidence model in a machine learning system using real world crowd-sourced data to make predictions of successful post-upgrade operations of peripheral devices with an associated level of confidence. The digital assistant personalizes the OS upgrade notification to the user based on the configuration of computing and peripheral devices, applicable context, and the confidence level.\",\n",
       " 'Initializing a workspace for building a natural language understanding system. Designing a natural language understanding (NLU) model for an application from scratch can be difficult for non-experts. A system can simplify the design process by providing an interface allowing a designer to input example usage sentences and build an NLU model based on presented matches to those example sentences. In one embodiment, a method for initializing a workspace for building an NLU system includes parsing a sample sentence to select at least one candidate stub grammar from among multiple candidate stub grammars. The method can include presenting, to a user, respective representations of the candidate stub grammars selected by the parsing of the sample sentence. The method can include enabling the user to choose one of the respective representations of the candidate stub grammars. The method can include adding to the workspace a stub grammar corresponding to the representation of the candidate stub grammar chosen by the user.',\n",
       " 'Allowing spelling of arbitrary words. Methods, systems, and apparatus, including computer programs encoded on computer storage media, for natural language processing. One of the methods includes receiving a first voice input from a user device; generating a first recognition output; receiving a user selection of one or more terms in the first recognition output; receiving a second voice input spelling a correction of the user selection; determining a corrected recognition output for the selected portion; and providing a second recognition output that merges the first recognition output and the corrected recognition output.',\n",
       " 'Leveraging content dimensions during the translation of human-readable languages. A content management system (CMS) and a translation management system (TMS) can utilize content dimensions for content items to manage and translate the content items between languages. Machine and human translations of complex dynamic content can also be improved by pre-rendering the content to remove localization-related syntax prior to machine or human translation. Content items can also be scored as to their suitability for localization prior to translation, and translation can be skipped for content items that do not have a sufficiently high score. Semantic and natural language processing (NLP) techniques can also be utilized for content categorization and routing. Translations of content items can also be continuously refined and higher quality re-translated content can be provided in an automated fashion.',\n",
       " 'Multi-language support for interfacing with distributed data. A data analysis system stores in-memory representation of a distributed data structure across a plurality of processors of a parallel or distributed system. Client applications interact with the in-memory distributed data structure to process queries using the in-memory distributed data structure and to modify the in-memory distributed data structure. The data analysis system creates uniform resource identifier (URI) to identify each in-memory distributed data structure. The URI can be communicated from one client application to another application using communication mechanisms outside the data analysis system, for example, by email, thereby allowing other client devices to interact with a particular in-memory distributed data structure. The in-memory distributed data structure can be a machine learning model that is trained by one client device and executed by another client device. A client application can interact with the in-memory distributed data structure using different programming languages.',\n",
       " 'Using priority scores for iterative precision reduction in structured lookups for questions. An approach is provided in which a knowledge manager matches a question to multiple natural language contexts that each correspond to relations associated with entities in a structured resource. The knowledge manager identifies database queries corresponding to the multiple natural language contexts and assigns priority scores to the database queries based upon their relative specificity. In turn, the knowledge manager invokes one of the database queries based upon the assigned priority scores.',\n",
       " 'Systems and methods for generating responses to natural language queries. Computer-implemented systems and methods are provided for analyzing and responding to a query from a user. Consistent with certain embodiments, systems and methods are provided for receiving a query from the user and dividing the query into query segments based on a set of grammar rules. Further, systems and methods are provided for selecting a first segment from the query segments, receiving at least one tuple stored in association with the user, selecting a second segment from the at least one tuple. Additionally, systems and methods are provided for receiving information related to the first and second segments, and generating a response to the query based on the received information. In addition, systems and methods are provided for transmitting information to a display device for presenting the response to the user.',\n",
       " 'System for determination of automated response follow-up. Aspects include determination of automated response follow-up. A response to a question is received at a response follow-up system. The response follow-up system analyzes the response using natural language processing to identify one or more response terms. The response follow-up system determines one or more follow-up questions based on the one or more response terms. The response follow-up system modifies an aspect of a user interface based on the one or more follow-up questions.',\n",
       " 'System for generation of automated response follow-up. Aspects include generation of automated response follow-up. A response to a question is received at a response follow-up system. The response follow-up system analyzes the response using natural language processing to identify one or more response terms. The response follow-up system generates one or more follow-up questions based on the one or more response terms. The response follow-up system modifies an aspect of a user interface based on the one or more follow-up questions.',\n",
       " 'High-capacity machine learning system. The present disclosure is directed to a high-capacity training and prediction machine learning platform that can support high-capacity parameter models (e.g., with 10 billion weights). The platform implements a generic feature transformation layer for joint updating and a distributed training framework utilizing shard servers to increase training speed for the high-capacity model size. The models generated by the platform can be utilized in conjunction with existing dense baseline models to predict compatibilities between different groupings of objects (e.g., a group of two objects, three objects, etc.).',\n",
       " 'Machine learning of predictive models using partial regression trends. An input is selected from a set of inputs used by a prediction model to produce an initial predicted value of an outcome. A changed predicted value of the outcome is produced by removing the selected input from the inputs to the model. An actual value of the outcome is obtained. A label residual is computed using the actual value and the changed predicted value. A second prediction model is formed to predict a value of the selected input. A variable residual is computed using an actual value and the predicted value of the selected input. An expression is generated of a plot of the label residual and the variable residual. The selected input is transformed, to form a transformed selected input, where the model produces a second predicted value of the outcome by using the transformed selected input.',\n",
       " 'System and method for providing follow-up responses to prior natural language inputs of a user. In certain implementations, follow-up responses may be provided for prior natural language inputs of a user. As an example, a natural language input associated with a user may be received at a computer system. A determination of whether information sufficient for providing an adequate response to the natural language input is currently accessible to the computer system may be effectuated. A first response to the natural language input (that indicates that a follow-up response will be provided) may be provided based on a determination that information sufficient for providing an adequate response to the natural language input is not currently accessible. Information sufficient for providing an adequate response to the natural language input may be received. A second response to the natural language input may then be provided based on the received sufficient information.',\n",
       " 'Device-described natural language control. A remote device has an associated natural language description that includes a record of commands supported by the remote device. This record of commands includes command names, the command functions to which those names correspond, and natural language strings that are the natural language words or phrases that correspond to the command. A computing device includes a device control module that obtains the natural language description for the remote device and provides the natural language strings to a natural language assistant on the computing device. The natural language assistant monitors the natural language inputs to the computing device, and notifies the device control module when a natural language input matches one of the natural language strings. The device control module uses the natural language description to determine the command name that corresponds to the matching natural language string, and communicates the command name to the remote device.',\n",
       " 'Natural language user interface for computer-aided design systems. A natural language user interface for computer-aided design systems (CAD) comprises a natural language command module including a parser, language database and a CAD model analyzer, and a natural language server module including a second, increased capability parser, a second, preferably larger language database and a CAD context database. The CAD model analyzer analyzes and retrieves associated CAD model information related to a parsed voice command and the CAD context database provides specific CAD related contextual information to facilitate parsing and interpreting CAD specific commands. The natural language server program module may also include an artificial intelligence based query generator and communicate through a network or cloud with resource providers such as third party market places or suppliers to generate queries for retrieval of third party supplied information necessary to respond to or execute CAD specific voice commands.',\n",
       " \"Contextual entity resolution. Methods and systems for resolving entities using multi-modal functionality are described herein. Voice activated electronic devices may, in some embodiments, be capable of displaying content using a display screen. Contextual metadata representing the content rendered by the display screen may describe entities having similar attributes as an identified intent from natural language understanding processing. When natural language understanding processing attempts to resolve one or more declared slots for a particular intent, matching slots from the contextual metadata may be determined, and the matching entities may be placed in an intent selected context file to be included with the natural language understanding's output data. The output data may be provided to a corresponding application for causing one or more actions to be performed.\",\n",
       " 'Scalable endpoint-dependent natural language understanding. A computer-implemented technique is described for processing a linguistic item (e.g., a query) in an efficient and scalable manner. The technique interprets the linguistic item using a language understanding (LU) system in a manner that is based on a particular endpoint mechanism from which the linguistic item originated. The LU system may include an endpoint-independent subsystem, an endpoint-dependent subsystem, and a ranking component. The endpoint-independent subsystem interprets the linguistic item in a manner that is independent of the particular endpoint mechanism. The endpoint-dependent subsystem interprets the linguistic item in a manner that is dependent on the particular endpoint mechanism. The ranking component generates final interpretation results based on intermediate results generated by the endpoint-independent subsystem and the endpoint-dependent subsystem, e.g., by identifying the most likely interpretation of the linguistic item.',\n",
       " 'Compound service performance metric framework. Techniques described herein include determining, maintaining, and applying compound service performance metrics, based on data metrics from a plurality of different services. Service-specific data metrics may be received from a plurality of different communication services offered by a service provider, for example, Internet service, voice service, video service, SMS service, etc. Different combinations, relationships, and weighting factors for the data metrics may be defined and stored for each compound performance metric. Compound performance metrics may be defined, including for example, compound customer sentiment metrics, compound customer value metrics, and/or compound customer resource usage metrics. In some cases, machine-learning and/or analytics may be performed using service-specific data metrics and corresponding customer actions, in order to determine correlations between particular combinations of data metrics and customer actions.',\n",
       " 'Identifying an entity associated with an online communication. An approach is described for identifying an entity associated with a communication in an online environment. An associated system may include a processor and a memory storing an application program, which, when executed on the processor, performs an operation. The operation may include receiving a communication within the online environment. The communication may include a plurality of sequential messages. The operation further may include facilitating parsing, via natural language processing, of language in the communication corresponding to an entity and one or more sentiments associated with the entity. The operation further may include determining whether the entity is unambiguously identifiable. Upon determining that the entity is not unambiguously identifiable, the operation may include identifying the entity based upon Bayesian inference. According to an embodiment, determining whether the entity is unambiguously identifiable may include determining whether the entity is among a plurality of participants in the communication.',\n",
       " \"Intelligently splitting text in messages posted on social media website to be more readable and understandable for user. A method, system and computer program product for improving readability and understandability in messages posted on a social media website. The messages posted on a social media website, such as the user's social networking feed, are scanned. The scanned messages are analyzed for topics, meaning and/or tenses using natural language processing. The text in the scanned messages are split into message segments based on topic, meaning, tenses, punctuation, custom identifiers, hashtags and/or @ symbols. These message segments are then grouped based on relatedness of the topics, meaning and/or tenses. The message segments are ordered in each group of message segments, such as based on timestamps. The ordered message segments are then displayed to the user. By displaying these message segments in separate groupings in a logical order, the user will be able to view the messages posted on the user's social media website in a more readable and understandable manner.\",\n",
       " 'Inferential analysis using feedback for extracting and combining cyber risk information. Various embodiments of the present technology include methods of assessing risk of a cyber security failure in a computer network of an entity. Some embodiments involve using continual or periodic data collecting to improve inferential analysis, as well as obtaining circumstantial or inferential information from social networks. Machine learning may be used to improve predicitive capabilities. Some embodiments allow for identification of an entity from circumstantial or inferential information based on the machine learning and comparative analyses.',\n",
       " 'Alignment of data captured by autonomous vehicles to generate high definition maps. A high-definition map system receives sensor data from vehicles travelling along routes and combines the data to generate a high definition map for use in driving vehicles, for example, for guiding autonomous vehicles. A pose graph is built from the collected data, each pose representing location and orientation of a vehicle. The pose graph is optimized to minimize constraints between poses. Points associated with surface are assigned a confidence measure determined using a measure of hardness/softness of the surface. A machine-learning-based result filter detects bad alignment results and prevents them from being entered in the subsequent global pose optimization. The alignment framework is parallelizable for execution using a parallel/distributed architecture. Alignment hot spots are detected for further verification and improvement. The system supports incremental updates, thereby allowing refinements of sub-graphs for incrementally improving the high-definition map for keeping it up to date.',\n",
       " \"Machine-learning approach to holographic particle characterization. Holograms of colloidal dispersions encode comprehensive information about individual particles' three-dimensional positions, sizes and optical properties. Extracting that information typically is computation-ally intensive, and thus slow. Machine-learning techniques based on support vector machines (SVMs) can analyze holographic video microscopy data in real time on low-power computers. The resulting stream of precise particle-resolved tracking and characterization data provides unparalleled insights into the composition and dynamics of colloidal dispersions and enables applications ranging from basic research to process control and quality assurance.\",\n",
       " 'Dynamic semantic analysis on free-text reviews to identify safety concerns. Disclosed are various embodiments for identifying safety concerns by employing dynamic semantic analysis on natural language provided in free-text reviews of products. A computing environment may encode user interface data that causes an event listener to monitor a free-text description provided in a text field in a user interface. A semantic analysis may be performed on the free-text description in real-time as the free-text description is generated. Remedial actions may be performed based on the semantics identified in the free-text description or severity levels associated with identified safety concerns.',\n",
       " 'Knowledge-based editor with natural language interface. A computer-implemented method for knowledge based ontology editing, is provided. The method receives a language instance to update a knowledge base, using a computer. The method semantically parses the language instance to detect an ontology for editing. The method maps one or more nodes for the ontology for editing based on an ontology database and the knowledge base. The method determines whether the mapped nodes are defined or undefined within the knowledge base. The method calculates a first confidence score based on a number of the defined and undefined mapped nodes. Furthermore, the method updates the knowledge base when the first confidence score meets a pre-defined threshold.',\n",
       " 'Abstraction of syntax in localization through pre-rendering. A content management system (CMS) and a translation management system (TMS) can utilize content dimensions for content items to manage and translate the content items between languages. Machine and human translations of complex dynamic content can also be improved by pre-rendering the content to remove localization-related syntax prior to machine or human translation. Content items can also be scored as to their suitability for localization prior to translation, and translation can be skipped for content items that do not have a sufficiently high score. Semantic and natural language processing (NLP) techniques can also be utilized for content categorization and routing. Translations of content items can also be continuously refined and higher quality re-translated content can be provided in an automated fashion.',\n",
       " 'Question and answer system emulating people and clusters of blended people. Embodiments are directed to an information processing system for generating answers in response to questions. The system includes a memory, a processor system communicatively coupled to the memory. The processor system is configured to store in the memory data of a corpus of a predetermined entity, and receive a question comprising a natural language format. The processor circuit is further configured to analyze the data of the corpus of the predetermined entity to derive an emulated answer to the question, wherein the emulated answer includes an emulation of an actual answer that would be provided by the predetermined entity.',\n",
       " 'Hybrid natural language processor. Methods and a natural language processor for processing a natural language query are provided. The processor includes a classifier, a rule-based pre-processor, a rule-based post-processor, a named entity recognizer, and an output module. The method involves receiving a text representation of the natural language query, pre-processing the text representation, applying a classification statistical model to the text representation when pre-processing fails, applying a post-processing rule, and performing name entity recognition.',\n",
       " 'Machine learning assisted reservoir simulation. An embodiment includes a method for use by at least one machine learning classifier. The method comprises the machine learning classifier obtaining one or more recent results from at least one geomechanical simulation; the machine learning classifier comparing the recent results to stored historical data; and, based on the comparing, the machine learning classifier deciding at least one reservoir model for use by at least one reservoir simulation.',\n",
       " 'Method and apparatus for identification of biomolecules. The present disclosure presents methods, systems, and devices for identifying new molecules directly from biological sequence information, with at least one of a desired bioactivity profile, functional attribute, biochemical reactivity, biological impact, pharmacological characteristic or therapeutic effect. The present disclosure further includes analyzing, at the processor, data features of biological sequence information and other data sources, including a feature-definition set by processing, using one or more bioinformatic techniques, computational algorithms, or methods of statistical machine learning, data sources relating to biological or chemical molecules, including biomolecules, including but not limited to peptides, having desired physical or chemical characteristics, bioactivities, functional attributes, biological impacts, pharmacologic properties or therapeutic effects.',\n",
       " 'Multi-modal electronic document classification. A method comprising operating at least one hardware processor for: receiving, as input, a plurality of electronic documents, training a machine learning classifier based, at least on part, on a training set comprising: (i) labels associated with the electronic documents, (ii) raw text from each of said plurality of electronic documents, and (iii) a rasterized version of each of said plurality of electronic documents, and applying said machine learning classifier to classify one or more new electronic documents.',\n",
       " 'Training data generating device, method, and program, and crowd state recognition device, method, and program. A rectangular region group storage unit stores a group of rectangular regions indicating portions to be recognized for a crowd state on an image. A crowd state recognition dictionary storage unit stores a dictionary of a discriminator acquired by machine learning by use of a plurality of pairs of crowd state image as an image which expresses a crowd state at a predetermined size and includes a person whose reference site is expressed as large as the size of the reference site of a person defined for the predetermined size, and training label for the crowd state image. A crowd state recognition unit extracts regions indicated in the group of rectangular regions stored in the rectangular region group storage unit from a given image, and recognizes states of the crowds shot in the extracted images based on the dictionary.',\n",
       " 'Model compression and fine-tuning. Compressing a machine learning network, such as a neural network, includes replacing one layer in the neural network with compressed layers to produce the compressed network. The compressed network may be fine-tuned by updating weight values in the compressed layer(s).',\n",
       " 'Behavioral modeling of a data center utilizing human knowledge to enhance a machine learning algorithm. A method generates a behavioral model of a data center when a machine learning algorithm is applied. A team of human modelers that partition the data center into a plurality of connected nodes is analyzed by a behavioral model. The behavioral model of the data center detects an anomaly in a system behavior center by recursively applying the behavioral model to each node and simple component. A compressed metric vector for the node is generated by reducing a dimension of an input metric vector. A root cause of a failure caused is determined by the anomaly and an action is automatically recommended to an operator to resolve a problem caused by the failure. The proactively actions are taken to keep the data center in a normal state based on the behavioral model using the machine learning algorithm.',\n",
       " 'Machine learning approach for query resolution via a dynamic determination and allocation of expert resources. The systems and methods described herein relate to mapping and identifying expert resources. The systems and methods described herein may provide a set of technologies, that work together as one solution, to effectively and efficiently resolve user questions. A cognitive engine may autonomously learn which experts have the knowledge to quickly solve a question or whether a previous question is similar enough to provide a solution instantly. Using machine learning, a know-how map may be created, linking all of the users of the system with their areas of expertise. Expert resources among the users may be mapped by determining connections between topics (and their corresponding tags) and calculating an expert score related to each topic for each user. These connections and expert scores are subsequently used during expert routing for each new question, to find those users with the expertise to give the best possible solution.',\n",
       " 'Systems and methods for selecting third party content based on feedback. The present disclosure selects third party content based on feedback. A selector identifies several content items including first and second content items (or more) responsive to a request. A machine learning engine determines a first feature of the first content item, a second feature of the second content item, and a third feature of the web page or a device associated with the request. The machine learning engine determines, responsive to the first feature and the third feature, a first score for the first content item based on a machine learning model generated using historical signals received from devices via a metadata channel formed from an electronic feedback interface. The machine learning engine determines a second score for the second content item responsive to the second feature and the third feature. A bidding module determines a price for the first content item based on the first and second scores.',\n",
       " 'Pipelined approach to fused kernels for optimization of machine learning workloads on graphical processing units. A method for optimization of machine learning (ML) workloads on a graphics processor unit (GPU). The method includes identifying a computation having a generic pattern commonly observed in ML processes. Hierarchical aggregation spanning a memory hierarchy of the GPU for processing is performed for the identified computation including maintaining partial output vector results in shared memory of the GPU. Hierarchical aggregation for vectors is performed including performing intra-block aggregation for multiple thread blocks of a partial output vector results on GPU global memory.',\n",
       " 'Aerial vehicle engine health prediction. Systems and methods for modeling engine health are provided. One example aspect of the present disclosure is directed to a method for modeling engine health. The method includes receiving, by one or more processors, engine acceptance test procedure (ATP) data. The method includes receiving, by the one or more processors, flight test data. The method includes generating, by the one or more processors, one or more coefficients for a power assistance check (PAC) based on the engine ATP data and the received flight test data using a machine learning technique. The method includes transmitting, by the one or more processors, the one or more coefficients for the PAC to a vehicle, wherein the vehicle uses the one or more coefficients in the PAC to predict engine health.',\n",
       " 'Systems and methods for expressive language, developmental disorder, and emotion assessment, and contextual feedback. In some embodiments, a method that includes capturing sound in a natural language environment using at least one sound capture device that is located in the natural language environment. The method also can include analyzing a sound signal from the sound captured by the at least one sound capture device to determine at least one characteristic of the sound signal. The method additionally can include reporting metrics that quantify the at least one characteristic of the sound signal. The metrics of the at least one characteristic can include a quantity of words spoken by one or more first persons in the natural language environment. Other embodiments are provided.',\n",
       " \"Dynamic gazetteers for personalized entity recognition. In speech processing systems personalization is added in the Natural Language Understanding (NLU) processor by incorporating external knowledge sources of user information to improve entity recognition performance of the speech processing system. Personalization in the NLU is effected by incorporating one or more dictionaries of entries, or gazetteers, with information personal to a respective user, that provide the user's information to permit disambiguation of semantic interpretation for input utterances to improve quality of speech processing results.\",\n",
       " 'Voice search assistant. Systems and methods for assisting voice searches are provided. An example method commences with receiving a voice query from a user and transmitting the voice query to a plurality of natural language processing systems. The method may continue with receiving a plurality of search parameter sets generated by the plurality of natural language processing systems based on the voice query. The method may further include transmitting at least one of the plurality of search parameter sets to a plurality of information search systems. The method may continue with receiving a plurality of responses from the plurality of information search systems. The plurality of responses may be generated by the plurality of information search systems based on the at least one of the plurality of search parameter sets. The method may conclude with providing at least one response of the plurality of responses to the user.',\n",
       " \"System and method of prediction through the use of latent semantic indexing. A predictive modeling method implemented on a computer for predicting patient outcomes and conditions from medical database records of a population of patients, and an optimization process of iterative variation of parameters of the method to achieve a best precision fit. Individual patient documents are created by concatenation of unstructured text fields from the patient's medical record, and these are processed using Natural Language Processing. A patient document corpus is built, and terms in the corpus are weighted and mapped to standard vocabularies. A term-by-document matrix is built and its dimensionality is reduced by Latent Semantic Indexing. Patient and term queries are combined and scored, producing a ranked list. The parameters of the model are iteratively optimized for an input list of patients with corresponding health score values.\",\n",
       " 'Identifying an entity associated with an online communication. An approach is described for identifying an entity associated with a communication in an online environment. A method pertaining to such approach may include receiving a communication within the online environment. The communication may be received via a communications network. The communication may include a plurality of sequential messages. The method further may include facilitating parsing, via natural language processing, of language in the communication corresponding to an entity and one or more sentiments associated with the entity. The method further may include determining whether the entity is unambiguously identifiable. Upon determining that the entity is not unambiguously identifiable, the method may include identifying the entity based upon Bayesian inference. According to an embodiment, determining whether the entity is unambiguously identifiable may include determining whether the entity is among a plurality of participants in the communication.',\n",
       " 'Verifying that the influence of a user data point has been removed from a machine learning classifier. Verifying that influence of a user data point has been removed from a machine learning classifier. In some embodiments, a method may include training a machine learning classifier using a training set of data points that includes a user data point, calculating a first loss of the machine learning classifier, updating the machine learning classifier by updating parameters of the machine learning classifier to remove influence of the user data point, calculating a second loss of the machine learning classifier, calculating an expected difference in loss of the machine learning classifier, and verifying that the influence of the user data point has been removed from the machine learning classifier by determining that the difference between the first loss and the second loss is within a threshold of the expected difference in loss.',\n",
       " 'System and method for verifying and detecting malware. A system configured to detect malware is described. The system including an infection verification pack configured to perform behavior detonation; identify a malware object based on machine-learning; and select one or more persistent artifacts of the malware on the target system based on one or more algorithms applied to behavior traces of the malware object to select one or more persistent artifacts of the malware on the target system.',\n",
       " 'Auto-documentation for application program interfaces based on network requests and responses. Disclosed embodiments are directed at systems, methods, and architecture for providing auto-documentation to APIs. The auto documentation plugin is architecturally placed between an API and a client thereof and parses API requests and responses in order to generate auto-documentation. In some embodiments, the auto-documentation plugin is used to update preexisting documentation after updates. In some embodiments, the auto-documentation plugin accesses an online documentation repository. In some embodiments, the auto-documentation plugin makes use of a machine learning model to determine how and which portions of an existing documentation file to update.',\n",
       " 'Machine learning based content delivery. Systems and methods for managing content delivery functionalities based on machine learning models are provided. In one aspect, content requests are routed in accordance with clusters of historical content requests to optimize cache performance. In another aspect, content delivery strategies for responding to content requests are determined based on a model trained on data related to historical content requests. The model may also be used to determine above-the-fold configurations for rendering responses to content requests. In some embodiments, portions of the model can be executed on client computing devices.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct pipeline\n",
    "# uncomment to download stop words\n",
    "# !python -m spacy download en\n",
    "stop_words = stopwords.words('/Users/lee/Documents/techniche/techniche/data/stopwords/english')\n",
    "\n",
    "nlp = build_pipeline()\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# pre-process documents\n",
    "processed_docs = process_docs(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build corpus and dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dictionary\n",
    "id_to_word = Dictionary(processed_docs)\n",
    "\n",
    "# apply term document frequency\n",
    "# converts documents in corpus to bag-of-words format, a list of (token_id, token_count) tuples\n",
    "corpus = [id_to_word.doc2bow(doc) for doc in processed_docs]\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view formatted corpus (term-doc-frequency)\n",
    "formatted_corpus = [[(id_to_word[id], freq) for id, freq in text] for text in corpus]\n",
    "#formatted_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model - model #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Lee) - deprecation warnings\n",
    "# construct LDA model\n",
    "model_lda = LdaModel(corpus=corpus,\n",
    "                     id2word=id_to_word,\n",
    "                     num_topics=25, \n",
    "                     random_state=100,\n",
    "                     update_every=1,\n",
    "                     chunksize=100,\n",
    "                     passes=10,\n",
    "                     alpha='auto',\n",
    "                     per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keywords in n topics in corpus\n",
    "# pprint(model_lda.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most important keywords, and the respective weight, that form topic with index 0\n",
    "# pprint(model_lda.print_topic(24))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate - model #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate perplexity metrics\n",
    "perplexity = model_lda.log_perplexity(corpus)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate coherence metric\n",
    "coherence = CoherenceModel(model=model_lda, texts=processed_docs, dictionary=id_to_word, coherence='c_v')\n",
    "coherence_1 = coherence.get_coherence()\n",
    "print(coherence_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate coherence metric or each of the n topicss\n",
    "coherence_1 = coherence.get_coherence_per_topic()\n",
    "coherence_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore topics\n",
    "pyLDAvis.enable_notebook()\n",
    "viz_topics_1 = pyLDAvis.gensim.prepare(model_lda, corpus, id_to_word)\n",
    "viz_topics_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2-  Mallet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to download Mallet topic model\n",
    "# !wget http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip\n",
    "# update path\n",
    "path_mallet = '/Users/lee/Documents/techniche/techniche/data/mallet-2.0.8/bin/mallet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = gensim.models.wrappers.LdaMallet(path_mallet, corpus=corpus, num_topics=25, id2word=id_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topics\n",
    "pprint(model_2.show_topics(formatted=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Lee) - calculate coherence metric\n",
    "coherence_2 = CoherenceModel(model=model_2, texts=data, dictionary=id_to_word, coherence='c_v')\n",
    "coherence_2 = coherence_2.get_coherence()\n",
    "print(coherence_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Lee)\n",
    "# def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "#     \"\"\"\n",
    "#     Compute c_v coherence for various number of topics\n",
    "\n",
    "#     Parameters:\n",
    "#     ----------\n",
    "#     dictionary : Gensim dictionary\n",
    "#     corpus : Gensim corpus\n",
    "#     texts : List of input texts\n",
    "#     limit : Max num of topics\n",
    "\n",
    "#     Returns:\n",
    "#     -------\n",
    "#     model_list : List of LDA topic models\n",
    "#     coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "#     \"\"\"\n",
    "#     coherence_values = []\n",
    "#     model_list = []\n",
    "#     for num_topics in range(start, limit, step):\n",
    "#         model = gensim.models.wrappers.LdaMallet(path_mallet, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "#         model_list.append(model)\n",
    "#         coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "#         coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "#     return model_list, coherence_values\n",
    "\n",
    "# model_list, coherence_values = compute_coherence_values(dictionary=id_to_word, corpus=corpus, texts=data, start=2, limit=40, step=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3 - Author topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct inventor-to-doc mapping as df from nested inventors column in json api response\n",
    "df_inventors = json_normalize(results['patents'], record_path=['inventors'], meta=['patent_number', 'patent_date'])\n",
    "df_inventors = df_inventors[['inventor_id', 'patent_number', 'patent_date']]\n",
    "df_inventors.sort_values(by=['patent_date'])\n",
    "df_inventors.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct inventor-to-doc mapping as df from nested inventors column in json api response\n",
    "df_inventors = json_normalize(raw_data, record_path=['inventors'], meta=['patent_number', 'patent_date'])\n",
    "df_inventors = df_inventors[['inventor_id', 'patent_number', 'patent_date']]\n",
    "df_inventors.sort_values(by=['patent_date'])\n",
    "df_inventors.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_pv_table():\n",
    "    # construct inventor-to-doc mapping as df from nested inventors column in json api response\n",
    "    df_inventors = json_normalize(results['patents'], record_path=['inventors'], meta=['patent_number', 'patent_date'])\n",
    "    df_inventors = df_inventors[['inventor_id', 'patent_number', 'patent_date']]\n",
    "    df_inventors.sort_values(by=['patent_date'])\n",
    "    df_inventors.head(3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author2doc (dict of (str, list of int), optional)\n",
    " A dictionary where keys are the names of authors and values are lists of document IDs that the author contributes to.\n",
    "\n",
    "doc2author (dict of (int, list of str), optional)\n",
    "- A dictionary where the keys are document IDs and the values are lists of author names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# { int(patent_number, [str(inventor_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foodict = {k: v for k, v in mydict.items() if k.startswith('foo')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dict = {}\n",
    "for dictionary in raw_data:\n",
    "    dictionary['patent_number']\n",
    "    new_dict.update(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dictionary in raw_data:\n",
    "    new_dict = dict(dictionary['patent_number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = {\"type\":\"insecure\",\"id\":\"1\",\"name\":\"peter\"}\n",
    "black_list = {\"type\"}\n",
    "rename ={\"id\":\"identity\"}  #use a mapping dictionary in case you want to rename multiple items\n",
    "dic = {rename.get(key,key) : val for key ,val in abc.items() if key not in black_list}\n",
    "print dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapdict\n",
    "{}\n",
    "\n",
    "'patent_number', 'inventors'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keys = ['patent_number', 'inventors']\n",
    "dict2 = {x: raw_data[0][x] for x in ['patent_number', 'inventors']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_list = []\n",
    "new_dict = {}\n",
    "for patent in raw_data:\n",
    "    inv_list = [inventor['inventor_id'] for inventor in patent['inventors']]\n",
    "    new_dict = {patent['patent_number'] : inv_list for patent in raw_data}\n",
    "        \n",
    "    #    inv_list.append(new_dict) = {patent['patent_number'], inv_list for ['patent_number', 'inventors']}\n",
    "    #new_dict.update({dictionary['patent_number']: inv_list for k,v in dictionary})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pat_inv_map(data):\n",
    "    pat_inv_dict = {}\n",
    "    for patent in data:\n",
    "        inventors = [inventor['inventor_id'] for inventor in patent['inventors']]\n",
    "        pat_number = int(patent['patent_number'])\n",
    "        pat_inv_dict[pat_number] = inventors\n",
    "    return pat_inv_dict\n",
    "    #    inv_list.append(new_dict) = {patent['patent_number'], inv_list for ['patent_number', 'inventors']}\n",
    "    #new_dict.update({dictionary['patent_number']: inv_list for k,v in dictionary})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pat2inv.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pat2inv = pat_inv_map(data)\n",
    "pat2inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat2inv = {k: list(str(v)) for k,v in df_inventors.groupby(\"patent_number\")[\"inventor_id\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patdf2inv = dict((df_pat_idx[key], value) for (key, value) in pat2inv.items())\n",
    "patdf2inv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct author-topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct author-topic model\n",
    "model_at = AuthorTopicModel(corpus=corpus,\n",
    "                         doc2author=pat2inv,\n",
    "                         id2word=id_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pat2inv.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct vectors for authors\n",
    "author_vecs = [model_at.get_author_topics(author) for author in model_at.id2author.values()]\n",
    "author_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the topic distribution for an author using use model[name] syntax\n",
    "# each topic has a probability of being expressed given the particular author, but only the ones above a certain threshold are shown.\n",
    "\n",
    "model_at['7788103-1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def show_author(name):\n",
    "#     print('\\n%s' % name)\n",
    "#     print('Docs:', model.author2doc[name])\n",
    "#     print('Topics:')\n",
    "#     pprint([(topic_labels[topic[0]], topic[1]) for topic in model[name]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate per-word bound, which is a measure of the model's predictive performance (reconstruction error?)\n",
    "\n",
    "build doc2author dictionary\n",
    "\n",
    "doc2author = atmodel.construct_doc2author(model.corpus, model.author2doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import atmodel\n",
    "doc2author = atmodel.construct_doc2author(model.corpus, model.author2doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim.models.atmodel.construct_author2doc(doc2author)\n",
    "# construct mapping from author IDs to document IDs.\n",
    "\n",
    "Parameters:\tdoc2author (dict of (int, list of str))  Mapping of document id to authors.\n",
    "Returns:\tMapping of authors to document ids.\n",
    "Return type:\tdict of (str, list of int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim.models.atmodel.construct_doc2author(corpus, author2doc)\n",
    "construct mapping from document IDs to author IDs\n",
    "\n",
    "Parameters:\t\n",
    "corpus (iterable of list of (int, float))  Corpus in BoW format.\n",
    "author2doc (dict of (str, list of int))  Mapping of authors to documents.\n",
    "Returns:\t\n",
    "Document to Author mapping.\n",
    "\n",
    "Return type:\t\n",
    "dict of (int, list of str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
