{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from test_model import (get_patent_fields_list, get_ml_patents, \n",
    "                        create_title_abstract_col,trim_data, \n",
    "                        structure_dataframe, partition_dataframe, \n",
    "                        build_pipeline, process_docs, pat_inv_map, get_topics)\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.corpora import Dictionary, mmcorpus\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models import AuthorTopicModel\n",
    "from gensim.test.utils import common_dictionary, datapath, temporary_file\n",
    "from smart_open import smart_open\n",
    "\n",
    "import spacy\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, punkt, RegexpTokenizer, wordpunct_tokenize\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
    "\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "import calendar\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Lee) - resolve deprecation warnings\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acquire data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# acquire dataset of ML patents by call to PatentsView API \n",
    "raw_data_1000 = get_ml_patents(pats_per_page=1000)\n",
    "raw_data_2000 = get_ml_patents(pats_per_page=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Acquire data - Structure data - 1000 pats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify fields (key:val pairs) to retain from full api response\n",
    "retained_keys = ['patent_number', 'patent_date', 'patent_title', 'patent_abstract', 'inventors']\n",
    "data_1000 = trim_data(data=raw_data_2000, keys=retained_keys)\n",
    "\n",
    "# create new key:value pair by combining values from patent_title and patent_abstract keys\n",
    "data_1000 = create_title_abstract_col(data=data_1000)\n",
    "\n",
    "# create dataframe, organize columns and sort by patent_date\n",
    "df_1000 = structure_dataframe(data=data_1000)\n",
    "\n",
    "# partition data\n",
    "data_train_1000, data_test_1000 = partition_dataframe(df, .8)\n",
    "\n",
    "# convert dataframe to list\n",
    "text_data_1000 = df_1000.patent_title_abstract.tolist()\n",
    "text_train_1000 = data_train_1000.patent_title_abstract.tolist()\n",
    "text_test_1000 = data_test_1000.patent_title_abstract.tolist()\n",
    "\n",
    "# TODO (Lee) - this explores direct structuring from api response without df\n",
    "text_list_1000 = []\n",
    "for i in data:\n",
    "    text_list.append(i['patent_title_abstract'])\n",
    "# text_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Acquire data - Structure data - 2000 pats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify fields (key:val pairs) to retain from full api response\n",
    "retained_keys = ['patent_number', 'patent_date', 'patent_title', 'patent_abstract', 'inventors']\n",
    "data_2000 = trim_data(data=raw_data_2000, keys=retained_keys)\n",
    "\n",
    "# create new key:value pair by combining values from patent_title and patent_abstract keys\n",
    "data_2000 = create_title_abstract_col(data=data_2000)\n",
    "\n",
    "# create dataframe, organize columns and sort by patent_date\n",
    "df_2000 = structure_dataframe(data=data_2000)\n",
    "\n",
    "# partition data\n",
    "data_train_2000, data_test_2000 = partition_dataframe(df, .8)\n",
    "\n",
    "# convert dataframe to list\n",
    "text_data_2000 = df_2000.patent_title_abstract.tolist()\n",
    "text_train_2000 = data_train_2000.patent_title_abstract.tolist()\n",
    "text_test_2000 = data_test_2000.patent_title_abstract.tolist()\n",
    "\n",
    "# TODO (Lee) - this explores direct structuring from api response without df\n",
    "text_list_2000 = []\n",
    "for i in data:\n",
    "    text_list.append(i['patent_title_abstract'])\n",
    "# text_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'ner']\n"
     ]
    }
   ],
   "source": [
    "# uncomment to download stop words\n",
    "# !python -m spacy download en\n",
    "stop_words = stopwords.words('/Users/lee/Documents/techniche/techniche/data/stopwords/english')\n",
    "\n",
    "# construct pipeline\n",
    "nlp = build_pipeline()\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# TODO (Lee) - pre-process documents TODO (Lee) - via text_list directly from api response\n",
    "# processed_docs_1 = process_docs(text_list)\n",
    "\n",
    "# pre-process documents TODO (Lee) - via df to list\n",
    "processed_docs_1000train = process_docs(text_train_1000)\n",
    "\n",
    "### Build corpus and dictionary\n",
    "\n",
    "# build dictionary\n",
    "id_to_word_1000train = Dictionary(processed_docs_1000train)\n",
    "\n",
    "# apply term-doc frequency (list of (token_id, token_count) tuples) to docs in corpus \n",
    "corpus_1000train = [id_to_word_1000train.doc2bow(doc) for doc in processed_docs_1000train]\n",
    "\n",
    "# view formatted corpus\n",
    "# uncomment below to view\n",
    "# formatted_corpus_1000 = [[(id_to_word[id], freq) for id, freq in text] for text in corpus_1000train]\n",
    "# formatted_corpus_1000\n",
    "# id_to_word_1000train.token2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model #1: Genism LDA model\n",
    "Model #1: implementation: Gensim LDAmodel; k_topics=5; n_docs=1000, partition = 80/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct model #1\n",
    "# TODO (Lee) - resolve deprecation warnings\n",
    "model_1 = LdaModel(corpus=corpus_1000train,\n",
    "                   id2word=id_to_word_1000train,\n",
    "                   num_topics=5, \n",
    "                   random_state=100,\n",
    "                   update_every=1,\n",
    "                   chunksize=100,\n",
    "                   passes=10,\n",
    "                   alpha='auto',\n",
    "                   per_word_topics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #1 - Explore and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore topics visually\n",
    "pyLDAvis.enable_notebook()\n",
    "viz_topics_model_1 = pyLDAvis.gensim.prepare(model_lda_k5_n1000, corpus_1000train, id_to_word_1000train)\n",
    "# viz_topics_model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keywords in n topics in corpus\n",
    "# uncomment below to view\n",
    "# pprint(model_1.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most important keywords, and the respective weight, that form topic with index 0\n",
    "# uncomment below to view\n",
    "# pprint(model_1.print_topic(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #1 - Evaluate\n",
    "As unsupervised learning task, no labels with which to evaluate the \"expected\" prediction. There is an open research agenda on various evaluation approaches (intrinsic vs extrinsic; machine vs human-interpretable, etc., task-specific)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #1 - Evaluate - Pre-process test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-process documents TODO (Lee) - via df to list\n",
    "processed_docs_1000test = process_docs(text_test_1000)\n",
    "\n",
    "### Build corpus and dictionary\n",
    "\n",
    "# build dictionary\n",
    "id_to_word_1000test = Dictionary(processed_docs_1000test)\n",
    "\n",
    "# apply term-doc frequency (list of (token_id, token_count) tuples) to docs in corpus \n",
    "corpus_1000test = [id_to_word_1000test.doc2bow(doc) for doc in processed_docs_1000test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #1 - Evaluate - Coherence\n",
    "Calculate topic coherence for topic models.\n",
    "Implements 'CoherenceModel' coherence pipeline (segmentation, probability estimation, confirmation measure, aggregation) from Roeder et al., 2015. \"Exploring the space of topic coherence measures\", WSDM '15 Proceedings of the Eighth ACM International Conference on Web Search and Data Mining (WSDM) 2015, 399-408."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3557055926610265\n"
     ]
    }
   ],
   "source": [
    "# calculate coherence metric for train set ((n = 800 docs/1000 docs total in dataset))\n",
    "coherence_model_1train = CoherenceModel(model=model_lda_k5_n1000, \n",
    "                                        texts=processed_docs_1000train,\n",
    "                                        dictionary=id_to_word_1000train,\n",
    "                                        coherence='c_v')\n",
    "coherence_model_1train_get = coherence_model_1train.get_coherence()\n",
    "print(coherence_model_1train_get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6020573135746659\n"
     ]
    }
   ],
   "source": [
    "# calculate coherence metric for test_set (n = 200 docs/100 docs total in dataset)\n",
    "coherence_model_1test = CoherenceModel(model=model_lda_k5_n1000, \n",
    "                                              texts=processed_docs_1000test, \n",
    "                                              dictionary=id_to_word_1000test, \n",
    "                                              coherence='c_v')\n",
    "coherence_model_1test_get = coherence_model_lda_k5_n1000test.get_coherence()\n",
    "print(coherence_model_1test_get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate coherence metric for each of the n topics in the test set\n",
    "coherence_model_1_per_topic = coherence_model_1test.get_coherence_per_topic()\n",
    "# print(coherence_model_1_per_topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #1 - Evaluate - Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate perplexity metric \n",
    "\n",
    "# metric calculates and returns per-word likelihood bound using a chunk of documents as evaluation corpus\n",
    "# output calculated statistics, including the perplexity=2^(-bound), to log at INFO level\n",
    "# Returns the variational bound score calculated for each word\n",
    "\n",
    "perplexity_train = model_lda.log_perplexity(corpus_2)\n",
    "print(perplexity_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity_test = model_lda.log_perplexity(corpus_3)\n",
    "print(perplexity_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #1 - Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #1 - Predict - Pickle model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle model\n",
    "pickle.dump(model_lda, open('/Users/lee/Documents/techniche/techniche/data/model_lda_1.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lda = pickle.load(open('/Users/lee/Documents/techniche/techniche/data/model_lda_1.pkl','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #1 - inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracts topics for given document from Gensim\n",
    "# TODO (Lee) - call from test_model.py\n",
    "def get_topics(doc, k=5, model_lda=model_lda):\n",
    "    topic_id = sorted(model_lda[doc][0], key=lambda x: -x[1])\n",
    "    top_k_topics = [x[0] for x in topic_id[:k]]\n",
    "    return [(i, model_lda.print_topic(i)) for i in top_k_topics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `get_document_topics()` returns topic probability distribution for given document\n",
    "# topic_dist_675_a = model_lda.get_document_topics(corpus[50])\n",
    "# pprint(sorted(topic_dist_50_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topicid = 3\n",
    "# model_lda.get_topic_terms(topicid, topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_train[doc_id]\n",
    "# doc_id = 675\n",
    "# topic_dist_675_b = sorted(get_topics(corpus[doc_id], k=10)), text_train[doc_id]\n",
    "# pprint(topic_dist_675_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = 'virtual dictionary lexicon enablement voice'.split()\n",
    "text_input_1 = 'smart assistant transformer model translation'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_word.doc2bow(text_input_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_topics(id_to_word.doc2bow(text_input_1), k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input_2 = \"\"\"At the Siri International team within Apple we bring the Siri intelligent assistant to our customers worldwide in over 40 languages and dialects. Join us, and tackle some of the most challenging problems in natural language processing and large scale applied machine learning. You will build cutting edge natural language understanding technologies and deploy them on a global scale. Your work will advance and shape the future vision of our multi-lingual, multi-cultural Siri assistant, and Search applications used by millions across the world Key Qualifications\n",
    "Extensive track record of scientific research in NLP and Machine Learning, or similar experience in developing language technologies for shipping products.\n",
    "Strong coding and software engineering skills in a mainstream programming language, such as Python, Java, C/C++.\n",
    "Familiarity with NLP/ML tools and packages like Caffe, pyTorch, TensorFlow, Weka, scikit-learn, nltk, etc.\n",
    "Practical experience building production quality applications related to natural language processing and machine learning.\n",
    "In-depth knowledge of machine learning algorithms and ability to apply them in data driven natural language processing systems.\n",
    "Ability to quickly prototype ideas / solutions, perform critical analysis, and use creative approaches for solving complex problems.\n",
    "Attention to detail and excellent communication skills.\n",
    "Description\n",
    "We are looking for a highly motivated technologist with a strong background in Natural Language Processing and Machine Learning research. The ideal candidate will have a strong track record of taking research ideas to real-world applications. In this position you will apply your problem solving skills to challenges and opportunities within Siri International, which involves development of large-scale language technologies for many natural languages worldwide. The primary responsibility of this role is to conduct research and develop innovative machine learning, artificial intelligence and NLP solutions for multi-lingual conversational agents. You will have the opportunity to investigate cutting edge research methods that will improve customer experience of our products and enable our engineers to scale these technologies across a variety of natural languages. You will also provide technical leadership and experiment-driven insights for engineering teams on their machine learning modeling and data decisions. You will play a central role in defining the future technical directions of Siri International through quick prototyping, critical analysis and development of core multi-lingual NLP technologies.\n",
    "Education & Experience\n",
    "* PhD in Machine Learning, Statistics, Computer Science, Mathematics or related field with specialization in natural language processing and/or machine learning, OR * Masters degree in a related field with a strong academic/industrial track record. * Hands-on research experience in an academic or industrial setting.\"\"\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_topics(id_to_word.doc2bow(text_input_2), k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print keywords in n topics\n",
    "sorted(model_lda.show_topics(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print keywords in n topics\n",
    "sorted(model_lda.print_topics(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print keywords in n topics\n",
    "sorted(model_lda.print_topics(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print keywords in n topics\n",
    "sorted(model_lda.print_topics(), key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_topic() returns n most important/relevant words, and their weights, that comprise given topic\n",
    "pprint(model_lda.show_topic(1, topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(model_lda.show_topics(num_topics=5, num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Gensim example - Alternate predict workflow - Create a new corpus, made of previously unseen documents.\n",
    "# other_texts = [\n",
    "#      ['computer', 'time', 'graph'],\n",
    "#      ['survey', 'response', 'eps'],\n",
    "#      ['human', 'system', 'computer']\n",
    "#  ]\n",
    "# other_corpus = [common_dictionary.doc2bow(text) for text in other_texts]\n",
    "# unseen_doc = other_corpus[0]\n",
    "# vector = lda[unseen_doc]  # get topic probability distribution for a document\n",
    "# Update the model by incrementally training on the new corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #2 - Mallet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to download Mallet topic model\n",
    "# !wget http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip\n",
    "# update path\n",
    "path_mallet = '/Users/lee/Documents/techniche/techniche/data/mallet-2.0.8/bin/mallet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = gensim.models.wrappers.LdaMallet(path_mallet, corpus=corpus, num_topics=25, id2word=id_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topics\n",
    "# pprint(model_2.show_topics(formatted=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Lee) - calculate coherence metric\n",
    "coherence_2 = CoherenceModel(model=model_2, texts=data, dictionary=id_to_word, coherence='c_v')\n",
    "coherence_2 = coherence_2.get_coherence()\n",
    "# print(coherence_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Lee)\n",
    "# def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "#     \"\"\"\n",
    "#     Compute c_v coherence for various number of topics\n",
    "\n",
    "#     Parameters:\n",
    "#     ----------\n",
    "#     dictionary : Gensim dictionary\n",
    "#     corpus : Gensim corpus\n",
    "#     texts : List of input texts\n",
    "#     limit : Max num of topics\n",
    "\n",
    "#     Returns:\n",
    "#     -------\n",
    "#     model_list : List of LDA topic models\n",
    "#     coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "#     \"\"\"\n",
    "#     coherence_values = []\n",
    "#     model_list = []\n",
    "#     for num_topics in range(start, limit, step):\n",
    "#         model = gensim.models.wrappers.LdaMallet(path_mallet, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "#         model_list.append(model)\n",
    "#         coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "#         coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "#     return model_list, coherence_values\n",
    "\n",
    "# model_list, coherence_values = compute_coherence_values(dictionary=id_to_word, corpus=corpus, texts=data, start=2, limit=40, step=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #3 - Author topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct inventor-to-doc mapping as df from nested inventors column in json api response\n",
    "# df_inventors = json_normalize(data, record_path=['inventors'], meta=['patent_number', 'patent_date'])\n",
    "# df_inventors = df_inventors[['inventor_id', 'patent_number', 'patent_date']]\n",
    "# df_inventors.sort_values(by=['patent_date'])\n",
    "# df_inventors.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick visual index to patent number mapping\n",
    "# for i in data:\n",
    "#     print(data.index(i), i['patent_number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO (Lee) review fix to pat_inv_map, in which \"patent\" in mapping is idx of pat, not pat_number from api\n",
    "pat2inv = pat_inv_map(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct author-topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct author-topic model\n",
    "model_at = AuthorTopicModel(corpus=corpus,\n",
    "                         doc2author=pat2inv,\n",
    "                         id2word=id_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct vectors for authors\n",
    "author_vecs = [model_at.get_author_topics(author) for author in model_at.id2author.values()]\n",
    "author_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve topic distribution for author using use model[name] syntax\n",
    "# each topic has a probability of being expressed given the particular author, \n",
    "# but only the ones above a certain threshold are displayed\n",
    "\n",
    "model_at['7788103-1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def show_author(name):\n",
    "#     print('\\n%s' % name)\n",
    "#     print('Docs:', model.author2doc[name])\n",
    "#     print('Topics:')\n",
    "#     pprint([(topic_labels[topic[0]], topic[1]) for topic in model[name]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build mapping from inventor to patent\n",
    "inv2pat = gensim.models.atmodel.construct_author2doc(pat2inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #3 - Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction functions that take input of new text string, and predict topic distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #1 - Evaluate - holdout set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Lee) - evaluate on 1k documents **not** used in LDA training\n",
    "doc_stream = (tokens for _, tokens in iter_wiki('./data/simplewiki-20140623-pages-articles.xml.bz2'))  # generator\n",
    "test_docs = list(itertools.islice(doc_stream, 8000, 9000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #1 - Evaluate - Doc split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Lee) - split each document into two parts, and check that 1) topics of the first half are similar to \n",
    "topics of the second 2) halves of different documents are mostly dissimilar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Lee)\n",
    "def intra_inter(model, test_docs, num_pairs=10000):\n",
    "    # split each test document into two halves and compute topics for each half\n",
    "    part1 = [model[id2word.doc2bow(tokens[: len(tokens) / 2])] for tokens in test_docs]\n",
    "    part2 = [model[id2word.doc2bow(tokens[len(tokens) / 2 :])] for tokens in test_docs]\n",
    "    \n",
    "    # print computed similarities (uses cossim)\n",
    "    print(\"average cosine similarity between corresponding parts (higher is better):\")\n",
    "    print(np.mean([gensim.matutils.cossim(p1, p2) for p1, p2 in zip(part1, part2)]))\n",
    "\n",
    "    random_pairs = np.random.randint(0, len(test_docs), size=(num_pairs, 2))\n",
    "    print(\"average cosine similarity between 10,000 random parts (lower is better):\")    \n",
    "    print(np.mean([gensim.matutils.cossim(part1[i[0]], part2[i[1]]) for i in random_pairs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Lee)\n",
    "print(\"LDA results:\")\n",
    "intra_inter(lda_model, test_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model #1 - Evaluate - Log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
